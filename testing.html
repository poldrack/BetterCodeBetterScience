

<!DOCTYPE html>


<html lang="en" data-content_root="" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

    <title>Software testing &#8212; Better Code, Better Science</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="_static/styles/theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="_static/styles/bootstrap.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="_static/styles/pydata-sphinx-theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />

  
  <link href="_static/vendor/fontawesome/6.5.2/css/all.min.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.5.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.5.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.5.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="_static/pygments.css" />
    <link rel="stylesheet" type="text/css" href="_static/styles/sphinx-book-theme.css" />
    <link rel="stylesheet" type="text/css" href="_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-design.5ea377869091fd0449014c60fc090103.min.css" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b" />
<link rel="preload" as="script" href="_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b" />
  <script src="_static/vendor/fontawesome/6.5.2/js/all.min.js?digest=dfe6caa3a7d634c4db9b"></script>

    <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script>
    <script src="_static/jquery.js"></script>
    <script src="_static/underscore.js"></script>
    <script src="_static/_sphinx_javascript_frameworks_compat.js"></script>
    <script src="_static/doctools.js"></script>
    <script src="_static/sphinx_highlight.js"></script>
    <script src="_static/clipboard.min.js"></script>
    <script src="_static/copybutton.js"></script>
    <script src="_static/scripts/sphinx-book-theme.js"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="_static/togglebutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="_static/design-tabs.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script async="async" src="_static/sphinx-thebe.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'testing';</script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Coding with AI" href="AI_coding_assistants.html" />
    <link rel="prev" title="Principles of software engineering" href="software_engineering.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-primary-sidebar-checkbox"/>
  <label class="overlay overlay-primary" for="pst-primary-sidebar-checkbox"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-secondary-sidebar-checkbox"/>
  <label class="overlay overlay-secondary" for="pst-secondary-sidebar-checkbox"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  
    
  

<a class="navbar-brand logo" href="frontmatter.html">
  
  
  
  
  
  
    <p class="title logo__title">Better Code, Better Science</p>
  
</a></div>
        <div class="sidebar-primary-item">

 <script>
 document.write(`
   <button class="btn search-button-field search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
    <span class="search-button__default-text">Search</span>
    <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
   </button>
 `);
 </script></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="frontmatter.html">
                    Better Code, Better Science
                </a>
            </li>
        </ul>
        <ul class="current nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="introduction.html">Introduction</a></li>
<li class="toctree-l1"><a class="reference internal" href="essential_tools_and_techniques.html">Essential tools and techniques</a></li>
<li class="toctree-l1"><a class="reference internal" href="software_engineering.html">Principles of software engineering</a></li>
<li class="toctree-l1 current active"><a class="current reference internal" href="#">Software testing</a></li>
<li class="toctree-l1"><a class="reference internal" href="AI_coding_assistants.html">Coding with AI</a></li>
<li class="toctree-l1"><a class="reference internal" href="project_organization.html">Project structure and management</a></li>
<li class="toctree-l1"><a class="reference internal" href="data_management.html">Data Organization and Management</a></li>
<li class="toctree-l1"><a class="reference internal" href="bibliography.html">Bibliography</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><button class="sidebar-toggle primary-toggle btn btn-sm" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</button></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-source-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Source repositories">
    <i class="fab fa-github"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://github.com/poldrack/BetterCodeBetterScience" target="_blank"
   class="btn btn-sm btn-source-repository-button dropdown-item"
   title="Source repository"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="btn__text-container">Repository</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/poldrack/BetterCodeBetterScience/issues/new?title=Issue%20on%20page%20%2Ftesting.html&body=Your%20issue%20content%20here." target="_blank"
   class="btn btn-sm btn-source-issues-button dropdown-item"
   title="Open an issue"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="btn__text-container">Open issue</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="_sources/testing.md" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.md</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light"></i>
    <i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark"></i>
    <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"></i>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm pst-navbar-icon search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<button class="sidebar-toggle secondary-toggle btn btn-sm" title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</button>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Software testing</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#why-use-software-tests">Why use software tests?</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#types-of-tests">Types of tests</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#unit-tests">Unit tests</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#integration-tests">Integration tests</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#the-anatomy-of-a-test">The anatomy of a test</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#when-to-write-tests">When to write tests</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#bug-driven-testing-any-time-you-encounter-a-bug-write-a-test">Bug-driven testing: Any time you encounter a bug, write a test</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#the-structure-of-a-good-test">The structure of a good test</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#test-against-the-interface-not-the-implementation">Test against the interface, not the implementation</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#tests-should-be-independent">Tests should be independent</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#testing-frameworks">Testing frameworks</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#potential-problems-with-ai-generated-tests">Potential problems with AI-generated tests</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#test-driven-development-and-ai-assisted-coding">Test-driven development and AI-assisted coding</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#test-coverage">Test coverage</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#test-fixtures">Test fixtures</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#mocking">Mocking</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#parametrized-tests">Parametrized tests</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#property-based-testing">Property-based testing</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#automated-testing-and-continuous-integration">Automated testing and continuous integration</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#using-github-actions">Using GitHub Actions</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#optimizing-the-testing-workflow">Optimizing the testing workflow</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#cherry-picking-specific-tests">Cherry-picking specific tests</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#controlling-test-execution-order">Controlling test execution order</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#only-run-failed-tests-or-run-them-first">Only run failed tests, or run them first</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#separate-unit-tests-from-longer-running-tests">Separate unit tests from longer-running tests</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#longest-tests-last">Longest tests last</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#using-minimal-mock-datasets">Using minimal mock datasets</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#adding-minimal-processing-modes-for-integration-tests">Adding minimal processing modes for integration tests</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#parallelizing-testing">Parallelizing testing</a></li>
</ul>
</li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="software-testing">
<h1><a class="toc-backref" href="#id6">Software testing</a><a class="headerlink" href="#software-testing" title="Permalink to this heading">#</a></h1>
<div class="contents topic" id="contents">
<p class="topic-title">Contents</p>
<ul class="simple">
<li><p><a class="reference internal" href="#software-testing" id="id6">Software testing</a></p>
<ul>
<li><p><a class="reference internal" href="#why-use-software-tests" id="id7">Why use software tests?</a></p></li>
<li><p><a class="reference internal" href="#types-of-tests" id="id8">Types of tests</a></p>
<ul>
<li><p><a class="reference internal" href="#unit-tests" id="id9">Unit tests</a></p></li>
<li><p><a class="reference internal" href="#integration-tests" id="id10">Integration tests</a></p></li>
</ul>
</li>
<li><p><a class="reference internal" href="#the-anatomy-of-a-test" id="id11">The anatomy of a test</a></p></li>
<li><p><a class="reference internal" href="#when-to-write-tests" id="id12">When to write tests</a></p>
<ul>
<li><p><a class="reference internal" href="#bug-driven-testing-any-time-you-encounter-a-bug-write-a-test" id="id13">Bug-driven testing: Any time you encounter a bug, write a test</a></p></li>
</ul>
</li>
<li><p><a class="reference internal" href="#the-structure-of-a-good-test" id="id14">The structure of a good test</a></p>
<ul>
<li><p><a class="reference internal" href="#test-against-the-interface-not-the-implementation" id="id15">Test against the interface, not the implementation</a></p></li>
<li><p><a class="reference internal" href="#tests-should-be-independent" id="id16">Tests should be independent</a></p></li>
</ul>
</li>
<li><p><a class="reference internal" href="#testing-frameworks" id="id17">Testing frameworks</a></p>
<ul>
<li><p><a class="reference internal" href="#potential-problems-with-ai-generated-tests" id="id18">Potential problems with AI-generated tests</a></p></li>
</ul>
</li>
<li><p><a class="reference internal" href="#test-driven-development-and-ai-assisted-coding" id="id19">Test-driven development and AI-assisted coding</a></p></li>
<li><p><a class="reference internal" href="#test-coverage" id="id20">Test coverage</a></p></li>
<li><p><a class="reference internal" href="#test-fixtures" id="id21">Test fixtures</a></p></li>
<li><p><a class="reference internal" href="#mocking" id="id22">Mocking</a></p></li>
<li><p><a class="reference internal" href="#parametrized-tests" id="id23">Parametrized tests</a></p></li>
<li><p><a class="reference internal" href="#property-based-testing" id="id24">Property-based testing</a></p></li>
<li><p><a class="reference internal" href="#automated-testing-and-continuous-integration" id="id25">Automated testing and continuous integration</a></p>
<ul>
<li><p><a class="reference internal" href="#using-github-actions" id="id26">Using GitHub Actions</a></p></li>
</ul>
</li>
<li><p><a class="reference internal" href="#optimizing-the-testing-workflow" id="id27">Optimizing the testing workflow</a></p>
<ul>
<li><p><a class="reference internal" href="#cherry-picking-specific-tests" id="id28">Cherry-picking specific tests</a></p></li>
<li><p><a class="reference internal" href="#controlling-test-execution-order" id="id29">Controlling test execution order</a></p></li>
<li><p><a class="reference internal" href="#only-run-failed-tests-or-run-them-first" id="id30">Only run failed tests, or run them first</a></p></li>
<li><p><a class="reference internal" href="#separate-unit-tests-from-longer-running-tests" id="id31">Separate unit tests from longer-running tests</a></p></li>
<li><p><a class="reference internal" href="#longest-tests-last" id="id32">Longest tests last</a></p></li>
<li><p><a class="reference internal" href="#using-minimal-mock-datasets" id="id33">Using minimal mock datasets</a></p></li>
<li><p><a class="reference internal" href="#adding-minimal-processing-modes-for-integration-tests" id="id34">Adding minimal processing modes for integration tests</a></p></li>
<li><p><a class="reference internal" href="#parallelizing-testing" id="id35">Parallelizing testing</a></p></li>
</ul>
</li>
</ul>
</li>
</ul>
</div>
<p>Tests define the expected behavior of code, and detect when the code doesn’t match that expected behavior.</p>
<p>One useful analogy for software testing comes from the biosciences.
Think for a moment about the rapid COVID-19 tests that we all came to know during the pandemic.
These tests had two lines, one of which was a <em>control</em> line; if this line didn’t show up, then that meant that the test was not functioning as expected.
This is known as a <em>positive control</em> because it assesses the test’s ability to identify a positive response<a class="footnote-reference brackets" href="#id4" id="id1">1</a>.
Other tests also include <em>negative controls</em>, which ensure that the test returns a negative result when it should.</p>
<p>By analogy, we can think of software tests as being either positive or negative controls for the expected outcome of a software component.
A positive test assesses whether, given a particular valid input, the component returns the correct output.
A negative test assesses whether, in the absence of valid input, the component correctly returns the appropriate error message or null result.</p>
<section id="why-use-software-tests">
<h2><a class="toc-backref" href="#id7">Why use software tests?</a><a class="headerlink" href="#why-use-software-tests" title="Permalink to this heading">#</a></h2>
<p>The most obvious reason to write tests for code is to make sure that the answers that the code gives you are correct.
This becomes increasingly important as AI assistants write more of the code, to the degree that testing is becoming <em>more important</em> than code generation as a skill for generating good scientific code.
But creating correct code is far from the only reason for writing tests.</p>
<p>A second reason for testing was highlighted in our earlier discussion of test-driven development.
Tests can provide the coder with a measure of task completion; when the tests pass, the job is done, other than refactoring the code to make it cleaner and more robust.
Writing tests make one think harder about what exactly they want/need the code to do, and to specify those goals in as clear a way as possible.
Focusing on tests can help keep the coder’s “eyes on the MVP prize” and prevent generating too much extraneous code (“gold plating”).</p>
<p>A third reason to write tests is that they can help drive modularity in the code.
It’s much easier to write tests for a simple function that does a single thing than for a complex function with many different roles.
Testing can also help drive modularity by causing you to think more clearly about what a function does when developing the test; the inability to easily write a test for a function can suggest that the function might be overly complex and should be refactored.
In this way, writing tests can give us useful insights into the structure of the code.</p>
<p>A final reason to write tests is that they make it much easier to make changes to the code.
Without a robust test suite, one is always left worried that changing some aspect of the code will have unexpected effects on its former behavior (known as a “regression”).
Tests can provide you with the comfort you need to make changes, knowing that you will detect any untoward effects your changes might have.
This includes refactoring, where the changes are not meant to modify the function but simply to make the code more robust and readable.</p>
</section>
<section id="types-of-tests">
<h2><a class="toc-backref" href="#id8">Types of tests</a><a class="headerlink" href="#types-of-tests" title="Permalink to this heading">#</a></h2>
<section id="unit-tests">
<h3><a class="toc-backref" href="#id9">Unit tests</a><a class="headerlink" href="#unit-tests" title="Permalink to this heading">#</a></h3>
<p>Unit tests are the bread and butter of software testing.
They are meant to assess whether individual software components (in the case of Python, functions, classes, and methods) perform as expected.
This includes both assessing whether the component performs as it is supposed to perform given a particular input, but also assessing whether it performs correctly under boundary conditions or problematic conditions, where the correct response is often to raise an exception.
A major goal of unit testing in the latter case is preventing “garbage in, garbage out” behavior.
For example, say that we are testing a function that takes in two matrices, and that the size of these matrices along their first dimension is assumed to match.
In this case, we would want to test to make sure that if the function is provided with two matrices that mismatch in their first dimension, the function will respond by raising an exception rather than by giving back an answer that is incorrect or nonsensical (such as <em>NaN</em>, or “not a number”).
That is, we want to aim for “garbage in, exception out” behavior.</p>
</section>
<section id="integration-tests">
<h3><a class="toc-backref" href="#id10">Integration tests</a><a class="headerlink" href="#integration-tests" title="Permalink to this heading">#</a></h3>
<p>As the name suggests, an integration test assesses whether the entire application works as it should, integrating all of the components that were tested via unit testing.</p>
<p>One simple type of integration test is a “smoke test”.
This name <a class="reference external" href="https://learn.microsoft.com/en-us/previous-versions/ms182613(v=vs.80)">apparently</a> derives from the computer hardware industry, where one often performs an initial sanity test on an electronic component by plugging it in and seeing if it smokes.
In coding, a smoke test is a simple sanity check meant to ensure that the entire application runs without crashing.
This is usually accomplished by running a top-level function that exercises the entire application.
Smoke tests are useful for quickly identifying major problems, but they don’t actually test whether the application performs its function correctly.
They can be especially useful for large applications, where the full test suite may take hours to run.
An initial smoke test can determine whether something is broken downstream, saving lots of wasted testing time.</p>
<p>Full integration tests assess the function of the entire application; one can think of them as unit tests where the unit is the entire application.
Just as with unit tests, we want integration tests that both confirm proper operation under intended conditions, as well as confirming proper behavior (such as exiting with an error message) under improper conditions.</p>
</section>
</section>
<section id="the-anatomy-of-a-test">
<h2><a class="toc-backref" href="#id11">The anatomy of a test</a><a class="headerlink" href="#the-anatomy-of-a-test" title="Permalink to this heading">#</a></h2>
<p>A test is generally structured as a function that executes without raising an exception as long as the code behaves in an expected way.
Let’s say that we want to generate a function that returns the escape velocity of a planet:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">math</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>

<span class="k">def</span> <span class="nf">escape_velocity</span><span class="p">(</span><span class="n">mass</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span> <span class="n">radius</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span> <span class="n">G</span><span class="o">=</span><span class="mf">6.67430e-11</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Calculate the escape velocity from a celestial body, given its mass and radius.</span>

<span class="sd">    Args:</span>
<span class="sd">    mass (float): Mass of the celestial body in kg.</span>
<span class="sd">    radius (float): Radius of the celestial body in meters.</span>

<span class="sd">    Returns:</span>
<span class="sd">    float: Escape velocity in m/s.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    
    <span class="k">return</span> <span class="n">math</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">G</span> <span class="o">*</span> <span class="n">mass</span> <span class="o">/</span> <span class="n">radius</span><span class="p">)</span>
</pre></div>
</div>
<p>We can then generate a test to determine whether the value returned by our function matches the known value for a given planet:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">test_escape_velocity</span><span class="p">():</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Test the escape_velocity function with known values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">mass_earth</span> <span class="o">=</span> <span class="mf">5.972e24</span>  <span class="c1"># Earth mass in kg</span>
    <span class="n">radius_earth</span> <span class="o">=</span> <span class="mf">6.371e6</span>  <span class="c1"># Earth radius in meters</span>
    <span class="n">ev_expected</span> <span class="o">=</span> <span class="mf">11186.0</span>  <span class="c1"># Expected escape velocity for Earth in m/s</span>
    <span class="n">ev_computed</span> <span class="o">=</span> <span class="n">escape_velocity</span><span class="p">(</span><span class="n">mass_earth</span><span class="p">,</span> <span class="n">radius_earth</span><span class="p">)</span>
    <span class="k">assert</span> <span class="n">np</span><span class="o">.</span><span class="n">allclose</span><span class="p">(</span><span class="n">ev_expected</span><span class="p">,</span> <span class="n">ev_computed</span><span class="p">),</span> <span class="s2">&quot;Test failed!&quot;</span>
</pre></div>
</div>
<p>We can run this using <code class="docutils literal notranslate"><span class="pre">pytest</span></code> (more about this later), which tells us that the test passes:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>❯<span class="w"> </span>pytest<span class="w"> </span>src/BetterCodeBetterScience/escape_velocity.py
<span class="o">======================</span><span class="w"> </span><span class="nb">test</span><span class="w"> </span>session<span class="w"> </span><span class="nv">starts</span><span class="w"> </span><span class="o">======================</span>

src/BetterCodeBetterScience/escape_velocity.py<span class="w"> </span>..<span class="w">          </span><span class="o">[</span><span class="m">100</span>%<span class="o">]</span>

<span class="o">=======================</span><span class="w"> </span><span class="m">1</span><span class="w"> </span>passed<span class="w"> </span><span class="k">in</span><span class="w"> </span><span class="m">0</span>.10s<span class="w"> </span><span class="o">=======================</span>
</pre></div>
</div>
<p>If the returned value didn’t match the known value (within a given level of tolerance, which is handled by <code class="docutils literal notranslate"><span class="pre">np.allclose()</span></code>), then the assertion will fail and raise an exception, causing the test to fail.
For example, if we had mis-specified the expected value as 1186.0, we would have seen an error like this:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>❯<span class="w"> </span>pytest<span class="w"> </span>src/BetterCodeBetterScience/escape_velocity.py
<span class="o">======================</span><span class="w"> </span><span class="nb">test</span><span class="w"> </span>session<span class="w"> </span><span class="nv">starts</span><span class="w"> </span><span class="o">======================</span>

src/BetterCodeBetterScience/escape_velocity.py<span class="w"> </span>F<span class="w">          </span><span class="o">[</span><span class="m">100</span>%<span class="o">]</span>

<span class="o">===========================</span><span class="w"> </span><span class="nv">FAILURES</span><span class="w"> </span><span class="o">===========================</span>
_____________________<span class="w"> </span>test_escape_velocity<span class="w"> </span>_____________________

<span class="w">    </span>def<span class="w"> </span>test_escape_velocity<span class="o">()</span>:
<span class="w">        </span><span class="s2">&quot;&quot;&quot;</span>
<span class="s2">        Test the escape_velocity function with known values.</span>
<span class="s2">        &quot;&quot;&quot;</span>
<span class="w">        </span><span class="nv">mass_earth</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">5</span>.972e24<span class="w">  </span><span class="c1"># Earth mass in kg</span>
<span class="w">        </span><span class="nv">radius_earth</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">6</span>.371e6<span class="w">  </span><span class="c1"># Earth radius in meters</span>
<span class="w">        </span><span class="nv">ev_expected</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">1186</span>.0<span class="w"> </span><span class="c1"># 11186.0  # Expected escape velocity for Earth in m/s</span>
<span class="w">        </span><span class="nv">ev_computed</span><span class="w"> </span><span class="o">=</span><span class="w"> </span>escape_velocity<span class="o">(</span>mass_earth,<span class="w"> </span>radius_earth<span class="o">)</span>
&gt;<span class="w">       </span>assert<span class="w"> </span>np.allclose<span class="o">(</span>ev_expected,<span class="w"> </span>ev_computed<span class="o">)</span>,<span class="w"> </span><span class="s2">&quot;Test failed!&quot;</span>
E<span class="w">       </span>AssertionError:<span class="w"> </span>Test<span class="w"> </span>failed!
E<span class="w">       </span>assert<span class="w"> </span>False
E<span class="w">        </span>+<span class="w">  </span>where<span class="w"> </span><span class="nv">False</span><span class="w"> </span><span class="o">=</span><span class="w"> </span>&lt;<span class="k">function</span><span class="w"> </span>allclose<span class="w"> </span>at<span class="w"> </span>0x101403370&gt;<span class="o">(</span><span class="m">1186</span>.0,<span class="w"> </span><span class="m">11185</span>.97789184991<span class="o">)</span>
E<span class="w">        </span>+<span class="w">    </span>where<span class="w"> </span>&lt;<span class="k">function</span><span class="w"> </span>allclose<span class="w"> </span>at<span class="w"> </span>0x101403370&gt;<span class="w"> </span><span class="o">=</span><span class="w"> </span>np.allclose

src/BetterCodeBetterScience/escape_velocity.py:26:<span class="w"> </span><span class="nv">AssertionError</span>
<span class="o">=====================</span><span class="w"> </span>short<span class="w"> </span><span class="nb">test</span><span class="w"> </span>summary<span class="w"> </span><span class="nv">info</span><span class="w"> </span><span class="o">=====================</span>
FAILED<span class="w"> </span>src/BetterCodeBetterScience/escape_velocity.py::test_escape_velocity<span class="w"> </span>-<span class="w"> </span>AssertionError:<span class="w"> </span>Test<span class="w"> </span>failed!
<span class="o">========================</span><span class="w"> </span><span class="m">1</span><span class="w"> </span>failed<span class="w"> </span><span class="k">in</span><span class="w"> </span><span class="m">0</span>.11s<span class="w"> </span><span class="o">========================</span>
</pre></div>
</div>
<p>It’s also important to make sure that an exception is raised whenever it should be.
For example, the version of the <code class="docutils literal notranslate"><span class="pre">escape_velocity()</span></code> function above did not check to make sure that the mass and radius arguments had positive values, which means that it would give nonsensical results when passed a negative mass or radius value.
To address this we should add code to the function that causes it to raise an exception if either of the arguments is negative:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">escape_velocity</span><span class="p">(</span><span class="n">mass</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span> <span class="n">radius</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span> <span class="n">G</span><span class="o">=</span><span class="mf">6.67430e-11</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Calculate the escape velocity from a celestial body, given its mass and radius.</span>

<span class="sd">    Args:</span>
<span class="sd">    mass (float): Mass of the celestial body in kg.</span>
<span class="sd">    radius (float): Radius of the celestial body in meters.</span>

<span class="sd">    Returns:</span>
<span class="sd">    float: Escape velocity in m/s.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">mass</span> <span class="o">&lt;=</span> <span class="mi">0</span> <span class="ow">or</span> <span class="n">radius</span> <span class="o">&lt;=</span> <span class="mi">0</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Mass and radius must be positive values.&quot;</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">math</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">G</span> <span class="o">*</span> <span class="n">mass</span> <span class="o">/</span> <span class="n">radius</span><span class="p">)</span>

</pre></div>
</div>
<p>We can then specify a test that checks whether the function properly raises an exception when passed a negative value.
To do this we can use a feature of the <code class="docutils literal notranslate"><span class="pre">pytest</span></code> package (<code class="docutils literal notranslate"><span class="pre">pytest.raises</span></code>) that passes only if the specified exception is raised:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">test_escape_velocity_negative</span><span class="p">():</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Make sure the function raises ValueError for negative mass or radius.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">with</span> <span class="n">pytest</span><span class="o">.</span><span class="n">raises</span><span class="p">(</span><span class="ne">ValueError</span><span class="p">):</span>
        <span class="n">escape_velocity</span><span class="p">(</span><span class="o">-</span><span class="mf">5.972e24</span><span class="p">,</span> <span class="mf">6.371e6</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="when-to-write-tests">
<h2><a class="toc-backref" href="#id12">When to write tests</a><a class="headerlink" href="#when-to-write-tests" title="Permalink to this heading">#</a></h2>
<p>Too often researchers decide to write tests after they have written an entire codebase.
Having any tests is certainly better than having no tests, but integrating testing into ones development workflow from the start can help improve the development experience and ultimately lead to better and more maintainable software.
In Chapter 1 we mentioned the idea of <em>test-driven development</em>, which we outline in more detail below, but we first discuss a simple approach to introducing testing into the development process.</p>
<section id="bug-driven-testing-any-time-you-encounter-a-bug-write-a-test">
<h3><a class="toc-backref" href="#id13">Bug-driven testing: Any time you encounter a bug, write a test</a><a class="headerlink" href="#bug-driven-testing-any-time-you-encounter-a-bug-write-a-test" title="Permalink to this heading">#</a></h3>
<p>An easy way to introduce testing into the development process is to write a new test any time one encounters a bug, which we refer to as <em>bug-driven testing</em>.
This makes it easy to then work on fixing the bug, since the test will determine when the bug has been fixed.
In addition, the test will detect if future changes reintroduce the bug.</p>
<p>As an example, take the following function:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">find_outliers</span><span class="p">(</span><span class="n">data</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">float</span><span class="p">],</span> <span class="n">threshold</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">2.0</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Find outliers in a dataset using z-score method.</span>
<span class="sd">    </span>
<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    data : List[float]</span>
<span class="sd">        List of numerical values.</span>
<span class="sd">    threshold : float, default=2.0</span>
<span class="sd">        Number of standard deviations from the mean to consider a value as an outlier.</span>
<span class="sd">    </span>
<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    List[int]</span>
<span class="sd">        List of indices of outliers in the data.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    
    <span class="n">mean</span> <span class="o">=</span> <span class="nb">sum</span><span class="p">(</span><span class="n">data</span><span class="p">)</span> <span class="o">/</span> <span class="nb">len</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
    <span class="n">variance</span> <span class="o">=</span> <span class="nb">sum</span><span class="p">((</span><span class="n">x</span> <span class="o">-</span> <span class="n">mean</span><span class="p">)</span> <span class="o">**</span> <span class="mi">2</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">data</span><span class="p">)</span> <span class="o">/</span> <span class="nb">len</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
    <span class="n">std</span> <span class="o">=</span> <span class="n">variance</span> <span class="o">**</span> <span class="mf">0.5</span>
    
    <span class="c1"># Bug: division by zero when std is 0 (all values are identical)</span>
    <span class="c1"># This only happens when all data points are the same</span>
    <span class="n">outliers</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">value</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">data</span><span class="p">):</span>
        <span class="n">z_score</span> <span class="o">=</span> <span class="nb">abs</span><span class="p">(</span><span class="n">value</span> <span class="o">-</span> <span class="n">mean</span><span class="p">)</span> <span class="o">/</span> <span class="n">std</span> 
        <span class="k">if</span> <span class="n">z_score</span> <span class="o">&gt;</span> <span class="n">threshold</span><span class="p">:</span>
            <span class="n">outliers</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">i</span><span class="p">)</span>
    
    <span class="k">return</span> <span class="n">outliers</span>
</pre></div>
</div>
<p>This code works to properly identify outliers:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">In</span> <span class="p">:</span> <span class="n">data</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">1000</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">6</span><span class="p">]</span>

<span class="n">In</span> <span class="p">:</span> <span class="n">find_outliers</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
<span class="n">Out</span><span class="p">:</span> <span class="p">[</span><span class="mi">3</span><span class="p">]</span>
</pre></div>
</div>
<p>However, it fails due to a division by zero if all of the values are equal:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">In</span> <span class="p">:</span> <span class="n">data</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span>

<span class="n">In</span> <span class="p">:</span> <span class="n">find_outliers</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
<span class="o">---------------------------------------------------------------------------</span>
<span class="ne">ZeroDivisionError</span>                         <span class="n">Traceback</span> <span class="p">(</span><span class="n">most</span> <span class="n">recent</span> <span class="n">call</span> <span class="n">last</span><span class="p">)</span>
<span class="n">Cell</span> <span class="n">In</span><span class="p">[</span><span class="mi">21</span><span class="p">],</span> <span class="n">line</span> <span class="mi">1</span>
<span class="o">----&gt;</span> <span class="mi">1</span> <span class="n">find_outliers</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>

<span class="n">Cell</span> <span class="n">In</span><span class="p">[</span><span class="mi">9</span><span class="p">],</span> <span class="n">line</span> <span class="mi">26</span><span class="p">,</span> <span class="ow">in</span> <span class="n">find_outliers</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">threshold</span><span class="p">)</span>
     <span class="mi">24</span> <span class="n">outliers</span> <span class="o">=</span> <span class="p">[]</span>
     <span class="mi">25</span> <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">value</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">data</span><span class="p">):</span>
<span class="o">---&gt;</span> <span class="mi">26</span>     <span class="n">z_score</span> <span class="o">=</span> <span class="nb">abs</span><span class="p">(</span><span class="n">value</span> <span class="o">-</span> <span class="n">mean</span><span class="p">)</span> <span class="o">/</span> <span class="n">std</span> 
     <span class="mi">27</span>     <span class="k">if</span> <span class="n">z_score</span> <span class="o">&gt;</span> <span class="n">threshold</span><span class="p">:</span>
     <span class="mi">28</span>         <span class="n">outliers</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">i</span><span class="p">)</span>

<span class="ne">ZeroDivisionError</span><span class="p">:</span> <span class="nb">float</span> <span class="n">division</span> <span class="n">by</span> <span class="n">zero</span>

</pre></div>
</div>
<p>Our intended behavior if all of the values are equal is to return an empty list, since there are by definition no outliers.
But before we do this, let’s create a couple of tests to check for the intended behavior and provide useful error messages if the test fails:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">test_find_outliers_normal_case</span><span class="p">():</span>
    <span class="n">data</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">100</span><span class="p">]</span>  <span class="c1"># 100 is clearly an outlier</span>
    <span class="n">outliers</span> <span class="o">=</span> <span class="n">find_outliers</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">threshold</span><span class="o">=</span><span class="mf">2.0</span><span class="p">)</span>
    
    <span class="c1"># Should find the outlier at index 5</span>
    <span class="k">assert</span> <span class="mi">5</span> <span class="ow">in</span> <span class="n">outliers</span><span class="p">,</span> <span class="sa">f</span><span class="s2">&quot;Failed to detect outlier: </span><span class="si">{</span><span class="n">outliers</span><span class="si">}</span><span class="s2">&quot;</span>
    <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">outliers</span><span class="p">)</span> <span class="o">==</span> <span class="mi">1</span><span class="p">,</span> <span class="sa">f</span><span class="s1">&#39;Expected exactly one outlier, got: </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">outliers</span><span class="p">)</span><span class="si">}</span><span class="s1">&#39;</span>


<span class="k">def</span> <span class="nf">test_find_outliers_identical_values</span><span class="p">():</span>
    <span class="n">data</span> <span class="o">=</span> <span class="p">[</span><span class="mi">5</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">5</span><span class="p">]</span>  <span class="c1"># All identical values</span>
    
    <span class="n">outliers</span> <span class="o">=</span> <span class="n">find_outliers</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">threshold</span><span class="o">=</span><span class="mf">2.0</span><span class="p">)</span>
    <span class="k">assert</span> <span class="n">outliers</span> <span class="o">==</span> <span class="p">[],</span> <span class="sa">f</span><span class="s2">&quot;Expected no outliers for identical values, got </span><span class="si">{</span><span class="n">outliers</span><span class="si">}</span><span class="s2">&quot;</span>
</pre></div>
</div>
<p>Running this with the original function definition, we see that it fails:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span>❯ pytest src/BetterCodeBetterScience/bug_driven_testing.py
=========================== test session starts ===========================
collected 2 items

src/BetterCodeBetterScience/bug_driven_testing.py .F                [100%]

================================ FAILURES =================================
___________________ test_find_outliers_identical_values ___________________

    def test_find_outliers_identical_values():
        data = [5, 5, 5, 5, 5]  # All identical values

&gt;       outliers = find_outliers(data, threshold=2.0)

src/BetterCodeBetterScience/bug_driven_testing.py:50:
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _

data = [5, 5, 5, 5, 5], threshold = 2.0

    def find_outliers(data: List[float], threshold: float = 2.0) -&gt; List[int]:
        &quot;&quot;&quot;
        Find outliers in a dataset using z-score method.

        Parameters
        ----------
        data : List[float]
            List of numerical values.
        threshold : float, default=2.0
            Number of standard deviations from the mean to consider a value as an outlier.

        Returns
        -------
        List[int]
            List of indices of outliers in the data.
        &quot;&quot;&quot;

        mean = sum(data) / len(data)
        variance = sum((x - mean) ** 2 for x in data) / len(data)
        std = variance ** 0.5

        # Bug: division by zero when std is 0 (all values are identical)
        # This only happens when all data points are the same
        outliers = []
        for i, value in enumerate(data):
&gt;           z_score = abs(value - mean) / std  # Bug: std can be 0!
E           ZeroDivisionError: float division by zero

src/BetterCodeBetterScience/bug_driven_testing.py:31: ZeroDivisionError
========================= short test summary info =========================
FAILED src/BetterCodeBetterScience/bug_driven_testing.py::test_find_outliers_identical_values
 - ZeroDivisionError: float division by zero
======================= 1 failed, 1 passed in 0.10s =======================
</pre></div>
</div>
<p>We can now fix the code by returning an empty list if zero standard deviation is detected:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span>    <span class="o">...</span>
    <span class="k">if</span> <span class="n">std</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="c1"># If standard deviation is zero, all values are identical, so no outliers</span>
        <span class="k">return</span> <span class="p">[]</span>
</pre></div>
</div>
<p>Here we add a comment to explain the intention of the statement.
Running the tests now will show that the problem is fixed:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span>❯ pytest src/BetterCodeBetterScience/bug_driven_testing.py
=========================== test session starts ===========================
collected 2 items

src/BetterCodeBetterScience/bug_driven_testing.py ..                [100%]

============================ 2 passed in 0.08s ============================

</pre></div>
</div>
<p>Now we can continue coding with confidence that if we happen to accidentally reintroduce the bug, it will be caught.</p>
</section>
</section>
<section id="the-structure-of-a-good-test">
<h2><a class="toc-backref" href="#id14">The structure of a good test</a><a class="headerlink" href="#the-structure-of-a-good-test" title="Permalink to this heading">#</a></h2>
<p>A commonly used scheme for writing a test is “given/when/then”:</p>
<ul class="simple">
<li><p>given some particular situation as background</p></li>
<li><p>when something happens (such as a particular input)</p></li>
<li><p>then something else should happen (such as a particular output or exception)</p></li>
</ul>
<p>Importantly, a test should only test one thing at a time.
This doesn’t mean that the test should necessarily only test for one specific error at a time; rather, it means that the test should assess a specific situation (“given/when”), and then assess all of the possible outcomes that are necessary to ensure that the component functions properly (“then”).
You can see this in the test for zero standard deviation that we generated in the earlier example, which actually tested for two conditions (the intended value being present in the list, and the list having a length of one) that together define the condition that we are interested in testing for.</p>
<p>How do we test that the output of a function is correct given the input? There are different answers for different situations:</p>
<ul class="simple">
<li><p><em>commonly known answer</em>: Sometimes we possess inputs where the output is known.  For example, if we were creating a function that computes the circumference of a circle, then we know that the output for an input radius of 1 should be 2 * pi.  This is generally only the case for very simple functions.</p></li>
<li><p><em>reference implementation</em>: In other cases we may have a standard implementation of an algorithm that we can compare against.  While in general it’s not a good idea to reimplement code that already exists in a standard library, in come cases we may want to extend existing code but also check that the basic version still works as planned.</p></li>
<li><p><em>parallel implementation</em>: Some times we don’t have a reference implementation, but we can code up another parallel implementation to compare our code to.  It’s important that this isn’t just a copy of the code used in the function; in that case, it’s really not a test at all!</p></li>
<li><p><em>behavioral test</em>: Sometimes the best we can do is to run the code repeatedly and ensure that it behaves as expected on average.  For example, if a function outputs a numerical value and we know the expected distribution of that value given a particular input, we can ensure that the result matches that distribution with a high probability.  Such <em>probabilistic tests</em> are not optimal in the sense that they can occasionally fail even when the code is correct, but they are sometimes the best we can do.</p></li>
</ul>
<section id="test-against-the-interface-not-the-implementation">
<h3><a class="toc-backref" href="#id15">Test against the interface, not the implementation</a><a class="headerlink" href="#test-against-the-interface-not-the-implementation" title="Permalink to this heading">#</a></h3>
<p>A good test shouldn’t know about the internal implementation details of the function that it is testing, and changes in the internal code that do not modify the input-output relationship should not affect the test.
That is, from the standpoint of the test, a function should be a “black box”.</p>
<p>The most common way in which a test can violate this principle is by accessing the internal variables of a class that it is testing.
For example, we might generate a class that performs a scaling operation on a numpy matrix:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">SimpleScaler</span><span class="p">:</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">transformed_</span> <span class="o">=</span> <span class="kc">None</span>

    <span class="k">def</span> <span class="nf">fit</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">mean_</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">std_</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">transform</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">transformed_</span> <span class="o">=</span> <span class="p">(</span><span class="n">X</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">mean_</span><span class="p">)</span> <span class="o">/</span> <span class="bp">self</span><span class="o">.</span><span class="n">std_</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">transformed_</span>

    <span class="k">def</span> <span class="nf">fit_transform</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
</pre></div>
</div>
<p>We could write a test that checks the values returned by the <code class="docutils literal notranslate"><span class="pre">fit_transform()</span></code> method, treating the the class as a black box:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">test_simple_scaler_interface</span><span class="p">():</span>
    <span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">],</span> <span class="p">[</span><span class="mi">5</span><span class="p">,</span> <span class="mi">6</span><span class="p">]])</span>
    <span class="n">scaler</span> <span class="o">=</span> <span class="n">SimpleScaler</span><span class="p">()</span>
    
    <span class="c1"># Test the interface without accessing internals</span>
    <span class="n">transformed_X</span> <span class="o">=</span> <span class="n">scaler</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
    <span class="k">assert</span> <span class="n">np</span><span class="o">.</span><span class="n">allclose</span><span class="p">(</span><span class="n">transformed_X</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">]))</span>
    <span class="k">assert</span> <span class="n">np</span><span class="o">.</span><span class="n">allclose</span><span class="p">(</span><span class="n">transformed_X</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">]))</span>
</pre></div>
</div>
<p>Alternatively one might use knowledge of the internals of the class to test the transformed value:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">test_simple_scaler_internals</span><span class="p">():</span>

    <span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">],</span> <span class="p">[</span><span class="mi">5</span><span class="p">,</span> <span class="mi">6</span><span class="p">]])</span>
    <span class="n">scaler</span> <span class="o">=</span> <span class="n">SimpleScaler</span><span class="p">()</span>
    <span class="n">_</span> <span class="o">=</span> <span class="n">scaler</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
    
    <span class="c1"># Test that the transformed data is correct using the internal</span>
    <span class="k">assert</span> <span class="n">np</span><span class="o">.</span><span class="n">allclose</span><span class="p">(</span><span class="n">scaler</span><span class="o">.</span><span class="n">transformed_</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">]))</span>
    <span class="k">assert</span> <span class="n">np</span><span class="o">.</span><span class="n">allclose</span><span class="p">(</span><span class="n">scaler</span><span class="o">.</span><span class="n">transformed_</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">]))</span>

</pre></div>
</div>
<p>Both of these tests pass against the class definition shown above.
However, if we were to change the way that the transformation is performed (for example, we decide to use the <code class="docutils literal notranslate"><span class="pre">StandardScaler</span></code> function from <code class="docutils literal notranslate"><span class="pre">scikit-learn</span></code> instead of writing our own), then the implementation-aware tests are likely to fail unless the sample internal variable names are used.
In general we should only interact with a function or class via its explicit interfaces.</p>
</section>
<section id="tests-should-be-independent">
<h3><a class="toc-backref" href="#id16">Tests should be independent</a><a class="headerlink" href="#tests-should-be-independent" title="Permalink to this heading">#</a></h3>
<p>In scientific computing it’s common to compose many different operations into a workflow.
If we want to test the workflow, then the tests of later steps in the workflow must necessarily rely upon earlier steps.
We could in theory write a set of tests that operate on a shared object, but the tests would fail if executed in an incorrect order, even if the code was correct.
Similarly, a failure on an early test would cause cascading failures in later tests, even if their code was correct.
The use of ordered tests also prevents the parallel execution of tests, which may slow down testing for complex projects.
For these reasons, we should always aim to create tests that can be executed independently.</p>
<p>Here is an example where coupling between tests could cause failures.
First we generate two functions that make changes in place to a data frame:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">split_names</span><span class="p">(</span><span class="n">df</span><span class="p">):</span>
    <span class="n">df</span><span class="p">[</span><span class="s1">&#39;firstname&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="s1">&#39;name&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">x</span><span class="o">.</span><span class="n">split</span><span class="p">()[</span><span class="mi">0</span><span class="p">])</span>
    <span class="n">df</span><span class="p">[</span><span class="s1">&#39;lastname&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="s1">&#39;name&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">x</span><span class="o">.</span><span class="n">split</span><span class="p">()[</span><span class="mi">1</span><span class="p">])</span>

<span class="k">def</span> <span class="nf">get_initials</span><span class="p">(</span><span class="n">df</span><span class="p">):</span>
    <span class="n">df</span><span class="p">[</span><span class="s1">&#39;initials&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="s1">&#39;firstname&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">str</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">+</span> <span class="n">df</span><span class="p">[</span><span class="s1">&#39;lastname&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">str</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

</pre></div>
</div>
<p>In this case, the <code class="docutils literal notranslate"><span class="pre">get_initials()</span></code> function relies upon the <code class="docutils literal notranslate"><span class="pre">split_names()</span></code> function having been run, since otherwise the necessary columns won’t exist in the data frame.
We can then create tests for each of these, and a data frame that they can both use:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">people_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">({</span><span class="s1">&#39;name&#39;</span><span class="p">:</span> <span class="p">[</span><span class="s1">&#39;Alice Smith&#39;</span><span class="p">,</span> <span class="s1">&#39;Bob Howard&#39;</span><span class="p">,</span> <span class="s1">&#39;Charlie Ashe&#39;</span><span class="p">]})</span> 

<span class="k">def</span> <span class="nf">test_split_names</span><span class="p">():</span>
    <span class="n">split_names</span><span class="p">(</span><span class="n">people_df</span><span class="p">)</span>
    <span class="k">assert</span> <span class="n">people_df</span><span class="p">[</span><span class="s1">&#39;firstname&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">tolist</span><span class="p">()</span> <span class="o">==</span> <span class="p">[</span><span class="s1">&#39;Alice&#39;</span><span class="p">,</span> <span class="s1">&#39;Bob&#39;</span><span class="p">,</span> <span class="s1">&#39;Charlie&#39;</span><span class="p">]</span>
    <span class="k">assert</span> <span class="n">people_df</span><span class="p">[</span><span class="s1">&#39;lastname&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">tolist</span><span class="p">()</span> <span class="o">==</span> <span class="p">[</span><span class="s1">&#39;Smith&#39;</span><span class="p">,</span> <span class="s1">&#39;Howard&#39;</span><span class="p">,</span> <span class="s1">&#39;Ashe&#39;</span><span class="p">]</span>

<span class="k">def</span> <span class="nf">test_get_initials</span><span class="p">():</span>
    <span class="n">get_initials</span><span class="p">(</span><span class="n">people_df</span><span class="p">)</span>
    <span class="k">assert</span> <span class="n">people_df</span><span class="p">[</span><span class="s1">&#39;initials&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">tolist</span><span class="p">()</span> <span class="o">==</span> <span class="p">[</span><span class="s1">&#39;AS&#39;</span><span class="p">,</span> <span class="s1">&#39;BH&#39;</span><span class="p">,</span> <span class="s1">&#39;CA&#39;</span><span class="p">]</span>
</pre></div>
</div>
<p>These tests run correctly, but the same tests fail if we change their order such that <code class="docutils literal notranslate"><span class="pre">test_get_intials()</span></code> runs first, because the necessary columns (<code class="docutils literal notranslate"><span class="pre">firstname</span></code> and <code class="docutils literal notranslate"><span class="pre">lastname</span></code>) have not yet been created.</p>
<p>One simple way to deal with this is to set up all of the necessary structure locally within each test:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span>
<span class="k">def</span> <span class="nf">get_people_df</span><span class="p">():</span>
    <span class="k">return</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">({</span><span class="s1">&#39;name&#39;</span><span class="p">:</span> <span class="p">[</span><span class="s1">&#39;Alice Smith&#39;</span><span class="p">,</span> <span class="s1">&#39;Bob Howard&#39;</span><span class="p">,</span> <span class="s1">&#39;Charlie Ashe&#39;</span><span class="p">]})</span> 

<span class="k">def</span> <span class="nf">test_split_names_fullsetup</span><span class="p">():</span>
    <span class="n">local_people_df</span> <span class="o">=</span> <span class="n">get_people_df</span><span class="p">()</span>
    <span class="n">split_names</span><span class="p">(</span><span class="n">local_people_df</span><span class="p">)</span>
    <span class="k">assert</span> <span class="n">local_people_df</span><span class="p">[</span><span class="s1">&#39;firstname&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">tolist</span><span class="p">()</span> <span class="o">==</span> <span class="p">[</span><span class="s1">&#39;Alice&#39;</span><span class="p">,</span> <span class="s1">&#39;Bob&#39;</span><span class="p">,</span> <span class="s1">&#39;Charlie&#39;</span><span class="p">]</span>
    <span class="k">assert</span> <span class="n">local_people_df</span><span class="p">[</span><span class="s1">&#39;lastname&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">tolist</span><span class="p">()</span> <span class="o">==</span> <span class="p">[</span><span class="s1">&#39;Smith&#39;</span><span class="p">,</span> <span class="s1">&#39;Howard&#39;</span><span class="p">,</span> <span class="s1">&#39;Ashe&#39;</span><span class="p">]</span>

<span class="k">def</span> <span class="nf">test_get_initials_fullsetup</span><span class="p">():</span>
    <span class="n">local_people_df</span> <span class="o">=</span> <span class="n">get_people_df</span><span class="p">()</span>
    <span class="n">split_names</span><span class="p">(</span><span class="n">local_people_df</span><span class="p">)</span>
    <span class="n">get_initials</span><span class="p">(</span><span class="n">local_people_df</span><span class="p">)</span>
    <span class="k">assert</span> <span class="n">local_people_df</span><span class="p">[</span><span class="s1">&#39;initials&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">tolist</span><span class="p">()</span> <span class="o">==</span> <span class="p">[</span><span class="s1">&#39;AS&#39;</span><span class="p">,</span> <span class="s1">&#39;BH&#39;</span><span class="p">,</span> <span class="s1">&#39;CA&#39;</span><span class="p">]</span>
</pre></div>
</div>
<p>For simple functions like these this would not cause too much computational overhead, but for computationally intensive functions we would like to be able to reuse the results from the first time each function is run.
In a later section we will discuss the use of <em>fixtures</em> which allow this kind of reuse across tests while avoiding the ordering problems that we saw above when using a global variable across tests.</p>
</section>
</section>
<section id="testing-frameworks">
<h2><a class="toc-backref" href="#id17">Testing frameworks</a><a class="headerlink" href="#testing-frameworks" title="Permalink to this heading">#</a></h2>
<p>One could write tests without the help of any specialized packages, but we generally use a testing framework to help automate the activities of testing.
There are several testing frameworks for Python; we will focus on the popular and flexible <code class="docutils literal notranslate"><span class="pre">pytest</span></code> framework.</p>
<p>We will start with a very simple example: a function that generates the Euclidean distance between two points.
Copilot generates the following for us based on the prompt in the comment:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># generate a function that calculates the distance between two points</span>
<span class="c1"># where each point is defined as a tuple of two numbers</span>

<span class="kn">import</span> <span class="nn">math</span>

<span class="k">def</span> <span class="nf">distance</span><span class="p">(</span><span class="n">p1</span><span class="p">,</span> <span class="n">p2</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Calculate the distance between two points&quot;&quot;&quot;</span>
    <span class="n">x1</span><span class="p">,</span> <span class="n">y1</span> <span class="o">=</span> <span class="n">p1</span>
    <span class="n">x2</span><span class="p">,</span> <span class="n">y2</span> <span class="o">=</span> <span class="n">p2</span>
    <span class="k">return</span> <span class="n">math</span><span class="o">.</span><span class="n">sqrt</span><span class="p">((</span><span class="n">x2</span> <span class="o">-</span> <span class="n">x1</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span> <span class="o">+</span> <span class="p">(</span><span class="n">y2</span> <span class="o">-</span> <span class="n">y1</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span>
</pre></div>
</div>
<p>Now we would like to generate some tests for this code to make sure that it works properly.
If we ask Copilot to generate some tests, it does a seeming decent job:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">test_distance_zero</span><span class="p">():</span>
    <span class="k">assert</span> <span class="n">distance</span><span class="p">((</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">),</span> <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">))</span> <span class="o">==</span> <span class="mi">0</span>

<span class="k">def</span> <span class="nf">test_distance_positive_coordinates</span><span class="p">():</span>
    <span class="k">assert</span> <span class="n">distance</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span> <span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span> <span class="o">==</span> <span class="mi">5</span>

<span class="k">def</span> <span class="nf">test_distance_negative_coordinates</span><span class="p">():</span>
    <span class="k">assert</span> <span class="n">distance</span><span class="p">((</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">2</span><span class="p">),</span> <span class="p">(</span><span class="o">-</span><span class="mi">4</span><span class="p">,</span> <span class="o">-</span><span class="mi">6</span><span class="p">))</span> <span class="o">==</span> <span class="mi">5</span>

<span class="k">def</span> <span class="nf">test_distance_mixed_coordinates</span><span class="p">():</span>
    <span class="k">assert</span> <span class="n">distance</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">2</span><span class="p">),</span> <span class="p">(</span><span class="o">-</span><span class="mi">4</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span> <span class="o">==</span> <span class="n">math</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="mi">125</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">test_distance_same_x</span><span class="p">():</span>
    <span class="k">assert</span> <span class="n">distance</span><span class="p">((</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">),</span> <span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">8</span><span class="p">))</span> <span class="o">==</span> <span class="mi">4</span>

<span class="k">def</span> <span class="nf">test_distance_same_y</span><span class="p">():</span>
    <span class="k">assert</span> <span class="n">distance</span><span class="p">((</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">),</span> <span class="p">(</span><span class="mi">7</span><span class="p">,</span> <span class="mi">4</span><span class="p">))</span> <span class="o">==</span> <span class="mi">4</span>
</pre></div>
</div>
<p>Now that we have our tests, we can run them using the <code class="docutils literal notranslate"><span class="pre">pytest</span></code> command:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>pytest<span class="w"> </span>src/BetterCodeBetterScience/distance_testing
</pre></div>
</div>
<p>This command will cause pytest to search (by default) for any files named <code class="docutils literal notranslate"><span class="pre">test_*.py</span></code> or <code class="docutils literal notranslate"><span class="pre">*_test.py</span></code> in the relevant path, and the select any functions whose name starts with the prefix “test”.
Running those tests, we get an error:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>&gt;<span class="w">       </span>assert<span class="w"> </span>distance<span class="o">((</span><span class="m">1</span>,<span class="w"> </span>-2<span class="o">)</span>,<span class="w"> </span><span class="o">(</span>-4,<span class="w"> </span><span class="m">6</span><span class="o">))</span><span class="w"> </span><span class="o">==</span><span class="w"> </span>math.sqrt<span class="o">(</span><span class="m">125</span><span class="o">)</span>
E<span class="w">       </span>assert<span class="w"> </span><span class="m">9</span>.433981132056603<span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="m">11</span>.180339887498949
E<span class="w">        </span>+<span class="w">  </span>where<span class="w"> </span><span class="m">9</span>.433981132056603<span class="w"> </span><span class="o">=</span><span class="w"> </span>distance<span class="o">((</span><span class="m">1</span>,<span class="w"> </span>-2<span class="o">)</span>,<span class="w"> </span><span class="o">(</span>-4,<span class="w"> </span><span class="m">6</span><span class="o">))</span>
E<span class="w">        </span>+<span class="w">  </span>and<span class="w">   </span><span class="m">11</span>.180339887498949<span class="w"> </span><span class="o">=</span><span class="w"> </span>&lt;built-in<span class="w"> </span><span class="k">function</span><span class="w"> </span>sqrt&gt;<span class="o">(</span><span class="m">125</span><span class="o">)</span>
E<span class="w">        </span>+<span class="w">    </span>where<span class="w"> </span>&lt;built-in<span class="w"> </span><span class="k">function</span><span class="w"> </span>sqrt&gt;<span class="w"> </span><span class="o">=</span><span class="w"> </span>math.sqrt
</pre></div>
</div>
<p>Here we see that the value returned by our function is different from the one expected by the test; in this case, the test value generated by Copilot is incorrect.
In our research, it was not uncommon for ChatGPT to generate incorrect test values, so these must always be checked by a domain expert.
Once we fix the expected value for that test (the square root of 89), then we can rerun the tests and see that they have passed:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>python<span class="w"> </span>-m<span class="w"> </span>pytest<span class="w"> </span>pytest<span class="w"> </span>src/BetterCodeBetterScience/distance_testing
<span class="o">====================</span><span class="w"> </span><span class="nb">test</span><span class="w"> </span>session<span class="w"> </span><span class="nv">starts</span><span class="w"> </span><span class="o">=====================</span><span class="w">                                     </span>

src/codingforscience/simple_testing/test_distance.py<span class="w"> </span>.<span class="w"> </span><span class="o">[</span><span class="w"> </span><span class="m">16</span>%<span class="o">]</span>
.....<span class="w">                                                  </span><span class="o">[</span><span class="m">100</span>%<span class="o">]</span>

<span class="o">=====================</span><span class="w"> </span><span class="m">6</span><span class="w"> </span>passed<span class="w"> </span><span class="k">in</span><span class="w"> </span><span class="m">0</span>.06s<span class="w"> </span><span class="o">======================</span>
</pre></div>
</div>
<section id="potential-problems-with-ai-generated-tests">
<h3><a class="toc-backref" href="#id18">Potential problems with AI-generated tests</a><a class="headerlink" href="#potential-problems-with-ai-generated-tests" title="Permalink to this heading">#</a></h3>
<p>If we are going to rely upon AI tools to generate our tests, we need to be sure that the tests are correct.
One of my early forays into AI-driven test generation uncovered an interesting example of how this can go wrong.</p>
<p>In our early project that examined the performance of GPT-4 for coding <span id="id2">[<a class="reference internal" href="bibliography.html#id18" title="Russell A Poldrack, Thomas Lu, and Gašper Beguš. Ai-assisted coding: experiments with gpt-4. 2023. URL: https://arxiv.org/abs/2304.13187, arXiv:2304.13187.">Poldrack <em>et al.</em>, 2023</a>]</span>, one of the analyses that we performed first asked GPT-4 to do was to generate a set of functions related to common problems in several scientific domains, and then to generate tests to make sure that the function performed correctly.
One of the functions that was generated was the escape velocity function shown above, for which GPT-4 generated the <a class="reference external" href="https://github.com/poldrack/ai-coding-experiments/blob/main/data/conceptual_prompting/testdirs/conceptual_prompting06/test_answer.py">following test</a>:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">test_escape_velocity</span><span class="p">():</span>

    <span class="n">mass_earth</span> <span class="o">=</span> <span class="mf">5.972e24</span>
    <span class="n">radius_earth</span> <span class="o">=</span> <span class="mf">6.371e6</span>
    <span class="n">result</span> <span class="o">=</span> <span class="n">escape_velocity</span><span class="p">(</span><span class="n">mass_earth</span><span class="p">,</span> <span class="n">radius_earth</span><span class="p">)</span>
    <span class="k">assert</span> <span class="n">pytest</span><span class="o">.</span><span class="n">approx</span><span class="p">(</span><span class="n">result</span><span class="p">,</span> <span class="n">rel</span><span class="o">=</span><span class="mf">1e-3</span><span class="p">)</span> <span class="o">==</span> <span class="mf">11186.25</span>

    <span class="n">mass_mars</span> <span class="o">=</span> <span class="mf">6.4171e23</span>
    <span class="n">radius_mars</span> <span class="o">=</span> <span class="mf">3.3895e6</span>
    <span class="n">result</span> <span class="o">=</span> <span class="n">escape_velocity</span><span class="p">(</span><span class="n">mass_mars</span><span class="p">,</span> <span class="n">radius_mars</span><span class="p">)</span>
    <span class="k">assert</span> <span class="n">pytest</span><span class="o">.</span><span class="n">approx</span><span class="p">(</span><span class="n">result</span><span class="p">,</span> <span class="n">rel</span><span class="o">=</span><span class="mf">1e-3</span><span class="p">)</span> <span class="o">==</span> <span class="mf">5027.34</span>

    <span class="n">mass_jupiter</span> <span class="o">=</span> <span class="mf">1.8982e27</span>
    <span class="n">radius_jupiter</span> <span class="o">=</span> <span class="mf">6.9911e7</span>
    <span class="n">result</span> <span class="o">=</span> <span class="n">escape_velocity</span><span class="p">(</span><span class="n">mass_jupiter</span><span class="p">,</span> <span class="n">radius_jupiter</span><span class="p">)</span>
    <span class="k">assert</span> <span class="n">pytest</span><span class="o">.</span><span class="n">approx</span><span class="p">(</span><span class="n">result</span><span class="p">,</span> <span class="n">rel</span><span class="o">=</span><span class="mf">1e-3</span><span class="p">)</span> <span class="o">==</span> <span class="mf">59564.97</span>
</pre></div>
</div>
<p>When we run this test (renaming it <code class="docutils literal notranslate"><span class="pre">test_escape_velocity_gpt4</span></code>), we see that one of the tests fails:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>❯<span class="w"> </span>pytest<span class="w"> </span>src/BetterCodeBetterScience/escape_velocity.py::test_escape_velocity_gpt4
<span class="o">====================================</span><span class="w"> </span><span class="nb">test</span><span class="w"> </span>session<span class="w"> </span><span class="nv">starts</span><span class="w"> </span><span class="o">====================================</span>
platform<span class="w"> </span>darwin<span class="w"> </span>--<span class="w"> </span>Python<span class="w"> </span><span class="m">3</span>.12.0,<span class="w"> </span>pytest-8.4.1,<span class="w"> </span>pluggy-1.5.0
rootdir:<span class="w"> </span>/Users/poldrack/Dropbox/code/BetterCodeBetterScience
configfile:<span class="w"> </span>pyproject.toml
plugins:<span class="w"> </span>cov-5.0.0,<span class="w"> </span>anyio-4.6.0,<span class="w"> </span>hypothesis-6.115.3,<span class="w"> </span>mock-3.14.0
collected<span class="w"> </span><span class="m">1</span><span class="w"> </span>item

src/BetterCodeBetterScience/escape_velocity.py<span class="w"> </span>F<span class="w">                                      </span><span class="o">[</span><span class="m">100</span>%<span class="o">]</span>

<span class="o">=========================================</span><span class="w"> </span><span class="nv">FAILURES</span><span class="w"> </span><span class="o">==========================================</span>
_________________________________<span class="w"> </span>test_escape_velocity_gpt4<span class="w"> </span>_________________________________

<span class="w">    </span>def<span class="w"> </span>test_escape_velocity_gpt4<span class="o">()</span>:

<span class="w">        </span><span class="nv">mass_earth</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">5</span>.972e24
<span class="w">        </span><span class="nv">radius_earth</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">6</span>.371e6
<span class="w">        </span><span class="nv">result</span><span class="w"> </span><span class="o">=</span><span class="w"> </span>escape_velocity<span class="o">(</span>mass_earth,<span class="w"> </span>radius_earth<span class="o">)</span>
<span class="w">        </span>assert<span class="w"> </span>pytest.approx<span class="o">(</span>result,<span class="w"> </span><span class="nv">rel</span><span class="o">=</span>1e-3<span class="o">)</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="m">11186</span>.25

<span class="w">        </span><span class="nv">mass_mars</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">6</span>.4171e23
<span class="w">        </span><span class="nv">radius_mars</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">3</span>.3895e6
<span class="w">        </span><span class="nv">result</span><span class="w"> </span><span class="o">=</span><span class="w"> </span>escape_velocity<span class="o">(</span>mass_mars,<span class="w"> </span>radius_mars<span class="o">)</span>
<span class="w">        </span>assert<span class="w"> </span>pytest.approx<span class="o">(</span>result,<span class="w"> </span><span class="nv">rel</span><span class="o">=</span>1e-3<span class="o">)</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="m">5027</span>.34

<span class="w">        </span><span class="nv">mass_jupiter</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">1</span>.8982e27
<span class="w">        </span><span class="nv">radius_jupiter</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">6</span>.9911e7
<span class="w">        </span><span class="nv">result</span><span class="w"> </span><span class="o">=</span><span class="w"> </span>escape_velocity<span class="o">(</span>mass_jupiter,<span class="w"> </span>radius_jupiter<span class="o">)</span>
&gt;<span class="w">       </span>assert<span class="w"> </span>pytest.approx<span class="o">(</span>result,<span class="w"> </span><span class="nv">rel</span><span class="o">=</span>1e-3<span class="o">)</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="m">59564</span>.97
E<span class="w">       </span>assert<span class="w"> </span><span class="m">60202</span>.716344497014<span class="w"> </span>±<span class="w"> </span><span class="m">60</span>.2027<span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="m">59564</span>.97
E
E<span class="w">         </span>comparison<span class="w"> </span>failed
E<span class="w">         </span>Obtained:<span class="w"> </span><span class="m">59564</span>.97
E<span class="w">         </span>Expected:<span class="w"> </span><span class="m">60202</span>.716344497014<span class="w"> </span>±<span class="w"> </span><span class="m">60</span>.2027

src/BetterCodeBetterScience/escape_velocity.py:52:<span class="w"> </span><span class="nv">AssertionError</span>
<span class="o">==================================</span><span class="w"> </span>short<span class="w"> </span><span class="nb">test</span><span class="w"> </span>summary<span class="w"> </span><span class="nv">info</span><span class="w"> </span><span class="o">==================================</span>
FAILED<span class="w"> </span>src/BetterCodeBetterScience/escape_velocity.py::test_escape_velocity_gpt4<span class="w"> </span>-<span class="w"> </span>assert<span class="w"> </span><span class="m">60202</span>.716344497014<span class="w"> </span>±<span class="w"> </span><span class="m">60</span>.2027<span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="m">59564</span>.97
<span class="o">=====================================</span><span class="w"> </span><span class="m">1</span><span class="w"> </span>failed<span class="w"> </span><span class="k">in</span><span class="w"> </span><span class="m">0</span>.12s<span class="w"> </span><span class="o">=====================================</span>
</pre></div>
</div>
<p>It seems that the first two assertions pass but the third one, for Jupiter, fails.
This failure took a bit of digging to fully understand.
In this case, the code and test value are both correct, depending on where you stand on Jupiter! The problem is that planets are <em>oblate</em>, meaning that they are slightly flattened such that the radius around the equator is higher than at other points.
NASA’s <a class="reference external" href="https://nssdc.gsfc.nasa.gov/planetary/factsheet/jupiterfact.html">Jupiter fact sheet</a> claims an escape velocity of 59.5 km/s, which seems to be the source of the test value.
This is correct when computed using the equatorial radius of 71492 km.
However, the radius given for Jupiter in GPT-4’s test (69911 km) is the volumetric mean radius rather than the equatorial radius, and the value generated by the code (60.2 km/s) is correct when computed using the volumetric mean radius.
Thus, the test failed not due to any problems with the code itself, but due to a mismatch in assumptions regarding the combination of test values.
This example highlights the importance of understanding and checking the tests that are generated by AI coding tools.</p>
</section>
</section>
<section id="test-driven-development-and-ai-assisted-coding">
<h2><a class="toc-backref" href="#id19">Test-driven development and AI-assisted coding</a><a class="headerlink" href="#test-driven-development-and-ai-assisted-coding" title="Permalink to this heading">#</a></h2>
<p>Here we will dive into a more realistic example of an application that one might develop using AI assistance, specifically looking at how we could develop the application using a test-driven development (TDD) approach.
We will develop a Python application that takes in a query for the PubMed database and returns a data frame containing the number of database records matching that query for each year.
We start by decomposing the problem and sketching out the main set of functions that we will need to develop, with understandable names for each:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">get_PubmedIDs_for_query</span></code>: A function that will search pubmed for a given query and return a list of pubmed IDs</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">get_record_from_PubmedID</span></code>: A function that will retrieve the record for a given pubmed ID</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">parse_year_from_Pubmed_record</span></code>: A function that will parse a record to extract the year of publication</p></li>
<li><p>A function that will summarize the number of records per year</p></li>
<li><p>The main function that will take in a query and return a data frame with the number of records per year for the query</p></li>
</ul>
<p>We start by creating <code class="docutils literal notranslate"><span class="pre">get_PubmedIDs_for_query</span></code>.
We could use the <code class="docutils literal notranslate"><span class="pre">Biopython.Entrez</span></code> module to perform this search, but Biopython is a relatively large module that could introduce technical debt.
Instead, we will directly retrieve the result using the Entrez API and the built-in <code class="docutils literal notranslate"><span class="pre">requests</span></code> module.
Note that for all of the code shown here we will not include docstrings, but they are available in the code within the repository.</p>
<p>If we are using the TDD approach, we would first want to develop a set of tests to make sure that our function is working correctly.
The following three tests specify several different outcomes that we might expect.
First, we give a query that is known to give a valid result, and test whether it in fact gives such a result:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">test_get_PubmedIDs_for_query_check_valid</span><span class="p">():</span>
    <span class="n">query</span> <span class="o">=</span> <span class="s2">&quot;friston-k AND &#39;free energy&#39;&quot;</span>
    <span class="n">ids</span> <span class="o">=</span> <span class="n">get_PubmedIDs_for_query</span><span class="p">(</span><span class="n">query</span><span class="p">)</span>

    <span class="c1"># make sure that a list is returned</span>
    <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">ids</span><span class="p">,</span> <span class="nb">list</span><span class="p">)</span>       
    <span class="c1"># make sure the list is not empty</span>
    <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">ids</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">0</span>                 
</pre></div>
</div>
<p>Second, we give a query with a known empty result, and make sure it returns an empty list:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">test_get_PubmedIDs_for_query_check_empty</span><span class="p">():</span>
    <span class="n">query</span> <span class="o">=</span> <span class="s2">&quot;friston-k AND &#39;fizzbuzz&#39;&quot;</span>
    <span class="n">ids</span> <span class="o">=</span> <span class="n">get_PubmedIDs_for_query</span><span class="p">(</span><span class="n">query</span><span class="p">)</span>

    <span class="c1"># make sure that a list is returned</span>
    <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">ids</span><span class="p">,</span> <span class="nb">list</span><span class="p">)</span>   
    <span class="c1"># make sure the resulting list is empty</span>
    <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">ids</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span>
</pre></div>
</div>
<p>With the minimal tests in place, we then move to writing the code for the module.
We first create an empty function to ensure that the tests fail:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">get_PubmedIDs_for_query</span><span class="p">(</span><span class="n">query</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> 
                            <span class="n">retmax</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
                            <span class="n">esearch_url</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="kc">None</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">list</span><span class="p">:</span>
    <span class="k">return</span> <span class="kc">None</span>
</pre></div>
</div>
<p>The test result shows that all of the tests fail:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>❯<span class="w"> </span>python<span class="w"> </span>-m<span class="w"> </span>pytest<span class="w"> </span>-v<span class="w"> </span>tests/textmining
<span class="o">==================================</span><span class="w"> </span><span class="nb">test</span><span class="w"> </span>session<span class="w"> </span><span class="nv">starts</span><span class="w"> </span><span class="o">===================================</span>
...
tests/textmining/test_textmining.py::test_get_PubmedIDs_for_query_check_valid<span class="w"> </span>FAILED<span class="w"> </span><span class="o">[</span><span class="w"> </span><span class="m">50</span>%<span class="o">]</span>
tests/textmining/test_textmining.py::test_get_PubmedIDs_for_query_check_empty<span class="w"> </span>FAILED<span class="w"> </span><span class="o">[</span><span class="m">100</span>%<span class="o">]</span>

<span class="o">========================================</span><span class="w"> </span><span class="nv">FAILURES</span><span class="w"> </span><span class="o">========================================</span>
________________________<span class="w"> </span>test_get_PubmedIDs_for_query_check_valid<span class="w"> </span>________________________

<span class="nv">ids</span><span class="w"> </span><span class="o">=</span><span class="w"> </span>None

<span class="w">    </span>def<span class="w"> </span>test_get_PubmedIDs_for_query_check_valid<span class="o">(</span>ids<span class="o">)</span>:
&gt;<span class="w">       </span>assert<span class="w"> </span>isinstance<span class="o">(</span>ids,<span class="w"> </span>list<span class="o">)</span>
E<span class="w">       </span>assert<span class="w"> </span>False
E<span class="w">        </span>+<span class="w">  </span>where<span class="w"> </span><span class="nv">False</span><span class="w"> </span><span class="o">=</span><span class="w"> </span>isinstance<span class="o">(</span>None,<span class="w"> </span>list<span class="o">)</span>

tests/textmining/test_textmining.py:32:<span class="w"> </span>AssertionError
________________________<span class="w"> </span>test_get_PubmedIDs_for_query_check_empty<span class="w"> </span>________________________

<span class="w">    </span>def<span class="w"> </span>test_get_PubmedIDs_for_query_check_empty<span class="o">()</span>:
<span class="w">        </span><span class="nv">query</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">&quot;friston-k AND &#39;fizzbuzz&#39;&quot;</span>
<span class="w">        </span><span class="nv">ids</span><span class="w"> </span><span class="o">=</span><span class="w"> </span>get_PubmedIDs_for_query<span class="o">(</span>query<span class="o">)</span>
&gt;<span class="w">       </span>assert<span class="w"> </span>len<span class="o">(</span>ids<span class="o">)</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="m">0</span>
<span class="w">               </span>^^^^^^^^
E<span class="w">       </span>TypeError:<span class="w"> </span>object<span class="w"> </span>of<span class="w"> </span><span class="nb">type</span><span class="w"> </span><span class="s1">&#39;NoneType&#39;</span><span class="w"> </span>has<span class="w"> </span>no<span class="w"> </span>len<span class="o">()</span>

tests/textmining/test_textmining.py:39:<span class="w"> </span><span class="nv">TypeError</span>
<span class="o">================================</span><span class="w"> </span>short<span class="w"> </span><span class="nb">test</span><span class="w"> </span>summary<span class="w"> </span><span class="nv">info</span><span class="w"> </span><span class="o">=================================</span>
FAILED<span class="w"> </span>tests/textmining/test_textmining.py::test_get_PubmedIDs_for_query_check_valid<span class="w"> </span>-<span class="w"> </span>assert<span class="w"> </span>False
FAILED<span class="w"> </span>tests/textmining/test_textmining.py::test_get_PubmedIDs_for_query_check_empty<span class="w"> </span>-<span class="w"> </span>TypeError:<span class="w"> </span>object<span class="w"> </span>of<span class="w"> </span><span class="nb">type</span><span class="w"> </span><span class="s1">&#39;NoneType&#39;</span><span class="w"> </span>has<span class="w"> </span>no<span class="w"> </span>len<span class="o">()</span>
<span class="o">===================================</span><span class="w"> </span><span class="m">2</span><span class="w"> </span>failed<span class="w"> </span><span class="k">in</span><span class="w"> </span><span class="m">0</span>.12s<span class="w"> </span><span class="o">====================================</span>
</pre></div>
</div>
<p>Now we work with Copilot write the code to make the tests pass:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># define the eutils base URL globally for the module</span>
<span class="c1"># - not best practice but probably ok here</span>
<span class="n">BASE_URL</span> <span class="o">=</span> <span class="s2">&quot;https://eutils.ncbi.nlm.nih.gov/entrez/eutils&quot;</span>


<span class="k">def</span> <span class="nf">get_PubmedIDs_for_query</span><span class="p">(</span>
    <span class="n">query</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">retmax</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="n">esearch_url</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="kc">None</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">list</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Search database for a given query and return a list of IDs.</span>
<span class="sd">    :param query: str, the query to search for</span>
<span class="sd">    :param retmax: int, the maximum number of results to return</span>
<span class="sd">    :base_url: str, the base url for the pubmed search</span>
<span class="sd">    :return: list, a list of pubmed IDs</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># define the base url for the pubmed search</span>
    <span class="k">if</span> <span class="n">esearch_url</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">esearch_url</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">BASE_URL</span><span class="si">}</span><span class="s2">/esearch.fcgi&quot;</span>

    <span class="n">params</span> <span class="o">=</span> <span class="n">format_pubmed_query_params</span><span class="p">(</span><span class="n">query</span><span class="p">,</span> <span class="n">retmax</span><span class="o">=</span><span class="n">retmax</span><span class="p">)</span>

    <span class="n">response</span> <span class="o">=</span> <span class="n">requests</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">esearch_url</span><span class="p">,</span> <span class="n">params</span><span class="o">=</span><span class="n">params</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">get_idlist_from_response</span><span class="p">(</span><span class="n">response</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">format_pubmed_query_params</span><span class="p">(</span><span class="n">query</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">retmax</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">10000</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">str</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Format a query for use with the pubmed api.</span>
<span class="sd">    :param query: str, the query to format</span>
<span class="sd">    :return: dict, the formatted query dict</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="c1"># define the parameters for the search</span>
    <span class="k">return</span> <span class="p">{</span><span class="s2">&quot;db&quot;</span><span class="p">:</span> <span class="s2">&quot;pubmed&quot;</span><span class="p">,</span> <span class="s2">&quot;term&quot;</span><span class="p">:</span> <span class="n">query</span><span class="p">,</span> <span class="s2">&quot;retmode&quot;</span><span class="p">:</span> <span class="s2">&quot;json&quot;</span><span class="p">,</span> <span class="s2">&quot;retmax&quot;</span><span class="p">:</span> <span class="n">retmax</span><span class="p">}</span>


<span class="k">def</span> <span class="nf">get_idlist_from_response</span><span class="p">(</span><span class="n">response</span><span class="p">:</span> <span class="n">requests</span><span class="o">.</span><span class="n">Response</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">list</span><span class="p">:</span>
    <span class="k">if</span> <span class="n">response</span><span class="o">.</span><span class="n">status_code</span> <span class="o">==</span> <span class="mi">200</span><span class="p">:</span>
        <span class="c1"># extract the pubmed IDs from the response</span>
        <span class="n">ids</span> <span class="o">=</span> <span class="n">response</span><span class="o">.</span><span class="n">json</span><span class="p">()[</span><span class="s2">&quot;esearchresult&quot;</span><span class="p">][</span><span class="s2">&quot;idlist&quot;</span><span class="p">]</span>
        <span class="k">return</span> <span class="n">ids</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Bad request&quot;</span><span class="p">)</span>
</pre></div>
</div>
<p>Note that we have split parts of the functionality into separate functions in order to make the code more understandable.
Running the tests, we see that both of them pass.
Assuming that our tests cover all possible outcomes of interest, we can consider our function complete.
We can also add additional tests to cover additional functions that we generated; we won’t go into the details here, but you can see them on the Github repo.</p>
</section>
<section id="test-coverage">
<h2><a class="toc-backref" href="#id20">Test coverage</a><a class="headerlink" href="#test-coverage" title="Permalink to this heading">#</a></h2>
<p>It can be useful to know if there are any portions of our code that are not being exercised by our tests, which is known as <em>code coverage</em>.
The <code class="docutils literal notranslate"><span class="pre">pytest-cov</span></code> extension for the <code class="docutils literal notranslate"><span class="pre">pytest</span></code> testing package can provide us with a report of test coverage for these tests:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>----------<span class="w"> </span>coverage:<span class="w"> </span>platform<span class="w"> </span>darwin,<span class="w"> </span>python<span class="w"> </span><span class="m">3</span>.12.0-final-0<span class="w"> </span>----------
Name<span class="w">                                                   </span>Stmts<span class="w">   </span>Miss<span class="w">  </span>Cover<span class="w">   </span>Missing
------------------------------------------------------------------------------------
src/BetterCodeBetterScience/textmining/textmining.py<span class="w">      </span><span class="m">30</span><span class="w">      </span><span class="m">1</span><span class="w">    </span><span class="m">97</span>%<span class="w">   </span><span class="m">70</span>
------------------------------------------------------------------------------------
TOTAL<span class="w">                                                     </span><span class="m">30</span><span class="w">      </span><span class="m">1</span><span class="w">    </span><span class="m">97</span>%
</pre></div>
</div>
<p>This report shows that of the 30 statements in our code, one of them is not covered by the tests.
When we look at the missing code (denoted as being on line 70), we see that the missing line is this one from <code class="docutils literal notranslate"><span class="pre">get_idlist_from_response</span></code>:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span>    <span class="k">else</span><span class="p">:</span>
        <span class="c1"># raise an exception if the search didn&#39;t return a usable response</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Bad request&quot;</span><span class="p">)</span>
</pre></div>
</div>
<p>Since none of our test cases caused a bad request to occur, this line never gets executed in the tests.
We can address this by adding a test that makes sure that an exception is raised if an invalid base url is provided.
To check for an exception, we need to use the <code class="docutils literal notranslate"><span class="pre">pytest.raises</span></code> context manager:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">test_get_PubmedIDs_for_query_check_badurl</span><span class="p">():</span>
    <span class="n">query</span> <span class="o">=</span> <span class="s2">&quot;friston-k AND &#39;free energy&#39;&quot;</span>
    <span class="c1"># bad url</span>
    <span class="n">base_url</span> <span class="o">=</span> <span class="s1">&#39;https://eutils.ncbi.nlm.nih.gov/entrez/eutils/esearch.f&#39;</span>
    
    <span class="c1"># make sure that the function raises an exception</span>
    <span class="k">with</span> <span class="n">pytest</span><span class="o">.</span><span class="n">raises</span><span class="p">(</span><span class="ne">Exception</span><span class="p">):</span>
        <span class="n">ids</span> <span class="o">=</span> <span class="n">get_PubmedIDs_for_query</span><span class="p">(</span><span class="n">query</span><span class="p">,</span> <span class="n">base_url</span><span class="o">=</span><span class="n">base_url</span><span class="p">)</span>
    
</pre></div>
</div>
<p>After adding this test, we see that we now have 100% coverage.
It’s important not to get too hung up on test coverage; rather than always aspiring to 100% coverage, it’s important to make sure that the most likely possible situations are tested.
Just because you have 100% coverage doesn’t mean that your code is perfectly tested, since there could always be situations that you haven’t checked for.
And spending too much time testing for unlikely problems can divert your efforts from other most useful activities.</p>
</section>
<section id="test-fixtures">
<h2><a class="toc-backref" href="#id21">Test fixtures</a><a class="headerlink" href="#test-fixtures" title="Permalink to this heading">#</a></h2>
<p>Sometimes we need to use a the same data for multiple tests.
Rather than duplicating potentially time-consuming processes across each of the tests, it is often preferable to create a single instance of the object that can be used across multiple tests, which is known as a <em>test fixture</em>.
This also helps maintain isolation between tests, since the order of tests shouldn’t matter if an appropriate fixture is generated as soon as it’s needed.</p>
<p>For our example above, it’s likely that we will need to reuse the list of pubmed IDs from the search to perform various tests on the subsequent functions.
We can create a single version of this list of IDs by creating a fixture.
In the <code class="docutils literal notranslate"><span class="pre">pytest</span></code> framework we do this using a special Python operator called a <em>decorator</em>, which is denoted by the symbol <code class="docutils literal notranslate"><span class="pre">&#64;</span></code> as a prefix.
A decorator is function that takes another function as input, modifies its functionality, and returns another function; you don’t need to understand in detail how decorators work for this particular usage.
To refactor our tests above, we would first create the fixture by decorating the function that generates the fixture with the <code class="docutils literal notranslate"><span class="pre">&#64;pytest.fixture</span></code> decorator, setting the <code class="docutils literal notranslate"><span class="pre">scope</span></code> variable to “session” so that the fixture is only generated once within the session:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="nd">@pytest</span><span class="o">.</span><span class="n">fixture</span><span class="p">(</span><span class="n">scope</span><span class="o">=</span><span class="s2">&quot;session&quot;</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">ids</span><span class="p">():</span>
    <span class="n">query</span> <span class="o">=</span> <span class="s2">&quot;friston-k AND &#39;free energy&#39;&quot;</span>
    <span class="n">ids</span> <span class="o">=</span> <span class="n">get_PubmedIDs_for_query</span><span class="p">(</span><span class="n">query</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">ids</span>
</pre></div>
</div>
<p>We can then refactor our tests for a valid query to use the fixture by passing it as an argument to the test function:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">test_get_PubmedIDs_for_query_check_valid</span><span class="p">(</span><span class="n">ids</span><span class="p">):</span>
    <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">ids</span><span class="p">,</span> <span class="nb">list</span><span class="p">)</span>
    <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">ids</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">0</span>
</pre></div>
</div>
<p>The result is the same, but we now have a set of ids that we can reuse in subsequent tests, so that we don’t have to make repeated queries.
It’s important to note while using a session-scoped fixture: If any of the subsequent tests modify the fixture, those modifications will persist, which will break the isolation between tests.
We could prevent this by removing the <code class="docutils literal notranslate"><span class="pre">scope=&quot;session&quot;</span></code> argument, which would then default to the standard scope which is within a specific function.
If you wish to use session-scoped fixtures and need to modify them within the test function, then it is best to first create a copy of the fixture object (e.g. <code class="docutils literal notranslate"><span class="pre">my_ids</span> <span class="pre">=</span> <span class="pre">ids.copy()</span></code>) so that the global fixture object won’t be modified.</p>
</section>
<section id="mocking">
<h2><a class="toc-backref" href="#id22">Mocking</a><a class="headerlink" href="#mocking" title="Permalink to this heading">#</a></h2>
<p>Sometimes tests require infrastructure that is outside of the control of the tester.
In the example above, we are assuming that the Pubmed API is working correctly for our tests to run; if we were to try to run these tests without an internet connection, they would fail.
In other cases, code may rely upon a database system that may or may not exist on a particular system.
In these cases, we can create a mock object that can stand in for and simulate the behavior of the system that the code needs to interact with.</p>
<p>In our example, we want to create a mock response that looks sufficiently like a response from the real API to pass our tests.
Using pytest’s <em>monkeypatch</em> fixture, we can temporarily replace the real requests.get function with our own fake function that returns a predictable, controlled response.
We first need to create a class that can replace the <code class="docutils literal notranslate"><span class="pre">requests.get</span></code> call in <code class="docutils literal notranslate"><span class="pre">get_PubmedIDs_for_query</span></code>, replacing it with a mock version that outputs a fixed simulacrum of an API response via its <code class="docutils literal notranslate"><span class="pre">.json()</span></code> method.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">MockPubmedResponse</span><span class="p">:</span>
    <span class="n">status_code</span> <span class="o">=</span> <span class="mi">200</span>

    <span class="k">def</span> <span class="nf">json</span><span class="p">():</span>
        <span class="k">return</span> <span class="p">{</span>
            <span class="s1">&#39;header&#39;</span><span class="p">:</span> <span class="p">{</span><span class="s1">&#39;type&#39;</span><span class="p">:</span> <span class="s1">&#39;esearch&#39;</span><span class="p">,</span> <span class="s1">&#39;version&#39;</span><span class="p">:</span> <span class="s1">&#39;0.3&#39;</span><span class="p">},</span>
            <span class="s1">&#39;esearchresult&#39;</span><span class="p">:</span> <span class="p">{</span>
                <span class="s1">&#39;count&#39;</span><span class="p">:</span> <span class="s1">&#39;2&#39;</span><span class="p">,</span>
                <span class="s1">&#39;retmax&#39;</span><span class="p">:</span> <span class="s1">&#39;20&#39;</span><span class="p">,</span>
                <span class="s1">&#39;retstart&#39;</span><span class="p">:</span> <span class="s1">&#39;0&#39;</span><span class="p">,</span>
                <span class="s1">&#39;idlist&#39;</span><span class="p">:</span> <span class="p">[</span><span class="s1">&#39;39312494&#39;</span><span class="p">,</span> <span class="s1">&#39;39089179&#39;</span><span class="p">]</span>
            <span class="p">}</span>
        <span class="p">}</span>
</pre></div>
</div>
<p>We now insert this mock response for the standard <code class="docutils literal notranslate"><span class="pre">requests.get</span></code> call within the test.
In my initial attempt, I created created a fixture based on the mocked response and then tested that fixture:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="nd">@pytest</span><span class="o">.</span><span class="n">fixture</span>
<span class="k">def</span> <span class="nf">ids_mocked</span><span class="p">(</span><span class="n">monkeypatch</span><span class="p">):</span>

    <span class="k">def</span> <span class="nf">mock_get</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">MockPubmedResponse</span><span class="p">()</span>

    <span class="c1"># apply the monkeypatch for requests.get to mock_get</span>
    <span class="n">monkeypatch</span><span class="o">.</span><span class="n">setattr</span><span class="p">(</span><span class="n">requests</span><span class="p">,</span> <span class="s2">&quot;get&quot;</span><span class="p">,</span> <span class="n">mock_get</span><span class="p">)</span>

    <span class="n">query</span> <span class="o">=</span> <span class="s2">&quot;friston-k AND &#39;free energy&#39;&quot;</span>
    <span class="n">ids</span> <span class="o">=</span> <span class="n">get_PubmedIDs_for_query</span><span class="p">(</span><span class="n">query</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">ids</span>

<span class="k">def</span> <span class="nf">test_get_PubmedIDs_for_query_check_valid_mocked</span><span class="p">(</span><span class="n">ids_mocked</span><span class="p">):</span>
    <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">ids_mocked</span><span class="p">,</span> <span class="nb">list</span><span class="p">)</span>
    <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">ids_mocked</span><span class="p">)</span> <span class="o">==</span> <span class="mi">2</span>

</pre></div>
</div>
<p>Turning off my network connection shows that the mocked test passes, while the tests that require connecting to the actual API fail.
However, my usual code review (using Google’s Gemini 2.5 Pro) identified a problem with this fixture: it conflates the setup (creating the mock API) with the execution of the function that uses the mock API.
A better approach (recommended by Gemini) is move the function execution out of the fixture and into the test:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Fixture ONLY does the setup (the mocking)</span>
<span class="nd">@pytest</span><span class="o">.</span><span class="n">fixture</span>
<span class="k">def</span> <span class="nf">mock_pubmed_api</span><span class="p">(</span><span class="n">monkeypatch</span><span class="p">):</span>

    <span class="k">class</span> <span class="nc">MockPubmedResponse</span><span class="p">:</span>
        <span class="n">status_code</span> <span class="o">=</span> <span class="mi">200</span>
        <span class="k">def</span> <span class="nf">json</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
            <span class="k">return</span> <span class="p">{</span>
                <span class="s1">&#39;header&#39;</span><span class="p">:</span> <span class="p">{</span><span class="s1">&#39;type&#39;</span><span class="p">:</span> <span class="s1">&#39;esearch&#39;</span><span class="p">,</span> <span class="s1">&#39;version&#39;</span><span class="p">:</span> <span class="s1">&#39;0.3&#39;</span><span class="p">},</span>
                <span class="s1">&#39;esearchresult&#39;</span><span class="p">:</span> <span class="p">{</span>
                    <span class="s1">&#39;count&#39;</span><span class="p">:</span> <span class="s1">&#39;2&#39;</span><span class="p">,</span>
                    <span class="s1">&#39;retmax&#39;</span><span class="p">:</span> <span class="s1">&#39;20&#39;</span><span class="p">,</span>
                    <span class="s1">&#39;retstart&#39;</span><span class="p">:</span> <span class="s1">&#39;0&#39;</span><span class="p">,</span>
                    <span class="s1">&#39;idlist&#39;</span><span class="p">:</span> <span class="p">[</span><span class="s1">&#39;39312494&#39;</span><span class="p">,</span> <span class="s1">&#39;39089179&#39;</span><span class="p">]</span>
                <span class="p">}</span>
            <span class="p">}</span>

    <span class="k">def</span> <span class="nf">mock_get</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">MockPubmedResponse</span><span class="p">()</span>

    <span class="c1"># Apply the monkeypatch for requests.get to mock_get</span>
    <span class="n">monkeypatch</span><span class="o">.</span><span class="n">setattr</span><span class="p">(</span><span class="n">requests</span><span class="p">,</span> <span class="s2">&quot;get&quot;</span><span class="p">,</span> <span class="n">mock_get</span><span class="p">)</span>

<span class="c1"># The test requests the setup, then performs the action and assertion.</span>
<span class="k">def</span> <span class="nf">test_get_PubmedIDs_for_query_check_valid_mocked</span><span class="p">(</span><span class="n">mock_pubmed_api</span><span class="p">):</span>
    <span class="c1"># Action: Call the function under test</span>
    <span class="n">query</span> <span class="o">=</span> <span class="s2">&quot;friston-k AND &#39;free energy&#39;&quot;</span>
    <span class="n">ids</span> <span class="o">=</span> <span class="n">get_PubmedIDs_for_query</span><span class="p">(</span><span class="n">query</span><span class="p">)</span>

    <span class="c1"># Assertion: Check the result</span>
    <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">ids</span><span class="p">,</span> <span class="nb">list</span><span class="p">)</span>
    <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">ids</span><span class="p">)</span> <span class="o">==</span> <span class="mi">2</span>
</pre></div>
</div>
<p>Note that while mocking can be useful for testing specific components by saving time and increasing robustness, integration tests and smoke tests should usually be run without mocking, in order to catch any errors that arise through interaction with the relevant components that are being mocked.
In fact, it’s always a good idea to have tests that specifically assess the usage of the external service and the system’s response to failures in that service (e.g. by using features of the testing framework that allow one to shut down access to the network).</p>
</section>
<section id="parametrized-tests">
<h2><a class="toc-backref" href="#id23">Parametrized tests</a><a class="headerlink" href="#parametrized-tests" title="Permalink to this heading">#</a></h2>
<p>Often a function needs to accept a range of inputs that can result in different behavior, and we want to test each of the possible inputs to ensure that the function works correctly across the range.
When the different inputs are known, one way to achieve this is to use a <em>parameterized test</em>, in which the test is repeatedly run across combinations of different possible values.</p>
<p>For our example, let’s move forward and develop the function <code class="docutils literal notranslate"><span class="pre">parse_year_from_Pubmed_record</span></code> to extract the year from Pubmed records, which can differ in their structure.
We first need to develop the function <code class="docutils literal notranslate"><span class="pre">get_record_from_PubmedID</span></code> to retrieve a record based on a Pubmed ID.
Following our TDD approach, we first develop two simple tests: one to ensure that it returns a non-empty dictionary for a valid Pubmed ID, and one to ensure that it raises an exception for an invalid Pubmed ID.
We also need to create empty functions so that they can be imported to run the (failing) tests:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">get_record_from_PubmedID</span><span class="p">(</span><span class="n">pmid</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">dict</span><span class="p">:</span>
    <span class="k">pass</span>

<span class="k">def</span> <span class="nf">parse_year_from_Pubmed_record</span><span class="p">(</span><span class="n">pubmed_record</span><span class="p">:</span> <span class="nb">dict</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
    <span class="k">pass</span>
</pre></div>
</div>
<p>Here are the initial tests; note that writing these tests requires a bit of knowledge about the expected structure of a Pubmed record.
We will generate a fixture so that the valid record and PubMed ID can be reused in a later test.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="nd">@pytest</span><span class="o">.</span><span class="n">fixture</span><span class="p">(</span><span class="n">scope</span><span class="o">=</span><span class="s2">&quot;session&quot;</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">valid_pmid</span><span class="p">():</span>
    <span class="k">return</span> <span class="s2">&quot;39312494&quot;</span>

<span class="nd">@pytest</span><span class="o">.</span><span class="n">fixture</span><span class="p">(</span><span class="n">scope</span><span class="o">=</span><span class="s2">&quot;session&quot;</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">pmid_record</span><span class="p">(</span><span class="n">valid_pmid</span><span class="p">):</span>
    <span class="n">record</span> <span class="o">=</span> <span class="n">get_record_from_PubmedID</span><span class="p">(</span><span class="n">valid_pmid</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">record</span>

<span class="k">def</span> <span class="nf">test_get_record_from_valid_PubmedID</span><span class="p">(</span><span class="n">pmid_record</span><span class="p">,</span> <span class="n">valid_pmid</span><span class="p">):</span>
    <span class="k">assert</span> <span class="n">pmid_record</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
    <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">pmid_record</span><span class="p">,</span> <span class="nb">dict</span><span class="p">)</span>
    <span class="k">assert</span> <span class="n">pmid_record</span><span class="p">[</span><span class="s1">&#39;uid&#39;</span><span class="p">]</span> <span class="o">==</span> <span class="n">valid_pmid</span>

<span class="k">def</span> <span class="nf">test_get_record_from_invalid_PubmedID</span><span class="p">():</span>
    <span class="n">pmid</span> <span class="o">=</span> <span class="s2">&quot;10000000000&quot;</span>
    <span class="k">with</span> <span class="n">pytest</span><span class="o">.</span><span class="n">raises</span><span class="p">(</span><span class="ne">ValueError</span><span class="p">):</span>
        <span class="n">record</span> <span class="o">=</span> <span class="n">get_record_from_PubmedID</span><span class="p">(</span><span class="n">pmid</span><span class="p">)</span>

</pre></div>
</div>
<p>Armed with these tests, we now work with Copilot to develop the code for <code class="docutils literal notranslate"><span class="pre">get_record_from_PubmedID</span></code>:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">get_record_from_PubmedID</span><span class="p">(</span><span class="n">pmid</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">esummary_url</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="kc">None</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">dict</span><span class="p">:</span>

    <span class="k">if</span> <span class="n">esummary_url</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">esummary_url</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">BASE_URL</span><span class="si">}</span><span class="s2">/esummary.fcgi?db=pubmed&amp;id=</span><span class="si">{</span><span class="n">pmid</span><span class="si">}</span><span class="s2">&amp;retmode=json&quot;</span>

    <span class="n">response</span> <span class="o">=</span> <span class="n">requests</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">esummary_url</span><span class="p">)</span>

    <span class="n">result_json</span> <span class="o">=</span> <span class="n">response</span><span class="o">.</span><span class="n">json</span><span class="p">()</span>

    <span class="k">if</span> <span class="p">(</span>
        <span class="n">response</span><span class="o">.</span><span class="n">status_code</span> <span class="o">!=</span> <span class="mi">200</span>
        <span class="ow">or</span> <span class="s2">&quot;result&quot;</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">result_json</span>
        <span class="ow">or</span> <span class="n">pmid</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">result_json</span><span class="p">[</span><span class="s2">&quot;result&quot;</span><span class="p">]</span>
        <span class="ow">or</span> <span class="s2">&quot;error&quot;</span> <span class="ow">in</span> <span class="n">result_json</span><span class="p">[</span><span class="s2">&quot;result&quot;</span><span class="p">][</span><span class="n">pmid</span><span class="p">]</span>
    <span class="p">):</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Bad request&quot;</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">result_json</span><span class="p">[</span><span class="s2">&quot;result&quot;</span><span class="p">][</span><span class="n">pmid</span><span class="p">]</span>
</pre></div>
</div>
<p>This passes the tests, so we can now move to writing some initial tests for <code class="docutils literal notranslate"><span class="pre">parse_year_from_Pubmed_record</span></code>:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">test_parse_year_from_Pubmed_record</span><span class="p">():</span>
    <span class="n">record</span> <span class="o">=</span> <span class="p">{</span>
        <span class="s2">&quot;pubdate&quot;</span><span class="p">:</span> <span class="s2">&quot;2021 Jan 1&quot;</span>
    <span class="p">}</span>
    <span class="n">year</span> <span class="o">=</span> <span class="n">parse_year_from_Pubmed_record</span><span class="p">(</span><span class="n">record</span><span class="p">)</span>
    <span class="k">assert</span> <span class="n">year</span> <span class="o">==</span> <span class="mi">2021</span>


<span class="k">def</span> <span class="nf">test_parse_year_from_Pubmed_record_empty</span><span class="p">():</span>
    <span class="n">record</span> <span class="o">=</span> <span class="p">{</span>
        <span class="s2">&quot;pubdate&quot;</span><span class="p">:</span> <span class="s2">&quot;&quot;</span>
    <span class="p">}</span>
    <span class="n">year</span> <span class="o">=</span> <span class="n">parse_year_from_Pubmed_record</span><span class="p">(</span><span class="n">record</span><span class="p">)</span>
    <span class="k">assert</span> <span class="n">year</span> <span class="ow">is</span> <span class="kc">None</span>
</pre></div>
</div>
<p>And then we use our AI tool to develop the implementation:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">parse_year_from_Pubmed_record</span><span class="p">(</span><span class="n">pubmed_record</span><span class="p">:</span> <span class="nb">dict</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
    <span class="n">pubdate</span> <span class="o">=</span> <span class="n">pubmed_record</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;pubdate&quot;</span><span class="p">)</span> 
    <span class="k">return</span> <span class="nb">int</span><span class="p">(</span><span class="n">pubdate</span><span class="o">.</span><span class="n">split</span><span class="p">()[</span><span class="mi">0</span><span class="p">])</span> <span class="k">if</span> <span class="n">pubdate</span> <span class="k">else</span> <span class="kc">None</span>
</pre></div>
</div>
<p>Now let’s say that you had a specific set of Pubmed IDs that you wanted to test the code against; for example, you might select IDs from papers published in various years across various journals.
To do this, we first create a list of tuples that include the information that we will need for the test; in this case it’s the Pubmed ID and the true year of publication.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">testdata</span> <span class="o">=</span> <span class="p">[</span>
    <span class="p">(</span><span class="s1">&#39;17773841&#39;</span><span class="p">,</span> <span class="mi">1944</span><span class="p">),</span>
    <span class="p">(</span><span class="s1">&#39;13148370&#39;</span><span class="p">,</span> <span class="mi">1954</span><span class="p">),</span>
    <span class="p">(</span><span class="s1">&#39;14208567&#39;</span><span class="p">,</span> <span class="mi">1964</span><span class="p">),</span>
    <span class="p">(</span><span class="s1">&#39;4621244&#39;</span><span class="p">,</span> <span class="mi">1974</span><span class="p">),</span>
    <span class="p">(</span><span class="s1">&#39;6728178&#39;</span><span class="p">,</span> <span class="mi">1984</span><span class="p">),</span>
    <span class="p">(</span><span class="s1">&#39;10467601&#39;</span><span class="p">,</span> <span class="mi">1994</span><span class="p">),</span>
    <span class="p">(</span><span class="s1">&#39;15050513&#39;</span><span class="p">,</span> <span class="mi">2004</span><span class="p">)</span>
<span class="p">]</span>
</pre></div>
</div>
<p>We then feed this into our test using the <code class="docutils literal notranslate"><span class="pre">&#64;pytest.mark.parametrize</span></code> decorator on the test, which will feed in each of the values into the test:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="nd">@pytest</span><span class="o">.</span><span class="n">mark</span><span class="o">.</span><span class="n">parametrize</span><span class="p">(</span><span class="s2">&quot;pmid, year_true&quot;</span><span class="p">,</span> <span class="n">testdata</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">test_parse_year_from_pmid_parametric</span><span class="p">(</span><span class="n">pmid</span><span class="p">,</span> <span class="n">year_true</span><span class="p">):</span>
    <span class="n">time</span><span class="o">.</span><span class="n">sleep</span><span class="p">(</span><span class="mf">0.5</span><span class="p">)</span> <span class="c1"># delay to avoid hitting the PubMed API too quickly</span>
    <span class="n">record</span> <span class="o">=</span> <span class="n">get_record_from_PubmedID</span><span class="p">(</span><span class="n">pmid</span><span class="p">)</span>
    <span class="n">year_result</span> <span class="o">=</span> <span class="n">parse_year_from_Pubmed_record</span><span class="p">(</span><span class="n">record</span><span class="p">)</span>
    <span class="k">assert</span> <span class="n">year_result</span> <span class="o">==</span> <span class="n">year_true</span>
</pre></div>
</div>
<p>Note that we inserted a delay at the beginning of the test; this is necessary because the PubMed API will has a rate limit on requests, and running these tests without a delay to limit the request rate will result in intermittent test failures.
Looking at the results of running the test, we will see that each parametric value is run as a separate test:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>...
tests/textmining/test_textmining.py::test_parse_year_from_pmid_parametric<span class="o">[</span><span class="m">17773841</span>-1944<span class="o">]</span><span class="w"> </span>PASSED<span class="w">       </span><span class="o">[</span><span class="w"> </span><span class="m">62</span>%<span class="o">]</span>
tests/textmining/test_textmining.py::test_parse_year_from_pmid_parametric<span class="o">[</span><span class="m">13148370</span>-1954<span class="o">]</span><span class="w"> </span>PASSED<span class="w">       </span><span class="o">[</span><span class="w"> </span><span class="m">68</span>%<span class="o">]</span>
tests/textmining/test_textmining.py::test_parse_year_from_pmid_parametric<span class="o">[</span><span class="m">14208567</span>-1964<span class="o">]</span><span class="w"> </span>PASSED<span class="w">       </span><span class="o">[</span><span class="w"> </span><span class="m">75</span>%<span class="o">]</span>
tests/textmining/test_textmining.py::test_parse_year_from_pmid_parametric<span class="o">[</span><span class="m">4621244</span>-1974<span class="o">]</span><span class="w"> </span>PASSED<span class="w">        </span><span class="o">[</span><span class="w"> </span><span class="m">81</span>%<span class="o">]</span>
tests/textmining/test_textmining.py::test_parse_year_from_pmid_parametric<span class="o">[</span><span class="m">6728178</span>-1984<span class="o">]</span><span class="w"> </span>PASSED<span class="w">        </span><span class="o">[</span><span class="w"> </span><span class="m">87</span>%<span class="o">]</span>
tests/textmining/test_textmining.py::test_parse_year_from_pmid_parametric<span class="o">[</span><span class="m">10467601</span>-1994<span class="o">]</span><span class="w"> </span>PASSED<span class="w">       </span><span class="o">[</span><span class="w"> </span><span class="m">93</span>%<span class="o">]</span>
tests/textmining/test_textmining.py::test_parse_year_from_pmid_parametric<span class="o">[</span><span class="m">15050513</span>-2004<span class="o">]</span><span class="w"> </span>PASSED<span class="w">       </span><span class="o">[</span><span class="m">100</span>%<span class="o">]</span>
</pre></div>
</div>
<p>This test requires a live API, so it would fail in cases where one didn’t have a proper network connection or if the API was down, and it would also be slow for a large number of tests.
It would be more efficient to mock the <code class="docutils literal notranslate"><span class="pre">get_record_from_PubmedID</span></code> function to avoid dependency on the live API, but for our simple purposes it’s fine to use the live API.</p>
</section>
<section id="property-based-testing">
<h2><a class="toc-backref" href="#id24">Property-based testing</a><a class="headerlink" href="#property-based-testing" title="Permalink to this heading">#</a></h2>
<p>Parameterized testing can be useful when we have specific values that we want to test, but sometimes we wish to test a large range of possible values drawn from some sort of distribution.
One approach to doing this is known as <em>property-based testing</em>, and basically involves generating random values that match some specification and testing the code against those.</p>
<p>Property-based testing can be particularly useful for testing mathematical code, so we will develop another simple example to show how to use the <code class="docutils literal notranslate"><span class="pre">hypothesis</span></code> module in Python to perform property-based testing.
Let’s say that we have developed a function to perform linear regression, taking in two vectors (X and y variables) and return a vector of length 2 (parameter estimates for slope and intercept).
Copilot generates some very terse code for us:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">linear_regression</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
    <span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">c_</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]),</span> <span class="n">X</span><span class="p">]</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">inv</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">T</span> <span class="o">@</span> <span class="n">X</span><span class="p">)</span> <span class="o">@</span> <span class="n">X</span><span class="o">.</span><span class="n">T</span> <span class="o">@</span> <span class="n">y</span>
</pre></div>
</div>
<p>Asking Copilot to make the code more readable, we get this somewhat overly verbose version:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">linear_regression_verbose</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
    <span class="c1"># Add a column of ones to the input data to account for the intercept term</span>
    <span class="n">X_with_intercept</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">c_</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]),</span> <span class="n">X</span><span class="p">]</span>

    <span class="c1"># Compute the parameters using the normal equation</span>
    <span class="n">X_transpose</span> <span class="o">=</span> <span class="n">X_with_intercept</span><span class="o">.</span><span class="n">T</span>
    <span class="n">X_transpose_X</span> <span class="o">=</span> <span class="n">X_transpose</span> <span class="o">@</span> <span class="n">X_with_intercept</span>
    <span class="n">X_transpose_y</span> <span class="o">=</span> <span class="n">X_transpose</span> <span class="o">@</span> <span class="n">y</span>
    <span class="n">beta</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">inv</span><span class="p">(</span><span class="n">X_transpose_X</span><span class="p">)</span> <span class="o">@</span> <span class="n">X_transpose_y</span>

    <span class="k">return</span> <span class="n">beta</span>
</pre></div>
</div>
<p>The linear regression computation requires several things to be true of the input data in order to proceed without error:</p>
<ul class="simple">
<li><p>The data must not contain any infinite or <em>NaN</em> values</p></li>
<li><p>The input data for X and y must each have at least two unique values</p></li>
<li><p>The X matrix must be full rank</p></li>
</ul>
<p>In order to ensure that the input data are valid, we generate a function that validates the inputs, raising an exception if they are not, and we include this in our linear regression function:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span>
<span class="k">def</span> <span class="nf">_validate_input</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">np</span><span class="o">.</span><span class="n">isinf</span><span class="p">(</span><span class="n">X</span><span class="p">)</span><span class="o">.</span><span class="n">any</span><span class="p">()</span> <span class="ow">or</span> <span class="n">np</span><span class="o">.</span><span class="n">isinf</span><span class="p">(</span><span class="n">y</span><span class="p">)</span><span class="o">.</span><span class="n">any</span><span class="p">():</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Input data contains infinite values&quot;</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">np</span><span class="o">.</span><span class="n">isnan</span><span class="p">(</span><span class="n">X</span><span class="p">)</span><span class="o">.</span><span class="n">any</span><span class="p">()</span> <span class="ow">or</span> <span class="n">np</span><span class="o">.</span><span class="n">isnan</span><span class="p">(</span><span class="n">y</span><span class="p">)</span><span class="o">.</span><span class="n">any</span><span class="p">():</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Input data contains NaN values&quot;</span><span class="p">)</span>
    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">unique</span><span class="p">(</span><span class="n">X</span><span class="p">))</span> <span class="o">&lt;</span> <span class="mi">2</span> <span class="ow">or</span> <span class="nb">len</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">unique</span><span class="p">(</span><span class="n">y</span><span class="p">))</span> <span class="o">&lt;</span> <span class="mi">2</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Input data must have at least 2 unique values&quot;</span><span class="p">)</span>

    <span class="n">X_with_intercept</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">c_</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]),</span> <span class="n">X</span><span class="p">]</span>
    <span class="k">if</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">matrix_rank</span><span class="p">(</span><span class="n">X_with_intercept</span><span class="p">)</span> <span class="o">&lt;</span> <span class="n">X_with_intercept</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Input data is not full rank&quot;</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">linear_regression</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">validate</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>

    <span class="k">if</span> <span class="n">validate</span><span class="p">:</span>
        <span class="n">_validate_input</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>

    <span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">c_</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]),</span> <span class="n">X</span><span class="p">]</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">inv</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">T</span> <span class="o">@</span> <span class="n">X</span><span class="p">)</span> <span class="o">@</span> <span class="n">X</span><span class="o">.</span><span class="n">T</span> <span class="o">@</span> <span class="n">y</span>
</pre></div>
</div>
<p>Now we can use the <code class="docutils literal notranslate"><span class="pre">hypothesis</span></code> module to throw a range of data at this function and see if it fails, using the following test:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="nd">@given</span><span class="p">(</span>
    <span class="c1"># Only generate data that is likely to be valid to start with</span>
    <span class="n">nps</span><span class="o">.</span><span class="n">arrays</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float64</span><span class="p">,</span> <span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">elements</span><span class="o">=</span><span class="n">st</span><span class="o">.</span><span class="n">floats</span><span class="p">(</span><span class="o">-</span><span class="mf">1e6</span><span class="p">,</span> <span class="mf">1e6</span><span class="p">)),</span>
    <span class="n">nps</span><span class="o">.</span><span class="n">arrays</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float64</span><span class="p">,</span> <span class="p">(</span><span class="mi">10</span><span class="p">,),</span> <span class="n">elements</span><span class="o">=</span><span class="n">st</span><span class="o">.</span><span class="n">floats</span><span class="p">(</span><span class="o">-</span><span class="mf">1e6</span><span class="p">,</span> <span class="mf">1e6</span><span class="p">)),</span>
<span class="p">)</span>
<span class="k">def</span> <span class="nf">test_linear_regression_without_validation</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Tests that our algorithm matches a reference implementation (scipy).&quot;&quot;&quot;</span>

    <span class="c1"># Now we can safely test the math against a reference implementation (scipy), </span>
    <span class="c1"># knowing the input is valid.</span>
    <span class="n">params</span> <span class="o">=</span> <span class="n">linear_regression</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">validate</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
    <span class="k">assert</span> <span class="n">params</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">,</span> <span class="s2">&quot;Parameters should not be None&quot;</span>
</pre></div>
</div>
<p>The <code class="docutils literal notranslate"><span class="pre">&#64;given</span></code> decorator contains commands that will generate two arrays of the same size, which are then used as our X and y variables.
The main purpose of the test is to see whether the function successfully executes (i.e. a smoke test), but we include a minimal assertion to make sure that it returns a value that is not None.
We will turn off the validation in order to see what happens if the linear regression function is given invalid data.
Running this test, we see that the test fails, with the following output:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>❯<span class="w"> </span>pytest<span class="w"> </span>tests/property_based_testing/test_propertybased_smoke.py
<span class="o">===========================</span><span class="w"> </span><span class="nb">test</span><span class="w"> </span>session<span class="w"> </span><span class="nv">starts</span><span class="w"> </span><span class="o">===========================</span>
tests/property_based_testing/test_propertybased_smoke.py<span class="w"> </span>F<span class="w">          </span><span class="o">[</span><span class="m">100</span>%<span class="o">]</span>

<span class="o">================================</span><span class="w"> </span><span class="nv">FAILURES</span><span class="w"> </span><span class="o">=================================</span>
________________<span class="w"> </span>test_linear_regression_without_validation<span class="w"> </span>________________
_<span class="w"> </span>_<span class="w"> </span>_<span class="w"> </span>_<span class="w"> </span>_<span class="w"> </span>_<span class="w"> </span>_<span class="w"> </span>_<span class="w"> </span>_<span class="w"> </span>_<span class="w"> </span>_<span class="w"> </span>_<span class="w"> </span>_<span class="w"> </span>_<span class="w"> </span>_<span class="w"> </span>_<span class="w"> </span>_<span class="w"> </span>_<span class="w"> </span>_<span class="w"> </span>_<span class="w"> </span>_<span class="w"> </span>_<span class="w"> </span>_<span class="w"> </span>_<span class="w"> </span>_<span class="w"> </span>_<span class="w"> </span>_<span class="w"> </span>_<span class="w"> </span>_<span class="w"> </span>_<span class="w"> </span>_<span class="w"> </span>_<span class="w"> </span>_<span class="w"> </span>_<span class="w"> </span>_<span class="w"> </span>_<span class="w"> </span>_<span class="w"> </span>_
tests/property_based_testing/test_propertybased_smoke.py:19:<span class="w"> </span><span class="k">in</span><span class="w"> </span>test_linear_regression_without_validation
<span class="w">    </span><span class="nv">params</span><span class="w"> </span><span class="o">=</span><span class="w"> </span>linear_regression<span class="o">(</span>X,<span class="w"> </span>y,<span class="w"> </span><span class="nv">validate</span><span class="o">=</span>False<span class="o">)</span>
<span class="w">             </span>^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
src/BetterCodeBetterScience/my_linear_regression.py:61:<span class="w"> </span><span class="k">in</span><span class="w"> </span>linear_regression
<span class="w">    </span><span class="k">return</span><span class="w"> </span>np.linalg.inv<span class="o">(</span>X.T<span class="w"> </span>@<span class="w"> </span>X<span class="o">)</span><span class="w"> </span>@<span class="w"> </span>X.T<span class="w"> </span>@<span class="w"> </span>y
<span class="w">           </span>^^^^^^^^^^^^^^^^^^^^^^
.venv/lib/python3.12/site-packages/numpy/linalg/_linalg.py:615:<span class="w"> </span><span class="k">in</span><span class="w"> </span>inv
<span class="w">    </span><span class="nv">ainv</span><span class="w"> </span><span class="o">=</span><span class="w"> </span>_umath_linalg.inv<span class="o">(</span>a,<span class="w"> </span><span class="nv">signature</span><span class="o">=</span>signature<span class="o">)</span>
<span class="w">           </span>^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
_<span class="w"> </span>_<span class="w"> </span>_<span class="w"> </span>_<span class="w"> </span>_<span class="w"> </span>_<span class="w"> </span>_<span class="w"> </span>_<span class="w"> </span>_<span class="w"> </span>_<span class="w"> </span>_<span class="w"> </span>_<span class="w"> </span>_<span class="w"> </span>_<span class="w"> </span>_<span class="w"> </span>_<span class="w"> </span>_<span class="w"> </span>_<span class="w"> </span>_<span class="w"> </span>_<span class="w"> </span>_<span class="w"> </span>_<span class="w"> </span>_<span class="w"> </span>_<span class="w"> </span>_<span class="w"> </span>_<span class="w"> </span>_<span class="w"> </span>_<span class="w"> </span>_<span class="w"> </span>_<span class="w"> </span>_<span class="w"> </span>_<span class="w"> </span>_<span class="w"> </span>_<span class="w"> </span>_<span class="w"> </span>_<span class="w"> </span>_<span class="w"> </span>_

<span class="nv">err</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s1">&#39;invalid value&#39;</span>,<span class="w"> </span><span class="nv">flag</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">8</span>

<span class="w">    </span>def<span class="w"> </span>_raise_linalgerror_singular<span class="o">(</span>err,<span class="w"> </span>flag<span class="o">)</span>:
&gt;<span class="w">       </span>raise<span class="w"> </span>LinAlgError<span class="o">(</span><span class="s2">&quot;Singular matrix&quot;</span><span class="o">)</span>
E<span class="w">       </span>numpy.linalg.LinAlgError:<span class="w"> </span>Singular<span class="w"> </span>matrix
E<span class="w">       </span>Falsifying<span class="w"> </span>example:<span class="w"> </span>test_linear_regression_without_validation<span class="o">(</span>
E<span class="w">           </span><span class="nv">X</span><span class="o">=</span>array<span class="o">([[</span><span class="m">0</span>.<span class="o">]</span>,
E<span class="w">                  </span><span class="o">[</span><span class="m">0</span>.<span class="o">]</span>,
E<span class="w">                  </span><span class="o">[</span><span class="m">0</span>.<span class="o">]</span>,
E<span class="w">                  </span><span class="o">[</span><span class="m">0</span>.<span class="o">]</span>,
E<span class="w">                  </span><span class="o">[</span><span class="m">0</span>.<span class="o">]</span>,
E<span class="w">                  </span><span class="o">[</span><span class="m">0</span>.<span class="o">]</span>,
E<span class="w">                  </span><span class="o">[</span><span class="m">0</span>.<span class="o">]</span>,
E<span class="w">                  </span><span class="o">[</span><span class="m">0</span>.<span class="o">]</span>,
E<span class="w">                  </span><span class="o">[</span><span class="m">0</span>.<span class="o">]</span>,
E<span class="w">                  </span><span class="o">[</span><span class="m">0</span>.<span class="o">]])</span>,
E<span class="w">           </span><span class="nv">y</span><span class="o">=</span>array<span class="o">([</span><span class="m">0</span>.,<span class="w"> </span><span class="m">0</span>.,<span class="w"> </span><span class="m">0</span>.,<span class="w"> </span><span class="m">0</span>.,<span class="w"> </span><span class="m">0</span>.,<span class="w"> </span><span class="m">0</span>.,<span class="w"> </span><span class="m">0</span>.,<span class="w"> </span><span class="m">0</span>.,<span class="w"> </span><span class="m">0</span>.,<span class="w"> </span><span class="m">0</span>.<span class="o">])</span>,<span class="w">  </span><span class="c1"># or any other generated value</span>
E<span class="w">       </span><span class="o">)</span>
E<span class="w">       </span>Explanation:
E<span class="w">           </span>These<span class="w"> </span>lines<span class="w"> </span>were<span class="w"> </span>always<span class="w"> </span>and<span class="w"> </span>only<span class="w"> </span>run<span class="w"> </span>by<span class="w"> </span>failing<span class="w"> </span>examples:
E<span class="w">               </span>/Users/poldrack/Dropbox/code/BetterCodeBetterScience/.venv/lib/python3.12/site-packages/numpy/linalg/_linalg.py:104

<span class="o">=========================</span><span class="w"> </span>short<span class="w"> </span><span class="nb">test</span><span class="w"> </span>summary<span class="w"> </span><span class="nv">info</span><span class="w"> </span><span class="o">=========================</span>
FAILED<span class="w"> </span>tests/property_based_testing/test_propertybased_smoke.py::test_linear_regression_without_validation<span class="w"> </span>-<span class="w"> </span>numpy.linalg.LinAlgError:<span class="w"> </span>Singular<span class="w"> </span><span class="nv">matrix</span>
<span class="o">============================</span><span class="w"> </span><span class="m">1</span><span class="w"> </span>failed<span class="w"> </span><span class="k">in</span><span class="w"> </span><span class="m">2</span>.33s<span class="w"> </span><span class="o">============================</span>
</pre></div>
</div>
<p>The test has identified a specific input that will cause the code to fail - namely, when the X variable is all zeros, which leads to an error when trying to invert the singular matrix.
We could get the test to pass by causing the function to return <code class="docutils literal notranslate"><span class="pre">None</span></code> when the matrix is no invertible, but this is not a great practice; we should announce problems loudly by raising an exception, rather than burying them quietly by returning <code class="docutils literal notranslate"><span class="pre">None</span></code>.</p>
<p>Now that we have seen how <code class="docutils literal notranslate"><span class="pre">hypothesis</span></code> can identify errors, let’s develop some tests for the code that we can use to make sure that it works properly.
We will first separately test the validator function, making sure that it can detect any of the potential problems that it should be able to detect:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="nd">@given</span><span class="p">(</span>
    <span class="n">nps</span><span class="o">.</span><span class="n">arrays</span><span class="p">(</span>
        <span class="n">np</span><span class="o">.</span><span class="n">float64</span><span class="p">,</span> <span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">elements</span><span class="o">=</span><span class="n">st</span><span class="o">.</span><span class="n">floats</span><span class="p">(</span><span class="n">allow_nan</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">allow_infinity</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="p">),</span>
    <span class="n">nps</span><span class="o">.</span><span class="n">arrays</span><span class="p">(</span>
        <span class="n">np</span><span class="o">.</span><span class="n">float64</span><span class="p">,</span> <span class="p">(</span><span class="mi">10</span><span class="p">,),</span> <span class="n">elements</span><span class="o">=</span><span class="n">st</span><span class="o">.</span><span class="n">floats</span><span class="p">(</span><span class="n">allow_nan</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">allow_infinity</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="p">),</span>
<span class="p">)</span>
<span class="k">def</span> <span class="nf">test_validate_input</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Tests that our validation function correctly identifies and rejects bad data.&quot;&quot;&quot;</span>
    <span class="k">try</span><span class="p">:</span>
        <span class="c1"># Call the validation function directly</span>
        <span class="n">_validate_input</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
        <span class="n">linear_regression</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">validate</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
        <span class="c1"># If it gets here, hypothesis generated valid data and the function ran successfully. </span>
    <span class="k">except</span> <span class="ne">ValueError</span><span class="p">:</span>
        <span class="c1"># If we get here, the data was invalid. The validator correctly</span>
        <span class="c1"># raised an error. This is also a successful test case.</span>
        <span class="k">pass</span> <span class="c1"># Explicitly show that catching the error is the goal.</span>
</pre></div>
</div>
<p>Note that this doesn’t actually whether our code actually gives the right answer, only that it runs without error and catches the appropriate problem cases, ensuring that any data passing the validator can run without error on the linear regression function.
When a reference implementation exists for a function (as it does in the case of linear regression), then we can compare our results to the results from the reference.
Here we will compare to the outputs from the from the linear regression function from the <code class="docutils literal notranslate"><span class="pre">scipy</span></code> module.
Using this, we can check the randomly generated input to see whether it should raise an exception, and otherwise compare the results of our function to the scipy function:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Test 2: Test the algorithm&#39;s correctness, assuming valid input</span>
<span class="c1"># --------------------------------------------------------------</span>
<span class="nd">@given</span><span class="p">(</span>
    <span class="c1"># Only generate data that is likely to be valid to start with</span>
    <span class="n">nps</span><span class="o">.</span><span class="n">arrays</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float64</span><span class="p">,</span> <span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">elements</span><span class="o">=</span><span class="n">st</span><span class="o">.</span><span class="n">floats</span><span class="p">(</span><span class="o">-</span><span class="mf">1e6</span><span class="p">,</span> <span class="mf">1e6</span><span class="p">)),</span>
    <span class="n">nps</span><span class="o">.</span><span class="n">arrays</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float64</span><span class="p">,</span> <span class="p">(</span><span class="mi">10</span><span class="p">,),</span> <span class="n">elements</span><span class="o">=</span><span class="n">st</span><span class="o">.</span><span class="n">floats</span><span class="p">(</span><span class="o">-</span><span class="mf">1e6</span><span class="p">,</span> <span class="mf">1e6</span><span class="p">)),</span>
<span class="p">)</span>
<span class="k">def</span> <span class="nf">test_linear_regression_correctness</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Tests that our algorithm matches a reference implementation (scipy).&quot;&quot;&quot;</span>
    <span class="c1"># Use `hypothesis.assume` to filter out any edge cases the validator would catch.</span>
    <span class="c1"># This tells hypothesis: &quot;If this data is bad, just discard it and try another.&quot;</span>
    <span class="k">try</span><span class="p">:</span>
        <span class="n">_validate_input</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
    <span class="k">except</span> <span class="ne">ValueError</span><span class="p">:</span>
        <span class="n">assume</span><span class="p">(</span><span class="kc">False</span><span class="p">)</span>  <span class="c1"># Prunes this example from the test run</span>

    <span class="c1"># Now we can safely test the math against a reference implementation (scipy), </span>
    <span class="c1"># knowing the input is valid.</span>
    <span class="n">params</span> <span class="o">=</span> <span class="n">linear_regression</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
    <span class="n">lr_result</span> <span class="o">=</span> <span class="n">linregress</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">flatten</span><span class="p">(),</span> <span class="n">y</span><span class="o">.</span><span class="n">flatten</span><span class="p">())</span>

    <span class="k">assert</span> <span class="n">np</span><span class="o">.</span><span class="n">allclose</span><span class="p">(</span><span class="n">params</span><span class="p">,</span> <span class="p">[</span><span class="n">lr_result</span><span class="o">.</span><span class="n">intercept</span><span class="p">,</span> <span class="n">lr_result</span><span class="o">.</span><span class="n">slope</span><span class="p">])</span>
</pre></div>
</div>
<p>This test passes, showing that our function closely matches the scipy reference implementation.
Note that we restricted the range of the values generated by the test to <code class="docutils literal notranslate"><span class="pre">[-1e6,</span> <span class="pre">1e6]</span></code>; when the test values were allowed to vary across the full range of 64-bit floating point values (+/- 1.79e+308), we observed minute differences in the parameter estimates between the two functions that nonetheless exceeded the tolerance limits of <code class="docutils literal notranslate"><span class="pre">np.allclose()</span></code>.
We decided to restrict the test values to a range that is within the usual range of input data; if one were planning to work with very small or very large numbers, they would want to possibly test the input over a wider range and understand the nature and magnitude of differences in results between the methods.</p>
</section>
<section id="automated-testing-and-continuous-integration">
<h2><a class="toc-backref" href="#id25">Automated testing and continuous integration</a><a class="headerlink" href="#automated-testing-and-continuous-integration" title="Permalink to this heading">#</a></h2>
<p>Once we have a set of tests for a project, it’s important to integrate them into our workflow so that we ensure that new changes to the code don’t break the existing code (known as a <em>regression</em>).</p>
<p>One useful way to ensure that the tests are run regularly is to run them automatically every time changes are pushed to version control.
This is known as “continuous integration” (CI for short), referring to the fact that it allows changes to be continuously integrated into the main code branch once they are confirmed to pass all of the tests.
There are a number of different platforms one can use for CI; we will focus on <em>Github Actions</em> since it is the most tightly integrated into the Github version control system.
Testing using CI also has a useful side effect: Since the CI system uses a virtual machine to run the tests, the use of CI for testing ensures that the code can run on a separate machine from the one where it was developed.
Because setting up the CI system also requires understanding all of the dependencies that are required for the code to run, the CI setup provides a recipe to run the code on any other system.</p>
<section id="using-github-actions">
<h3><a class="toc-backref" href="#id26">Using GitHub Actions</a><a class="headerlink" href="#using-github-actions" title="Permalink to this heading">#</a></h3>
<p>GitHub Actions is a system that allows one to automatically execute workflows in response to events related to any GitHub repository.
At the time of writing, Github Actions are free and unlimited for public repositories when using the standard GitHub-based workflow runners.</p>
<p>When setting up an automated action using GitHub Actions, there are two primary decisions to specify:</p>
<ul class="simple">
<li><p>What is the workflow that I want to run?</p></li>
<li><p>What are the events that I want to trigger the workflow?</p></li>
</ul>
<p>As an example, we will implement a workflow running to execute tests for a simple python package that was generated for this book project, called <a class="reference external" href="https://github.com/poldrack/mdnewline/">mdnewline</a>.
We start by going to the GitHub Actions tab in the repository, and selecting the “Python Package” option, which creates a workflow that builds and tests a Python package.
Generating the workflow results in a file that contains a description of the workflow, located at <code class="docutils literal notranslate"><span class="pre">.github/workflows/python-package.yml</span></code>.
Looking more closely at the workflow file, we can see how it works.
The first section specifies the name of the workflow, and defines the events that will trigger the workflow:</p>
<div class="highlight-yaml notranslate"><div class="highlight"><pre><span></span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">Python package</span>

<span class="nt">on</span><span class="p">:</span>
<span class="w">  </span><span class="nt">push</span><span class="p">:</span>
<span class="w">    </span><span class="nt">branches</span><span class="p">:</span><span class="w"> </span><span class="p p-Indicator">[</span><span class="w"> </span><span class="s">&quot;main&quot;</span><span class="w"> </span><span class="p p-Indicator">]</span>
<span class="w">  </span><span class="nt">pull_request</span><span class="p">:</span>
<span class="w">    </span><span class="nt">branches</span><span class="p">:</span><span class="w"> </span><span class="p p-Indicator">[</span><span class="w"> </span><span class="s">&quot;main&quot;</span><span class="w"> </span><span class="p p-Indicator">]</span>
</pre></div>
</div>
<p>This section specifies that the workflow will be run any time there is a push or a pull request to the main branch of the repo.
The next section sets up workflow jobs:</p>
<div class="highlight-yaml notranslate"><div class="highlight"><pre><span></span><span class="nt">jobs</span><span class="p">:</span>
<span class="w">  </span><span class="nt">build</span><span class="p">:</span>
<span class="w">    </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">python-build</span>
<span class="w">    </span><span class="nt">runs-on</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">ubuntu-latest</span>
<span class="w">    </span><span class="nt">strategy</span><span class="p">:</span>
<span class="w">      </span><span class="nt">matrix</span><span class="p">:</span>
<span class="w">        </span><span class="nt">python-version</span><span class="p">:</span><span class="w"> </span><span class="p p-Indicator">[</span><span class="s">&quot;3.12&quot;</span><span class="p p-Indicator">,</span><span class="w"> </span><span class="s">&quot;3.13&quot;</span><span class="p p-Indicator">]</span>
</pre></div>
</div>
<p>The <code class="docutils literal notranslate"><span class="pre">runs-on</span></code> argument tells the workflow runner which virtual machine to use to run the workflows; in this case, we will use the default which is the latest release of Ubuntu Linux.
We also tell it which versions of Python we want to test the code on, updating it to test on all current versions that are compatible with the package requirements (Python &gt;= 3.12).</p>
<p>We then specify the actual steps in the workflow.
The first steps set up the workflow runner so that it can check out the repository, and set up the Python installation.
Since this project uses <code class="docutils literal notranslate"><span class="pre">uv</span></code> to manage packages, we will use the recommended setup code from the <a class="reference external" href="https://docs.astral.sh/uv/guides/integration/github/">uv documentation</a> for multiple Python versions:</p>
<div class="highlight-yaml notranslate"><div class="highlight"><pre><span></span><span class="w">    </span><span class="nt">steps</span><span class="p">:</span>
<span class="w">      </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">uses</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">actions/checkout@v4</span>

<span class="w">      </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">Install uv and set the python version</span>
<span class="w">        </span><span class="nt">uses</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">astral-sh/setup-uv@v6</span>
<span class="w">        </span><span class="nt">with</span><span class="p">:</span>
<span class="w">          </span><span class="nt">python-version</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">${{ matrix.python-version }}</span>
</pre></div>
</div>
<p>This installs uv with the appropriate python version.
We can then install the project using <code class="docutils literal notranslate"><span class="pre">uv</span> <span class="pre">sync</span></code>, pip install the package within the <code class="docutils literal notranslate"><span class="pre">uv</span></code> environment, and run the tests:</p>
<div class="highlight-yaml notranslate"><div class="highlight"><pre><span></span><span class="w">      </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">Install the project</span>
<span class="w">        </span><span class="nt">run</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">uv sync --locked --all-extras --dev</span>

<span class="w">      </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">pip install the package</span>
<span class="w">        </span><span class="nt">run</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">uv pip install .</span>

<span class="w">      </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">Run tests</span>
<span class="w">        </span><span class="c1"># For example, using `pytest`</span>
<span class="w">        </span><span class="nt">run</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">uv run pytest tests</span>
</pre></div>
</div>
<p>When we commit and push this workflow file, it is automatically run by Github Actions.
If we got to the Actions tab in the repository, we will see that the tests failed, and by looking at the logs we can see that the <code class="docutils literal notranslate"><span class="pre">uv</span></code> installation process failed:</p>
<p><img alt="Github actions failure" src="_images/github_actions_failure.png" /></p>
<p>After fixing the <code class="docutils literal notranslate"><span class="pre">uv</span></code> command, we now get the green light!:</p>
<p><img alt="Github actions success" src="_images/github_actions_success.png" /></p>
<p>It’s nice to advertise our testing to the world, which we can do by <a class="reference external" href="https://docs.github.com/en/actions/how-tos/monitor-workflows/add-a-status-badge">adding a status badge</a> to our home page, using the markdown generated by Actions for us:</p>
<p><img alt="Github actions status badge" src="_images/actions_status_badge.png" /></p>
</section>
</section>
<section id="optimizing-the-testing-workflow">
<h2><a class="toc-backref" href="#id27">Optimizing the testing workflow</a><a class="headerlink" href="#optimizing-the-testing-workflow" title="Permalink to this heading">#</a></h2>
<p>As a project becomes larger and more complex, the tests will necessarily take longer to run - and whenever the tests take too long, it’s likely that you won’t run them as often.
This is particularly the case for data analysis tools, where testing the code on real data can take a very long time.
As an example, our lab develops a tool for fMRI data preprocessing called <a class="reference external" href="https://fmriprep.org/en/stable/">fMRIPrep</a> that performs a large set of operations on functional MRI datasets.
Without optimization, running the full test suite with real data would take roughly two hours, whereas with optimization we can complete it in 10-15 minutes.</p>
<p>There are a number of strategies to optimize one’s testing workflows.</p>
<section id="cherry-picking-specific-tests">
<h3><a class="toc-backref" href="#id28">Cherry-picking specific tests</a><a class="headerlink" href="#cherry-picking-specific-tests" title="Permalink to this heading">#</a></h3>
<p>When developing a new function, it’s usually sufficient to run only the tests that directly address that function rather than running the entire test suite.
If all of the tests for a specific function are located within a single test file, then one can simply call <code class="docutils literal notranslate"><span class="pre">pytest</span></code> with that file.
It’s also possible to run a specific test within a file by referring to a specific class or function using a double-colon marker:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>❯<span class="w"> </span>pytest<span class="w"> </span>tests/textmining/test_textmining.py::test_parse_year_from_Pubmed_record

<span class="o">=============================</span><span class="w"> </span><span class="nb">test</span><span class="w"> </span>session<span class="w"> </span><span class="nv">starts</span><span class="w"> </span><span class="o">==============================</span>
collected<span class="w"> </span><span class="m">1</span><span class="w"> </span>item

tests/textmining/test_textmining.py<span class="w"> </span>.<span class="w">                                    </span><span class="o">[</span><span class="m">100</span>%<span class="o">]</span>
<span class="o">==============================</span><span class="w"> </span><span class="m">1</span><span class="w"> </span>passed<span class="w"> </span><span class="k">in</span><span class="w"> </span><span class="m">0</span>.17s<span class="w"> </span><span class="o">===============================</span>
</pre></div>
</div>
<p>This allows one to focus on the tests that are immediately relevant to a specific development task.</p>
</section>
<section id="controlling-test-execution-order">
<h3><a class="toc-backref" href="#id29">Controlling test execution order</a><a class="headerlink" href="#controlling-test-execution-order" title="Permalink to this heading">#</a></h3>
<p>Several of the strategies described below require the ability to execute tests in specific orders.
There are several strategies one can use to do this.</p>
<p>One strategy that <em>will not</em> work reliably is to place the tests in a specific order in the test file.
While tests are often executed in the order that the functions appear in the file, this not guaranteed.</p>
<p>For precise ordering of particular tests, one can use the <a class="reference external" href="https://pypi.org/project/pytest-order/">pytest-order</a> plugin, which allows the use of marks to specify test order<a class="footnote-reference brackets" href="#id5" id="id3">2</a>.
We start with two tests that are out of order in the test code:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">pytest</span>

<span class="k">def</span> <span class="nf">test_second</span><span class="p">():</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;This is the second test.&quot;&quot;&quot;</span>
    <span class="k">assert</span> <span class="kc">True</span>

<span class="k">def</span> <span class="nf">test_first</span><span class="p">():</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;This is the first test.&quot;&quot;&quot;</span>
    <span class="k">assert</span> <span class="kc">True</span>
</pre></div>
</div>
<p>Running this, we see that these are run in the order they are defined in the test code:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>❯<span class="w"> </span>pytest<span class="w"> </span>-vv<span class="w"> </span>tests/ordering/test_order.py
<span class="o">===========================</span><span class="w"> </span><span class="nb">test</span><span class="w"> </span>session<span class="w"> </span><span class="nv">starts</span><span class="w"> </span><span class="o">============================</span>
collected<span class="w"> </span><span class="m">2</span><span class="w"> </span>items

tests/ordering/test_order.py::test_second<span class="w"> </span>PASSED<span class="w">                     </span><span class="o">[</span><span class="w"> </span><span class="m">50</span>%<span class="o">]</span>
tests/ordering/test_order.py::test_first<span class="w"> </span>PASSED<span class="w">                      </span><span class="o">[</span><span class="m">100</span>%<span class="o">]</span>
<span class="o">============================</span><span class="w"> </span><span class="m">2</span><span class="w"> </span>passed<span class="w"> </span><span class="k">in</span><span class="w"> </span><span class="m">0</span>.05s<span class="w"> </span><span class="o">=============================</span>
</pre></div>
</div>
<p>After installing the <code class="docutils literal notranslate"><span class="pre">pytest-order</span></code> plugin, we can now add marks that control the execution order of the tests:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">pytest</span>

<span class="nd">@pytest</span><span class="o">.</span><span class="n">mark</span><span class="o">.</span><span class="n">order</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">test_second</span><span class="p">():</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;This is the second test.&quot;&quot;&quot;</span>
    <span class="k">assert</span> <span class="kc">True</span>

<span class="nd">@pytest</span><span class="o">.</span><span class="n">mark</span><span class="o">.</span><span class="n">order</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">test_first</span><span class="p">():</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;This is the first test.&quot;&quot;&quot;</span>
    <span class="k">assert</span> <span class="kc">True</span>
</pre></div>
</div>
<p>Running this, we see that these are run in the order they are defined in the test code:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>❯<span class="w"> </span>pytest<span class="w"> </span>-vv<span class="w"> </span>tests/ordering/test_order.py
<span class="o">===========================</span><span class="w"> </span><span class="nb">test</span><span class="w"> </span>session<span class="w"> </span><span class="nv">starts</span><span class="w"> </span><span class="o">============================</span>
collected<span class="w"> </span><span class="m">2</span><span class="w"> </span>items

tests/ordering/test_order.py::test_first<span class="w"> </span>PASSED<span class="w">                      </span><span class="o">[</span><span class="w"> </span><span class="m">50</span>%<span class="o">]</span>
tests/ordering/test_order.py::test_second<span class="w"> </span>PASSED<span class="w">                     </span><span class="o">[</span><span class="m">100</span>%<span class="o">]</span>
<span class="o">============================</span><span class="w"> </span><span class="m">2</span><span class="w"> </span>passed<span class="w"> </span><span class="k">in</span><span class="w"> </span><span class="m">0</span>.05s<span class="w"> </span><span class="o">=============================</span>
</pre></div>
</div>
<p>This package also provides other useful ways to control execution order, such as relative markers like “before” or “last”.</p>
<p>Note that while it might be tempting to use test ordering to ensure the proper executions of tests that rely upon one another, this is generally a bad practice.
Instead, one should generate fixtures for any dependencies that are shared by tests.</p>
</section>
<section id="only-run-failed-tests-or-run-them-first">
<h3><a class="toc-backref" href="#id30">Only run failed tests, or run them first</a><a class="headerlink" href="#only-run-failed-tests-or-run-them-first" title="Permalink to this heading">#</a></h3>
<p>Pytest has a set of features that allow one to focus testing on those tests that have most recently failed, which can be useful in the context of test-driven development.
The first command, <code class="docutils literal notranslate"><span class="pre">pytest</span> <span class="pre">--lf</span></code> (short for <code class="docutils literal notranslate"><span class="pre">--last-failed</span></code>) will check the cache of the most recent test run results, and only run those tests that failed in the most recent run.
It’s important to realize that if one introduces a condition that would cause a previously passed test to fail, it may not be caught, such that one could run a string of tests using the <code class="docutils literal notranslate"><span class="pre">--lf</span></code> flag, get to the point where the tests all seem to pass, but then find that some tests that had passed earlier will fail due to conditions introduced when fixing the later tests.</p>
<p>A related command, <code class="docutils literal notranslate"><span class="pre">pytest</span> <span class="pre">--ff</span></code> (for <code class="docutils literal notranslate"><span class="pre">--failed-first</span></code>) will run the most recent failed tests before running the rest of the tasks.
This command is probably better for general use than the <code class="docutils literal notranslate"><span class="pre">--lf</span></code> flag, since it ensures that all tests are run, but prioritizes the running of the failing tests first.
This might be an option that one would consider using by default, which can be achieved by adding the option to one of the relevant configuration files.
In our case, this would be <code class="docutils literal notranslate"><span class="pre">pyproject.toml</span></code>, where we could add:</p>
<div class="highlight-toml notranslate"><div class="highlight"><pre><span></span><span class="k">[tool.pytest.ini_options]</span>
<span class="n">addopts</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">&quot;--ff&quot;</span>
</pre></div>
</div>
</section>
<section id="separate-unit-tests-from-longer-running-tests">
<h3><a class="toc-backref" href="#id31">Separate unit tests from longer-running tests</a><a class="headerlink" href="#separate-unit-tests-from-longer-running-tests" title="Permalink to this heading">#</a></h3>
<p>If we are writing short functions (which we are, right?), then our unit tests should usually run fairly quickly, whereas our integration tests (which integrate across a potentially large number of modules) may take much longer.
One thing we can do is to run only unit tests while we are developing, while having the integration tests run less frequently.
For example, we could have the unit tests run any time we commit our code (for example, by running them as pre-commit items), while having the integration tests run automatically on our CI system when the code is actually pushed to the GitHub repository.</p>
<p>One way to accomplish this is to use pytest markers (as we saw earlier) to mark groups of tests.
We first need to define our own custom markers in our <code class="docutils literal notranslate"><span class="pre">pyproject.toml</span></code> file:</p>
<div class="highlight-toml notranslate"><div class="highlight"><pre><span></span><span class="k">[tool.pytest.ini_options]</span>
<span class="n">markers</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="p">[</span>
<span class="w">    </span><span class="s2">&quot;unit: mark a test as a unit test.&quot;</span><span class="p">,</span>
<span class="w">    </span><span class="s2">&quot;integration: mark a test as an integration test.&quot;</span>
<span class="p">]</span>
</pre></div>
</div>
<p>We can then generate tests using these markers:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">pytest</span>

<span class="nd">@pytest</span><span class="o">.</span><span class="n">mark</span><span class="o">.</span><span class="n">unit</span>
<span class="k">def</span> <span class="nf">test_unit1</span><span class="p">():</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;This is a unit test.&quot;&quot;&quot;</span>
    <span class="k">assert</span> <span class="kc">True</span>

<span class="nd">@pytest</span><span class="o">.</span><span class="n">mark</span><span class="o">.</span><span class="n">unit</span>
<span class="k">def</span> <span class="nf">test_unit2</span><span class="p">():</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;This is a unit test.&quot;&quot;&quot;</span>
    <span class="k">assert</span> <span class="kc">True</span>

<span class="nd">@pytest</span><span class="o">.</span><span class="n">mark</span><span class="o">.</span><span class="n">integration</span>
<span class="k">def</span> <span class="nf">test_integration</span><span class="p">():</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;This is an integration test.&quot;&quot;&quot;</span>
    <span class="k">assert</span> <span class="kc">True</span>
</pre></div>
</div>
<p>Running these using the standard pytest command, both tests are executed:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>❯<span class="w"> </span>pytest<span class="w"> </span>-vv<span class="w"> </span>tests/markers/test_markers.py
<span class="o">===========================</span><span class="w"> </span><span class="nb">test</span><span class="w"> </span>session<span class="w"> </span><span class="nv">starts</span><span class="w"> </span><span class="o">============================</span>
collected<span class="w"> </span><span class="m">3</span><span class="w"> </span>items

tests/markers/test_markers.py::test_unit1<span class="w"> </span>PASSED<span class="w">                     </span><span class="o">[</span><span class="w"> </span><span class="m">33</span>%<span class="o">]</span>
tests/markers/test_markers.py::test_unit2<span class="w"> </span>PASSED<span class="w">                     </span><span class="o">[</span><span class="w"> </span><span class="m">66</span>%<span class="o">]</span>
tests/markers/test_markers.py::test_integration<span class="w"> </span>PASSED<span class="w">               </span><span class="o">[</span><span class="m">100</span>%<span class="o">]</span>
<span class="o">============================</span><span class="w"> </span><span class="m">3</span><span class="w"> </span>passed<span class="w"> </span><span class="k">in</span><span class="w"> </span><span class="m">0</span>.05s<span class="w"> </span><span class="o">=============================</span>
</pre></div>
</div>
<p>However, we can also specify that only the unit tests should be executed, using the <code class="docutils literal notranslate"><span class="pre">-m</span></code> flag to deselect all other flags:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>❯<span class="w"> </span>pytest<span class="w"> </span>-vv<span class="w"> </span>-m<span class="w"> </span>unit<span class="w"> </span>tests/markers/test_markers.py
<span class="o">===========================</span><span class="w"> </span><span class="nb">test</span><span class="w"> </span>session<span class="w"> </span><span class="nv">starts</span><span class="w"> </span><span class="o">============================</span>
collected<span class="w"> </span><span class="m">3</span><span class="w"> </span>items<span class="w"> </span>/<span class="w"> </span><span class="m">1</span><span class="w"> </span>deselected<span class="w"> </span>/<span class="w"> </span><span class="m">2</span><span class="w"> </span>selected

tests/markers/test_markers.py::test_unit1<span class="w"> </span>PASSED<span class="w">                     </span><span class="o">[</span><span class="w"> </span><span class="m">50</span>%<span class="o">]</span>
tests/markers/test_markers.py::test_unit2<span class="w"> </span>PASSED<span class="w">                     </span><span class="o">[</span><span class="m">100</span>%<span class="o">]</span>
<span class="o">=====================</span><span class="w"> </span><span class="m">2</span><span class="w"> </span>passed,<span class="w"> </span><span class="m">1</span><span class="w"> </span>deselected<span class="w"> </span><span class="k">in</span><span class="w"> </span><span class="m">0</span>.05s<span class="w"> </span><span class="o">======================</span>
</pre></div>
</div>
<p>We could also run all tests that to not match a particular marker:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>❯<span class="w"> </span>pytest<span class="w"> </span>-vv<span class="w"> </span>-m<span class="w"> </span><span class="s2">&quot;not unit&quot;</span><span class="w"> </span>tests/markers/test_markers.py
<span class="o">===========================</span><span class="w"> </span><span class="nb">test</span><span class="w"> </span>session<span class="w"> </span><span class="nv">starts</span><span class="w"> </span><span class="o">============================</span>
collected<span class="w"> </span><span class="m">3</span><span class="w"> </span>items<span class="w"> </span>/<span class="w"> </span><span class="m">2</span><span class="w"> </span>deselected<span class="w"> </span>/<span class="w"> </span><span class="m">1</span><span class="w"> </span>selected

tests/markers/test_markers.py::test_integration<span class="w"> </span>PASSED<span class="w">               </span><span class="o">[</span><span class="m">100</span>%<span class="o">]</span>
<span class="o">=====================</span><span class="w"> </span><span class="m">1</span><span class="w"> </span>passed,<span class="w"> </span><span class="m">2</span><span class="w"> </span>deselected<span class="w"> </span><span class="k">in</span><span class="w"> </span><span class="m">0</span>.05s<span class="w"> </span><span class="o">======================</span>
</pre></div>
</div>
</section>
<section id="longest-tests-last">
<h3><a class="toc-backref" href="#id32">Longest tests last</a><a class="headerlink" href="#longest-tests-last" title="Permalink to this heading">#</a></h3>
<p>Another simple strategy that can help optimize the testing workflow is to run tests in order to the time for completion.
This ensures that one doesn’t end up waiting a long time for long-running tests to complete, only to find that a quick test fails.</p>
<p>We can use pytest’s <code class="docutils literal notranslate"><span class="pre">--durations</span></code> flag to get the durations for each test.
Here is an example with three tests with different durations to completion:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">pytest</span>
<span class="kn">from</span> <span class="nn">time</span> <span class="kn">import</span> <span class="n">sleep</span>

<span class="k">def</span> <span class="nf">test_duration_3</span><span class="p">():</span>
    <span class="n">sleep</span><span class="p">(</span><span class="mi">3</span><span class="p">)</span>
    <span class="k">assert</span> <span class="kc">True</span>

<span class="k">def</span> <span class="nf">test_duration_5</span><span class="p">():</span>
    <span class="n">sleep</span><span class="p">(</span><span class="mi">5</span><span class="p">)</span>
    <span class="k">assert</span> <span class="kc">True</span>

<span class="k">def</span> <span class="nf">test_duration_1</span><span class="p">():</span>
    <span class="n">sleep</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
    <span class="k">assert</span> <span class="kc">True</span>
</pre></div>
</div>
<p>We can call pytest with <code class="docutils literal notranslate"><span class="pre">--durations=0</span></code> which will return the duration for all of the tests:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>❯<span class="w"> </span>pytest<span class="w"> </span>--durations<span class="o">=</span><span class="m">0</span><span class="w"> </span>tests/ordering/test_duration.py
<span class="o">=============================</span><span class="w"> </span><span class="nb">test</span><span class="w"> </span>session<span class="w"> </span><span class="nv">starts</span><span class="w"> </span><span class="o">==============================</span>
collected<span class="w"> </span><span class="m">3</span><span class="w"> </span>items

tests/ordering/test_duration.py<span class="w"> </span>...<span class="w">                                      </span><span class="o">[</span><span class="m">100</span>%<span class="o">]</span>

<span class="o">==============================</span><span class="w"> </span>slowest<span class="w"> </span><span class="nv">durations</span><span class="w"> </span><span class="o">===============================</span>
<span class="m">5</span>.00s<span class="w"> </span>call<span class="w">     </span>tests/ordering/test_duration.py::test_duration_5
<span class="m">3</span>.01s<span class="w"> </span>call<span class="w">     </span>tests/ordering/test_duration.py::test_duration_3
<span class="m">1</span>.00s<span class="w"> </span>call<span class="w">     </span>tests/ordering/test_duration.py::test_duration_1
<span class="o">==============================</span><span class="w"> </span><span class="m">3</span><span class="w"> </span>passed<span class="w"> </span><span class="k">in</span><span class="w"> </span><span class="m">9</span>.10s<span class="w"> </span><span class="o">===============================</span>
</pre></div>
</div>
<p>Using the results of this, we could impose ordering on test execution so that the slowest ones are executed last, or add a <code class="docutils literal notranslate"><span class="pre">slow</span></code> marker that we could exclude from our regular testing.</p>
<p>There are plugins for <code class="docutils literal notranslate"><span class="pre">pytest</span></code> that can measure the duration of each test and order tests accordingly, but they don’t seem to be very well maintained so I am avoiding them.
However, with agentic AI coding tools we can have our AI system make the appropriate call to pytest to obtain the durations and then add the appropriate ordering markers.
I tried this using the agentic chat window in VSCode (with Claude Sonnet 4).
On the first pass, Claude immediately noticed that it could infer the completion times directly from the <code class="docutils literal notranslate"><span class="pre">sleep()</span></code> commands in the code.
I asked it not to do this, and instead to use the outputs from <code class="docutils literal notranslate"><span class="pre">pytest</span> <span class="pre">--durations</span></code> but it had trouble waiting for the tests to finish, and ended up continuing to try to things that didn’t work.
However, Claude Code was able to successfully solve the problem; the first line marked with “&gt;” was my prompt, and the remainder is Claude Code’s output:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>&gt; the file tests/ordering/test_duration.py contains several
  tests.  Use pytest --duration to determine the durations for
  each of the tests in this file. DO NOT try to infer the 
  durations from the code itself. The tests will take about
  fifteen seconds to complete, so be sure to wait for them. 
  Then, add marks to each test using pytest-order to specify 
  that the fastest tests are executed first.

⏺ I&#39;ll run pytest with the --durations flag to measure the 
actual test durations, then add pytest-order marks to run the
fastest tests first.

⏺ Update Todos
  ⎿  ☐ Run pytest with --durations flag on test_duration.py
     ☐ Analyze test duration results
     ☐ Add pytest-order marks to tests based on duration

⏺ Bash(pytest tests/ordering/test_duration.py --durations=0 -v)
  ⎿ ============================= test session starts =====================
    =========
    platform darwin -- Python 3.12.0, pytest-8.4.1, pluggy-1.5.0 -- /Users/
    … +27 lines (ctrl+r to expand)

⏺ Based on the test durations:
  - test_duration_1: 1.00s (fastest)
  - test_duration_3: 3.00s
  - test_duration_5: 5.01s (slowest)

⏺ Now I&#39;ll add pytest-order marks to run tests from fastest to slowest:

⏺ Update(tests/ordering/test_duration.py)
  ⎿  Updated tests/ordering/test_duration.py with 3 additions
        2    from time import sleep
        3
        4
        5 +  @pytest.mark.order(2)
        6    def test_duration_3():
        7        sleep(3)
        8        assert True
        9
       10 +  @pytest.mark.order(3)
       11    def test_duration_5():
       12        sleep(5)
       13        assert True
       14
       15 +  @pytest.mark.order(1)
       16    def test_duration_1():
       17        sleep(1)
       18        assert True

⏺ Let me verify the ordering works correctly:

⏺ Bash(pytest tests/ordering/test_duration.py -v)
  ⎿ ============================= test session starts =====================
    =========
    platform darwin -- Python 3.12.0, pytest-8.4.1, pluggy-1.5.0 -- /Users/
    … +20 lines (ctrl+r to expand)

⏺ Perfect! The tests now run in order from fastest to slowest:
  1. test_duration_1 (1 second)
  2. test_duration_3 (3 seconds)
  3. test_duration_5 (5 seconds)

</pre></div>
</div>
<p>This is a great example of how we can increasingly rely upon AI coding models to solve problems that would once have required specialized tools.</p>
</section>
<section id="using-minimal-mock-datasets">
<h3><a class="toc-backref" href="#id33">Using minimal mock datasets</a><a class="headerlink" href="#using-minimal-mock-datasets" title="Permalink to this heading">#</a></h3>
<p>For code that performs data processing operations, the processing of full size datasets can often take a very long time.
One strategy in these cases is to generate minimal mock datasets that can exercise the functions without taking the full amount of time that a real dataset would.
In the tests for <code class="docutils literal notranslate"><span class="pre">fMRIPrep</span></code>, we use fMRI datasets that have been reduced in length, and structural MRI datasets that have been downsampled to reduce their spatial resolution.
The specific way to reduce the dataset will depend on the particular processes being run.
For example, downsampling the data too much for MRI preprocessing would likely cause some operations to fail, so one needs to have a good intuition for the data requirements for the relevant code.</p>
</section>
<section id="adding-minimal-processing-modes-for-integration-tests">
<h3><a class="toc-backref" href="#id34">Adding minimal processing modes for integration tests</a><a class="headerlink" href="#adding-minimal-processing-modes-for-integration-tests" title="Permalink to this heading">#</a></h3>
<p>When the goal is to the test the integration of components rather than the function of each component, one way to minimize testing time is to provide configuration features that minimize execution time for the component.
For example, in <code class="docutils literal notranslate"><span class="pre">fMRIPrep</span></code> there are a number of steps that involve optimization processes that take time to converge.
However, there the package has a “sloppy mode” configuration flag that one can turn on for testing, which provides a more lenient threshold for convergence of those operations, allowing them to finish faster.
Again, knowing where one can cut corners requires a good understanding of the specific requirements of the processing operations.</p>
</section>
<section id="parallelizing-testing">
<h3><a class="toc-backref" href="#id35">Parallelizing testing</a><a class="headerlink" href="#parallelizing-testing" title="Permalink to this heading">#</a></h3>
<p>If we have written good tests, they should be able to run independently, and thus their execution should be parallelizable, assuming that we are using a system with multiple CPU cores.
If we are using <code class="docutils literal notranslate"><span class="pre">pytest</span></code> as our testing framework, then we can use the <code class="docutils literal notranslate"><span class="pre">pytest-xdist</span></code> extension to enable the parallel execution of tests in pytest.
For example, let’s set up a parameterized test that includes a <code class="docutils literal notranslate"><span class="pre">time.sleep()</span></code> command so that execution will take a significant amount of time.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">pytest</span>
<span class="kn">import</span> <span class="nn">time</span>

<span class="nd">@pytest</span><span class="o">.</span><span class="n">mark</span><span class="o">.</span><span class="n">parametrize</span><span class="p">(</span><span class="s2">&quot;x&quot;</span><span class="p">,</span> <span class="nb">range</span><span class="p">(</span><span class="mi">10</span><span class="p">))</span>
<span class="k">def</span> <span class="nf">test_parallel</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="k">assert</span> <span class="n">x</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">10</span><span class="p">),</span> <span class="sa">f</span><span class="s2">&quot;Value </span><span class="si">{</span><span class="n">x</span><span class="si">}</span><span class="s2"> is not in the expected list.&quot;</span>
    <span class="n">time</span><span class="o">.</span><span class="n">sleep</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span> <span class="c1"># wait for one second</span>
</pre></div>
</div>
<p>If we run this using the standard <code class="docutils literal notranslate"><span class="pre">pytest</span></code> command, we should see that it takes about ten seconds, given that there are ten tests:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>❯<span class="w"> </span>pytest<span class="w"> </span>tests/parallel/tests_parallel.py
<span class="o">=============================</span><span class="w"> </span><span class="nb">test</span><span class="w"> </span>session<span class="w"> </span><span class="nv">starts</span><span class="w"> </span><span class="o">==============================</span>
collected<span class="w"> </span><span class="m">10</span><span class="w"> </span>items

tests/parallel/tests_parallel.py<span class="w"> </span>..........<span class="w">                              </span><span class="o">[</span><span class="m">100</span>%<span class="o">]</span>

<span class="o">=============================</span><span class="w"> </span><span class="m">10</span><span class="w"> </span>passed<span class="w"> </span><span class="k">in</span><span class="w"> </span><span class="m">10</span>.18s<span class="w"> </span><span class="o">==============================</span>
</pre></div>
</div>
<p>If we have installed <code class="docutils literal notranslate"><span class="pre">pytest-xdist</span></code> then we can add the <code class="docutils literal notranslate"><span class="pre">-n</span> <span class="pre">auto</span></code> flag which will automatically detect how manu CPU cores we have available and run the tests in parallel across those cores:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>❯<span class="w"> </span>pytest<span class="w"> </span>tests/parallel/tests_parallel.py<span class="w"> </span>-n<span class="w"> </span><span class="nv">auto</span>
<span class="o">=============================</span><span class="w"> </span><span class="nb">test</span><span class="w"> </span>session<span class="w"> </span><span class="nv">starts</span><span class="w"> </span><span class="o">==============================</span>
<span class="m">16</span><span class="w"> </span>workers<span class="w"> </span><span class="o">[</span><span class="m">10</span><span class="w"> </span>items<span class="o">]</span>
..........<span class="w">                                                               </span><span class="o">[</span><span class="m">100</span>%<span class="o">]</span>
<span class="o">==============================</span><span class="w"> </span><span class="m">10</span><span class="w"> </span>passed<span class="w"> </span><span class="k">in</span><span class="w"> </span><span class="m">1</span>.97s<span class="w"> </span><span class="o">==============================</span>
</pre></div>
</div>
<p>You can see that it detected the 16 cores in my laptop and ran the 10 tests in parallel, greatly reducing the testing time.</p>
<hr class="footnotes docutils" />
<dl class="footnote brackets">
<dt class="label" id="id4"><span class="brackets"><a class="fn-backref" href="#id1">1</a></span></dt>
<dd><p>This is slightly inaccurate, because a true positive control would contain the actual virus.
It would be more precise to call it a “procedural control” but these seem to be also referred to as “positive controls” so I am sticking with the more understandable terminology here.</p>
</dd>
<dt class="label" id="id5"><span class="brackets"><a class="fn-backref" href="#id3">2</a></span></dt>
<dd><p>As discussed in the earlier section on technical debt, I don’t think it’s generally a good policy to rely upon packages that one randomly finds on Pypi or GitHub.
Before recommending <code class="docutils literal notranslate"><span class="pre">python-order</span></code> as a possible solution, I looked at its <a class="reference external" href="https://github.com/pytest-dev/pytest-order">GitHub page</a>, where I saw that it appears to be a well-maintained and currently active package, with recent commits and solid handling of issues.
Conversely, during the course of writing I came across a number of other packages that had been recommended on Stack Overflow to solve various problems, some of which had not seen commits in several years or had longstanding unaddressed issues.</p>
</dd>
</dl>
</section>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./."
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="software_engineering.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">Principles of software engineering</p>
      </div>
    </a>
    <a class="right-next"
       href="AI_coding_assistants.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Coding with AI</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#why-use-software-tests">Why use software tests?</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#types-of-tests">Types of tests</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#unit-tests">Unit tests</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#integration-tests">Integration tests</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#the-anatomy-of-a-test">The anatomy of a test</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#when-to-write-tests">When to write tests</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#bug-driven-testing-any-time-you-encounter-a-bug-write-a-test">Bug-driven testing: Any time you encounter a bug, write a test</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#the-structure-of-a-good-test">The structure of a good test</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#test-against-the-interface-not-the-implementation">Test against the interface, not the implementation</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#tests-should-be-independent">Tests should be independent</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#testing-frameworks">Testing frameworks</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#potential-problems-with-ai-generated-tests">Potential problems with AI-generated tests</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#test-driven-development-and-ai-assisted-coding">Test-driven development and AI-assisted coding</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#test-coverage">Test coverage</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#test-fixtures">Test fixtures</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#mocking">Mocking</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#parametrized-tests">Parametrized tests</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#property-based-testing">Property-based testing</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#automated-testing-and-continuous-integration">Automated testing and continuous integration</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#using-github-actions">Using GitHub Actions</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#optimizing-the-testing-workflow">Optimizing the testing workflow</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#cherry-picking-specific-tests">Cherry-picking specific tests</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#controlling-test-execution-order">Controlling test execution order</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#only-run-failed-tests-or-run-them-first">Only run failed tests, or run them first</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#separate-unit-tests-from-longer-running-tests">Separate unit tests from longer-running tests</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#longest-tests-last">Longest tests last</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#using-minimal-mock-datasets">Using minimal mock datasets</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#adding-minimal-processing-modes-for-integration-tests">Adding minimal processing modes for integration tests</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#parallelizing-testing">Parallelizing testing</a></li>
</ul>
</li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Russell Poldrack et al.
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      © Copyright 2023.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b"></script>
<script src="_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>