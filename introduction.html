

<!DOCTYPE html>


<html lang="en" data-content_root="" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

    <title>Introduction &#8212; Better Code, Better Science</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="_static/styles/theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="_static/styles/bootstrap.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="_static/styles/pydata-sphinx-theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />

  
  <link href="_static/vendor/fontawesome/6.5.2/css/all.min.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.5.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.5.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.5.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="_static/pygments.css" />
    <link rel="stylesheet" type="text/css" href="_static/styles/sphinx-book-theme.css" />
    <link rel="stylesheet" type="text/css" href="_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-design.5ea377869091fd0449014c60fc090103.min.css" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b" />
<link rel="preload" as="script" href="_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b" />
  <script src="_static/vendor/fontawesome/6.5.2/js/all.min.js?digest=dfe6caa3a7d634c4db9b"></script>

    <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script>
    <script src="_static/jquery.js"></script>
    <script src="_static/underscore.js"></script>
    <script src="_static/_sphinx_javascript_frameworks_compat.js"></script>
    <script src="_static/doctools.js"></script>
    <script src="_static/sphinx_highlight.js"></script>
    <script src="_static/clipboard.min.js"></script>
    <script src="_static/copybutton.js"></script>
    <script src="_static/scripts/sphinx-book-theme.js"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="_static/togglebutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="_static/design-tabs.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script async="async" src="_static/sphinx-thebe.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'introduction';</script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Essential tools and techniques" href="essential_tools_and_techniques.html" />
    <link rel="prev" title="Better Code, Better Science" href="frontmatter.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-primary-sidebar-checkbox"/>
  <label class="overlay overlay-primary" for="pst-primary-sidebar-checkbox"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-secondary-sidebar-checkbox"/>
  <label class="overlay overlay-secondary" for="pst-secondary-sidebar-checkbox"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  
    
  

<a class="navbar-brand logo" href="frontmatter.html">
  
  
  
  
  
  
    <p class="title logo__title">Better Code, Better Science</p>
  
</a></div>
        <div class="sidebar-primary-item">

 <script>
 document.write(`
   <button class="btn search-button-field search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
    <span class="search-button__default-text">Search</span>
    <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
   </button>
 `);
 </script></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="frontmatter.html">
                    Better Code, Better Science
                </a>
            </li>
        </ul>
        <ul class="current nav bd-sidenav">
<li class="toctree-l1 current active"><a class="current reference internal" href="#">Introduction</a></li>
<li class="toctree-l1"><a class="reference internal" href="essential_tools_and_techniques.html">Essential tools and techniques</a></li>
<li class="toctree-l1"><a class="reference internal" href="software_engineering.html">Principles of software engineering</a></li>
<li class="toctree-l1"><a class="reference internal" href="testing.html">Software testing</a></li>
<li class="toctree-l1"><a class="reference internal" href="bibliography.html">Bibliography</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><button class="sidebar-toggle primary-toggle btn btn-sm" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</button></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-source-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Source repositories">
    <i class="fab fa-github"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://github.com/poldrack/BetterCodeBetterScience" target="_blank"
   class="btn btn-sm btn-source-repository-button dropdown-item"
   title="Source repository"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="btn__text-container">Repository</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/poldrack/BetterCodeBetterScience/issues/new?title=Issue%20on%20page%20%2Fintroduction.html&body=Your%20issue%20content%20here." target="_blank"
   class="btn btn-sm btn-source-issues-button dropdown-item"
   title="Open an issue"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="btn__text-container">Open issue</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="_sources/introduction.md" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.md</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light"></i>
    <i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark"></i>
    <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"></i>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm pst-navbar-icon search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<button class="sidebar-toggle secondary-toggle btn btn-sm" title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</button>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Introduction</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#why-poor-software-engineering-is-a-threat-to-science">Why poor software engineering is a threat to science</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#is-software-development-still-important-in-the-age-of-ai">Is software development still important in the age of AI?</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#why-better-code-can-mean-better-science">Why better code can mean better science</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#what-does-reproducibility-mean">What does “reproducibility” mean?</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#a-reproducibility-crisis">A reproducibility “crisis”</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#open-science-and-reproducibility">Open science and reproducibility</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#bug-hacking">Bug-hacking</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#how-not-to-fool-ourselves">How not to fool ourselves</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#beyond-reproducibility-getting-valid-answers">Beyond reproducibility: Getting valid answers</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#guiding-principles-for-this-book">Guiding principles for this book</a></li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="introduction">
<h1>Introduction<a class="headerlink" href="#introduction" title="Permalink to this heading">#</a></h1>
<p>There was a time when becoming a scientist meant developing and honing a set of very specific laboratory skills: a cell biologist would learn to culture cells and perform assays, a materials scientist would learn to use high-powered microscopes, and a sociologist would learn to develop surveys.
Each might also perform data analysis during the course of their research, but in most cases this was done using software packages that allowed them to enter their data and specify their analyses using a graphical user interface.
While many researchers knew how to program a computer, and for some fields it was necessary (e.g. to control recording instruments or run computer simulations), it was relatively uncommon for most scientists to spend a significant proportion of their day writing code.</p>
<p>How times have changed! In nearly every part of science today, working at the highest level requires the ability to write code.
While the preferred languages differ between different fields of science, it is rare for a graduate student to make it through graduate school today without having to spend some time writing code.
Whereas statistics classes before 2000 almost invariably taught the topic using statistical software packages with names that quickly becoming forgotten (SPSS, JMP), most graduate-level statistics classes are now taught using programming languages such as R, Python, or Stata.
The ubiquity of code has been accelerated even further by the increasing prevalence of machine learning techniques in science.
These techniques, which bring unprecedented analytic power to scientists, can only be tapped effectively by researchers with substantial coding skills.</p>
<p>The increasing prevalence of coding in scientific practice contrasts starkly with the lack of training that most researchers receive in software engineering.
By “software engineering” I don’t mean introductory classes in how to code in a particular language.
Rather, I am referring to the set of practices that have been developed within the field of computer science and engineering that aim to improve the quality and efficiency of the software development process and the resulting software products.
A glimpse into this field can be gotten from examining the <a class="reference external" href="https://www.computer.org/education/bodies-of-knowledge/software-engineering">Software Engineering Body of Knowledge (SWEBOK)</a>, first published in 2004 and updated most recently in 2024 (<em>NOTE: Not yet released but coming soon</em>).
While much of SWEBOK focuses on topics that are primarily relevant to large commercial software projects, it also includes numerous sections that are relevant to anyone writing code that aims to function correctly, such as how to test code for validity and how to maintain software once it has been developed.</p>
<p>One of us (RP) has spent the last decade giving talks on scientific coding practices.
He often starts by asking how many researchers in the audience have received software engineering training.
In most audiences the proportion of people raising their hands is well below 1/4; this is true both for the audiences of neuroscientists and psychologists that he usually speaks to, as well as researchers from other fields that he occasionally speaks to.
This impression is consistent with the results of a poll conducted on the social media platform X by author RP, which showed that the majority of scientists responded that they had received no training in software engineering (see <a class="reference internal" href="#xpoll-fig"><span class="std std-numref">Figure 1</span></a>).</p>
<figure class="align-default" id="xpoll-fig">
<a class="reference internal image-reference" href="_images/x_poll.png"><img alt="Social medial poll on software engineering training" src="_images/x_poll.png" style="width: 400px;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 1 </span><span class="caption-text">Results from a social media poll on the X platform about software engineering training for scientists who code.</span><a class="headerlink" href="#xpoll-fig" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
<p>Thus, a large number of scientists today are operating as amateur software engineers.</p>
<section id="why-poor-software-engineering-is-a-threat-to-science">
<h2>Why poor software engineering is a threat to science<a class="headerlink" href="#why-poor-software-engineering-is-a-threat-to-science" title="Permalink to this heading">#</a></h2>
<p>There is no such thing as bug-free code, even in domains where it really matters.
In 1999 a NASA space probe called the <em>Mars Climate Orbiter</em> was destroyed when measurements sent in the English unit of pound-seconds were mistakenly interpreted as being in the metric unit of newton-seconds, causing the spacecraft to be destroyed when it veered to close to the Martial atmosphere.
The total amount lost on the project was over $300 million.
Another NASA disaster occurred just a few months later when the <em>Mars Polar Lander</em> lost communication during its landing procedure on Mars.
This is now thought to be due to software design errors rather than to a bug per se:</p>
<blockquote>
<div><p>The cause of the communication loss is not known.
However, the Failure Review Board concluded that the most likely cause of the mishap was a software error that incorrectly identified vibrations, caused by the deployment of the stowed legs, as surface touchdown.
The resulting action by the spacecraft was the shutdown of the descent engines, while still likely 40 meters above the surface.
Although it was known that leg deployment could create the false indication, the software’s design instructions did not account for that eventuality. <a class="reference external" href="https://en.wikipedia.org/wiki/Mars_Polar_Lander">Wikipedia</a>.</p>
</div></blockquote>
<p>Studies of error rates in computer programs have consistently shown that code generated by professional coders has an error rate of 1-2 errors per 100 lines of code <span id="id1">[<a class="reference internal" href="bibliography.html#id25" title="John Symons and Jack K. Horner. Why there is no general solution to the problem of software verification. Foundations of Science, 25(3):541–557, 2020. URL: https://doi.org/10.1007/s10699-019-09611-w, doi:10.1007/s10699-019-09611-w.">Symons and Horner, 2020</a>]</span>.
Interestingly, the most commonly reported type of error in the analyses reported by Horner and Symons was “Misunderstanding the specification”; that is, the problem was correctly described but the programmer incorrectly interpreted this description.
Other common types of errors included numerical errors, logical errors, and memory management errors.</p>
<p>If professional coders make errors at a rate of 1-2 errors per hundred lines, it seems very likely that the error rates of amateur coders writing software for their research would be substantially higher.
While not all coding errors will make a difference in the final calculation, it’s likely that many will <span id="id2">[<a class="reference internal" href="bibliography.html#id24" title="David A W Soergel. Rampant software errors may undermine scientific results. F1000Res, 3:303, 2014. doi:10.12688/f1000research.5930.2.">Soergel, 2014</a>]</span>.
We have in fact experienced this within our own work (As described in <a class="reference external" href="https://reproducibility.stanford.edu/coding-error-postmortem/">this blog post</a>).
In 2020 we posted a preprint that criticized the design of a particular aspect of a large NIH-funded study, the Adolescent Brain Cognitive Development (ABCD) study.
This dataset is shared with researchers, and we also made the code openly available via GitHub.
The ABCD team eagerly reviewed our code, and discovered an error due to an overly complex index scheme with double negatives to that led to incorrect indexing of a data frame and thus changed the results.
This fortunately happened while the paper was under review, so we were able to motify the journal and modify the paper to reflect the bug fix; if we had not made the code publicly available then the error would either have never been caught, or would have been caught after publication, leading to the need for a published correction.</p>
<p>This was a minor error, but there are also prominent examples of scientific claims that suffered catastrophic errors due to software errors.
The best known is the case of Geoffrey Chang, a structural biologist who published several papers in the early 2000’s examining the structure of a protein called the ABC transporter.
Chang’s group had to retract 5 papers, including 3 published in the prestigious journal <em>Science</em>, after learning that their custom analysis code had mistakenly flipped two columns of data, which ultimately led to an erroneous estimate of the protein structure <span id="id3">[<a class="reference internal" href="bibliography.html#id23" title="Greg Miller. Scientific publishing. a scientist's nightmare: software problem leads to five retractions. Science, 314(5807):1856-7, Dec 2006. doi:10.1126/science.314.5807.1856.">Miller, 2006</a>]</span>.
This is an example of how a very simple coding mistake can have major scientific consequences.</p>
</section>
<section id="is-software-development-still-important-in-the-age-of-ai">
<h2>Is software development still important in the age of AI?<a class="headerlink" href="#is-software-development-still-important-in-the-age-of-ai" title="Permalink to this heading">#</a></h2>
<p>It would be an understatement to say that coding has undergone a revolution since the introduction of coding assistance tools based on artificial intellgence (AI) systems.
While some degree of assistance has long been available (such as smart autocompletion by code editors), the introduction of Copilot by GitHub in 2021 and its subsequent incorporation into a number of integrated development environments (IDEs) brought a new level automation into the coding process.
There are likely very few coders today who have not used these tools or at least tried them out.
The presence of these tools has led to some breathless hand-wringing about whether AI will eliminate the need for programmers, but nearly all voices on this topic agree that programming will change drastically with the introduction of AI assistance but that the ability to program will remain a foundational skill for years to come.</p>
<p>In early 2023 the frenzy about AI reached the boiling point with the introduction of the GPT-4 language model by OpenAI.
Analyses of early versions of this model <span id="id4">[<a class="reference internal" href="bibliography.html#id18" title="Sébastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes Gehrke, Eric Horvitz, Ece Kamar, Peter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lundberg, Harsha Nori, Hamid Palangi, Marco Tulio Ribeiro, and Yi Zhang. Sparks of artificial general intelligence: early experiments with gpt-4. 2023. URL: https://arxiv.org/abs/2303.12712, arXiv:2303.12712.">Bubeck <em>et al.</em>, 2023</a>]</span> showed that its ability to solve computer programming problems was on pair with human coders, and much better than the previous GPT-3 model.
Later that year GPT-4 became available as part of the GitHub CoPilot AI assistant, and those of us using Copilot saw some quite astonishing improvements in the performance of the model compared to the GPT-3 version.
One of us (RP) had been developing a workshop on software engineering practices for scientists, and the advent of this new tool led to some deep soul-searching about whether such training would even be necessary given the power of AI coding assistants.
He and his colleagues <span id="id5">[<a class="reference internal" href="bibliography.html#id17" title="Russell A Poldrack, Thomas Lu, and Gašper Beguš. Ai-assisted coding: experiments with gpt-4. 2023. URL: https://arxiv.org/abs/2304.13187, arXiv:2304.13187.">Poldrack <em>et al.</em>, 2023</a>]</span> subsequently performed a set of analyses that asked three questions about the ability of GPT-4 to perform scientific coding.
We will describe these experiments in more detail in Chapter XXX, but in short, they showed that while GPT-4 can solve many coding problems quite effectively, it is far from being able to solve common coding problems completely on its own.</p>
<p>We believe that AI coding assistants have the potential to greatly improve the experience of coding and to help new coders learn how to code effectively.
However, our experiences with AI-assisted coding have also led us to the conclusion that software engineering skills will remain <em>at least</em> as important in the future as they are now.
First, and most importantly, the hardest problem in programming is not the generation of code; rather, it is the decomposition of the problem into a set of steps that can be used to generate code.
It is no accident that the section of SWEBOK on “Computing Foundations” starts with the following:</p>
<ul class="simple">
<li><p>Problem Solving Techniques</p>
<ul>
<li><p>Definition of Problem Solving</p></li>
<li><p>Formulating the Real Problem</p></li>
<li><p>Analyze the Problem</p></li>
<li><p>Design a Solution Search Strategy</p></li>
<li><p>Problem Solving Using Programs</p></li>
</ul>
</li>
</ul>
<p>The motivation for why coding will remain as an essential skill even if code is no longer being written by humans was expressed by Robert Martin in 2009, well before the current AI tools were even imaginable:</p>
<blockquote>
<div><p>…some have suggested that we are close to the end of code.
That soon all code will be generated instead of written.
That programmers simply won’t be needed because business people will generate programs from specifications.
Nonsense! We will never be rid of code, because code represents the details of the requirements.
At some level those details cannot be ignored or abstracted; they have to be specified.
And specifying requirements in such detail that a machine can execute them is programming.
Such a specification is code. <span id="id6">[<a class="reference internal" href="bibliography.html#id16" title="Robert C Martin. Clean code: a handbook of agile software craftsmanship. Prentice Hall, Upper Saddle River, NJ, 2009. ISBN 0132350882 (pbk. : alk. paper). URL: http://www.loc.gov/catdir/toc/ecip0820/2008024750.html.">Martin, 2009</a>]</span></p>
</div></blockquote>
<p>For simple common problems AI tools may be able to generate a complete and accurate solution, but for the more complex problems that most scientists face, a combination of coding skills and domain knowledge will remain essential to figuring out how to decompose a problem and express it in a way that a computer can solve it.
Even if that involves generating prompts for a generative AI model rather than generating code de novo, a deep understanding of the generated code will be essential to making sure that the code accurately solves the problem at hand.</p>
<p>Second, scientific coding requires an extra level of accuracy.
Scientific research forms the basis for many important decisions in our society, from which medicines to prescribe to how effective a particular method for energy generation will be.
The technical report for GPT-4 makes it clear that we should think twice about an unquestioning reliance upon AI models in these kinds of situations:</p>
<blockquote>
<div><p>Care should be taken when using the outputs of GPT-4, particularly in contexts where reliability is important. <span id="id7">[<a class="reference internal" href="bibliography.html#id13" title="OpenAI, Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, Red Avila, Igor Babuschkin, Suchir Balaji, Valerie Balcom, Paul Baltescu, Haiming Bao, Mohammad Bavarian, Jeff Belgum, Irwan Bello, Jake Berdine, Gabriel Bernadett-Shapiro, Christopher Berner, Lenny Bogdonoff, Oleg Boiko, Madelaine Boyd, Anna-Luisa Brakman, Greg Brockman, Tim Brooks, Miles Brundage, Kevin Button, Trevor Cai, Rosie Campbell, Andrew Cann, Brittany Carey, Chelsea Carlson, Rory Carmichael, Brooke Chan, Che Chang, Fotis Chantzis, Derek Chen, Sully Chen, Ruby Chen, Jason Chen, Mark Chen, Ben Chess, Chester Cho, Casey Chu, Hyung Won Chung, Dave Cummings, Jeremiah Currier, Yunxing Dai, Cory Decareaux, Thomas Degry, Noah Deutsch, Damien Deville, Arka Dhar, David Dohan, Steve Dowling, Sheila Dunning, Adrien Ecoffet, Atty Eleti, Tyna Eloundou, David Farhi, Liam Fedus, Niko Felix, Simón Posada Fishman, Juston Forte, Isabella Fulford, Leo Gao, Elie Georges, Christian Gibson, Vik Goel, Tarun Gogineni, Gabriel Goh, Rapha Gontijo-Lopes, Jonathan Gordon, Morgan Grafstein, Scott Gray, Ryan Greene, Joshua Gross, Shixiang Shane Gu, Yufei Guo, Chris Hallacy, Jesse Han, Jeff Harris, Yuchen He, Mike Heaton, Johannes Heidecke, Chris Hesse, Alan Hickey, Wade Hickey, Peter Hoeschele, Brandon Houghton, Kenny Hsu, Shengli Hu, Xin Hu, Joost Huizinga, Shantanu Jain, Shawn Jain, Joanne Jang, Angela Jiang, Roger Jiang, Haozhun Jin, Denny Jin, Shino Jomoto, Billie Jonn, Heewoo Jun, Tomer Kaftan, Łukasz Kaiser, Ali Kamali, Ingmar Kanitscheider, Nitish Shirish Keskar, Tabarak Khan, Logan Kilpatrick, Jong Wook Kim, Christina Kim, Yongjik Kim, Jan Hendrik Kirchner, Jamie Kiros, Matt Knight, Daniel Kokotajlo, Łukasz Kondraciuk, Andrew Kondrich, Aris Konstantinidis, Kyle Kosic, Gretchen Krueger, Vishal Kuo, Michael Lampe, Ikai Lan, Teddy Lee, Jan Leike, Jade Leung, Daniel Levy, Chak Ming Li, Rachel Lim, Molly Lin, Stephanie Lin, Mateusz Litwin, Theresa Lopez, Ryan Lowe, Patricia Lue, Anna Makanju, Kim Malfacini, Sam Manning, Todor Markov, Yaniv Markovski, Bianca Martin, Katie Mayer, Andrew Mayne, Bob McGrew, Scott Mayer McKinney, Christine McLeavey, Paul McMillan, Jake McNeil, David Medina, Aalok Mehta, Jacob Menick, Luke Metz, Andrey Mishchenko, Pamela Mishkin, Vinnie Monaco, Evan Morikawa, Daniel Mossing, Tong Mu, Mira Murati, Oleg Murk, David Mély, Ashvin Nair, Reiichiro Nakano, Rajeev Nayak, Arvind Neelakantan, Richard Ngo, Hyeonwoo Noh, Long Ouyang, Cullen O'Keefe, Jakub Pachocki, Alex Paino, Joe Palermo, Ashley Pantuliano, Giambattista Parascandolo, Joel Parish, Emy Parparita, Alex Passos, Mikhail Pavlov, Andrew Peng, Adam Perelman, Filipe de Avila Belbute Peres, Michael Petrov, Henrique Ponde de Oliveira Pinto, Michael, Pokorny, Michelle Pokrass, Vitchyr H. Pong, Tolly Powell, Alethea Power, Boris Power, Elizabeth Proehl, Raul Puri, Alec Radford, Jack Rae, Aditya Ramesh, Cameron Raymond, Francis Real, Kendra Rimbach, Carl Ross, Bob Rotsted, Henri Roussez, Nick Ryder, Mario Saltarelli, Ted Sanders, Shibani Santurkar, Girish Sastry, Heather Schmidt, David Schnurr, John Schulman, Daniel Selsam, Kyla Sheppard, Toki Sherbakov, Jessica Shieh, Sarah Shoker, Pranav Shyam, Szymon Sidor, Eric Sigler, Maddie Simens, Jordan Sitkin, Katarina Slama, Ian Sohl, Benjamin Sokolowsky, Yang Song, Natalie Staudacher, Felipe Petroski Such, Natalie Summers, Ilya Sutskever, Jie Tang, Nikolas Tezak, Madeleine B. Thompson, Phil Tillet, Amin Tootoonchian, Elizabeth Tseng, Preston Tuggle, Nick Turley, Jerry Tworek, Juan Felipe Cerón Uribe, Andrea Vallone, Arun Vijayvergiya, Chelsea Voss, Carroll Wainwright, Justin Jay Wang, Alvin Wang, Ben Wang, Jonathan Ward, Jason Wei, CJ Weinmann, Akila Welihinda, Peter Welinder, Jiayi Weng, Lilian Weng, Matt Wiethoff, Dave Willner, Clemens Winter, Samuel Wolrich, Hannah Wong, Lauren Workman, Sherwin Wu, Jeff Wu, Michael Wu, Kai Xiao, Tao Xu, Sarah Yoo, Kevin Yu, Qiming Yuan, Wojciech Zaremba, Rowan Zellers, Chong Zhang, Marvin Zhang, Shengjia Zhao, Tianhao Zheng, Juntang Zhuang, William Zhuk, and Barret Zoph. Gpt-4 technical report. 2024. URL: https://arxiv.org/abs/2303.08774, arXiv:2303.08774.">OpenAI <em>et al.</em>, 2024</a>]</span></p>
</div></blockquote>
<p>The goal of this book is to show you how to generate code that can reliably solve scientific problems while taking advantage of all of the positive benefits of AI coding assistants.</p>
</section>
<section id="why-better-code-can-mean-better-science">
<h2>Why better code can mean better science<a class="headerlink" href="#why-better-code-can-mean-better-science" title="Permalink to this heading">#</a></h2>
<p>One of our main motivations for writing this book is to help make science better.
In particular, we want to increase the <em>reproducibility</em> of scientific results.
But what does <em>reproducibility</em> mean? And for that matter, what does it even mean for an activity to count as “science”?</p>
<p>This seemingly simple question turns out to be remarkably difficult to answer in a satisfying way.
Although it is easy to give examples of forms of inquiry that we we think are scientific (astrophysics, molecular biology) and forms that we think are not (astrology, creationism), defining an “essence” of science has eluded philosophers of science who have examined this question (known as the <a class="reference external" href="https://plato.stanford.edu/entries/pseudo-science/">“demarcation problem”</a>) over the last century.
One answer is that there are a set of features that together distinguish between scientific and non- or pseudo-scientific enterprises.
Some of these have to do with the social characteristics of the enterprise, best described in terms of the features that are often found in pseudoscience, such as an overriding belief in authority and the willingness to disregard information that contradicts the theory.
But nearly everyone agrees that an essential aspect of science is the ability to reproduce results reported by others.
The idea of replication as a sine qua non of science goes back to the 17th Century, when Christian Huygens built an air pump based on the designs of Robert Boyle and demonstrated a phenomenon called “anomalous suspension” that initially could not be replicated by Boyle, leading Huygens to ultimately travel to London and demonstrate the phenomenon directly <span id="id8">[<a class="reference internal" href="bibliography.html#id12" title="Steven Shapin, Simon Schaffer, and Thomas Hobbes. Leviathan and the air-pump: Hobbes, Boyle, and the experimental life : including a translation of Thomas Hobbes, Dialogus physicus de natura aeris by Simon Schaffer. Princeton University Press, Princeton, N.J., 1985. ISBN 0691083932. URL: http://www.loc.gov/catdir/enhancements/fy1409/85042705-d.html.">Shapin <em>et al.</em>, 1985</a>]</span>.
Throughout the development of modern science, the ability for researchers to replicate results by other scientists has been a foundational feature of science.</p>
<p>An example serves to show how well science can work when the stakes are high.
In 1989, the chemists Martin Fleischmann and Stanley Pons reported that they had achieved nuclear fusion at temperatures much less than usually thought to be required.
If true this would have been a revolutionary new source of energy for the world, so scientists quickly began to try to reproduce the result.
Within just a few months, the idea of “cold fusion” was full discredited, while the New York Times labeled the cold fusion work as an example of <a class="reference external" href="https://www.nytimes.com/1989/09/24/magazine/cold-fusion-confusion.html">“pathological science”</a>, the entire episode showed how well science can sometimes self-correct.</p>
<p>Cold fusion was a case in which science worked as it should, with other researchers quickly trying (and in this case failing) to reproduce a result.
However, there are other cases in which scientific claims have lingered for years, only to be discredited when examined deeply enough.
Within psychology, a well known example comes from the study of “ego depletion”, a phenomenon in which exerting self control in one domain was thought to “deplete” the ability to exert self control in a different domain.
This phenomenon was first reported in 1994 by a group of researchers who reported that giving people a difficult mental task to solve made them more likely to eat a cookie on their way out of the laboratory, compared to people who didn’t have to solve the task.
Hundreds of papers were published on the phenomenon over the subsequent decades, mostly using more simple laboratory tasks that didn’t require a kitchen and oven in the laboratory to bake cookies.
Nearly all of these subsequent studies reported to have found ego depletion effects.
But two large-scale efforts to reproduce the finding, including data from more than 5,000 participants, have shown that the effect is so small as to likely be non-existent.
Below we will talk more about the reasons that we now think these kinds of irreproducible findings can come about.</p>
<section id="what-does-reproducibility-mean">
<h3>What does “reproducibility” mean?<a class="headerlink" href="#what-does-reproducibility-mean" title="Permalink to this heading">#</a></h3>
<p>There are many different senses of the term “reproducibility”, which can cause confusion.
A framework that we like for this concept comes from the <a class="reference external" href="https://book.the-turing-way.org/">Turing Way</a>, an outstanding guide for open and reproducible science practices.
This framework, shown in <a class="reference internal" href="#turingwayreproducibility-fig"><span class="std std-numref">Figure 2</span></a>, distinguishes between whether the data and analysis are either same or different between two analyses.</p>
<figure class="align-default" id="turingwayreproducibility-fig">
<a class="reference internal image-reference" href="https://book.the-turing-way.org/build/reproducible-matrix-1f714c1d292747c95fe172a47d6a0937.jpg"><img alt="Schematic of Turing Way reproducibility framework" src="https://book.the-turing-way.org/build/reproducible-matrix-1f714c1d292747c95fe172a47d6a0937.jpg" style="width: 500px;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 2 </span><span class="caption-text">A schematic of the Turing Way framework for different concepts of reproducibility.
Reproduced from The Turing Way under CC-BY.</span><a class="headerlink" href="#turingwayreproducibility-fig" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
<p>People sometimes get hung up on these terminological differences, but that’s often just a distraction from the central point: We want to ensure that scientific research generates answers to questions that can generalize to a broad range of situations beyond the initial study.</p>
</section>
<section id="a-reproducibility-crisis">
<h3>A reproducibility “crisis”<a class="headerlink" href="#a-reproducibility-crisis" title="Permalink to this heading">#</a></h3>
<p>Starting in the early 2010’s, scientists became deeply concerned about whether results in their fields were reproducible, focusing primarily on the concept of <em>replication</em>; that is, whether another researcher could collect a new dataset using the same method and achieve the same answer using the same analysis approach.
While this concern spanned many different domains of science, the field of psychology was most prominent in tackling it head-on.
A large consortium banded together in an attempt to replicate the findings from 100 published psychology papers, and the results were startling <span id="id9">[<a class="reference internal" href="bibliography.html#id31" title="Open Science Collaboration. Psychology. estimating the reproducibility of psychological science. Science, 349(6251):aac4716, Aug 2015. doi:10.1126/science.aac4716.">Open Science Collaboration, 2015</a>]</span>: Whereas 97 of the original studies had reported statistically significant results, only 36% of the replication attempts reported a significant finding.
This finding led to a firestorm of criticism and rebuttal, but ultimately other attempts have similarly shown that a substantial portion of published psychology findings cannot be replicated, leading to what was termed a “reproducibility crisis” in psychology <span id="id10">[<a class="reference internal" href="bibliography.html#id30" title="Brian A Nosek, Tom E Hardwicke, Hannah Moshontz, Aurélien Allard, Katherine S Corker, Anna Dreber, Fiona Fidler, Joe Hilgard, Melissa Kline Struhl, Michèle B Nuijten, Julia M Rohrer, Felipe Romero, Anne M Scheel, Laura D Scherer, Felix D Schönbrodt, and Simine Vazire. Replicability, robustness, and reproducibility in psychological science. Annu Rev Psychol, 73:719-748, Jan 2022. doi:10.1146/annurev-psych-020821-114157.">Nosek <em>et al.</em>, 2022</a>]</span>.
Similar efforts subsequently uncovered problems in other areas, such as cancer biology <span id="id11">[<a class="reference internal" href="bibliography.html#id29" title="Timothy M Errington, Maya Mathur, Courtney K Soderberg, Alexandria Denis, Nicole Perfito, Elizabeth Iorns, and Brian A Nosek. Investigating the replicability of preclinical cancer biology. Elife, 10:e71601, Dec 2021. doi:10.7554/eLife.71601.">Errington <em>et al.</em>, 2021</a>]</span>, where it was only possible to even complete a replication of about 1/4 of the intended studies due to a lack of critical details in the published studies and lack of cooperation by about 1/3 the original authors.</p>
<p>As this crisis unfolded, attention turned to the potential causes for such a lack of reproducibility.
One major focus was the role of “questionable research practices” (QRPs) - practices that have the potential to decrease the reproducibility of research.
In a prominent 2011 article titled “False Positive Psychology”, <span id="id12"></span> showed that commonly used practices within psychology have the potential to substantially inflate the false positive rates of research studies.
Given the prominence of statistical hypothesis testing and the bias towards positive and statistically significant results (usually at p &lt; .05) in the psychology literature, these practices were termed “p-hacking”.
Many of the efforts to improve reproducibilty have focused on reducing the prevalence of p-hacking, such as the pre-registration of hypotheses and data analyses.</p>
</section>
<section id="open-science-and-reproducibility">
<h3>Open science and reproducibility<a class="headerlink" href="#open-science-and-reproducibility" title="Permalink to this heading">#</a></h3>
<p>There is a well known quote from Jonathan Buckheit and David Donoho <span id="id13">[<a class="reference internal" href="bibliography.html#id26">Buckheit and Donoho, 1995</a>]</span> that highlights the importance of openly available research objects in science:</p>
<blockquote>
<div><p>An article about computational science in a scientic publication is not the scholarship itself, it is merely advertising of the scholarship.
The actual scholarship is the complete software development environment and the complete set of instructions which generated the figures.</p>
</div></blockquote>
<p>There was surprisingly little focus on code during the reproducibility crisis, but it is clear that there are problems even with what would seem like the easiest quadrant of the Turing Way framework: namely, the ability to reproduce results give the same data and same analysis code.
Tom Hardwicke, Michael Frank and colleagues have examined the ease of reproducing results from psychology papers where both the data are openly available, and the results were not encouraging.
In one analysis <span id="id14">[<a class="reference internal" href="bibliography.html#id27" title="Tom E Hardwicke, Manuel Bohn, Kyle MacDonald, Emily Hembacher, Michèle B Nuijten, Benjamin N Peloquin, Benjamin E deMayo, Bria Long, Erica J Yoon, and Michael C Frank. Analytic reproducibility in articles receiving open data badges at the journal psychological science: an observational study. R Soc Open Sci, 8(1):201494, Jan 2021. doi:10.1098/rsos.201494.">Hardwicke <em>et al.</em>, 2021</a>]</span>, they attempted to reproduce the published results from 25 papers with open data.
Their initial analyses showed major numerical discrepancies in about 2/3 of the papers; strikingly, for about 1/4 of the papers they were unable to reproduce the values reported in the original publication <em>even with the help of the authors</em>!</p>
<p>It is increasingly common for researchers to share both code and data from their published research articles, in part due to incentives such as “badges” that are offered by some journals.
However, in our experience, it can be very difficult to actually run the shared code, due to various problems that limit the portability of the code.
Throughout this book we will discuss the tools and techniques that can help improve the portability of shared code and thus increase the reproducibility of published results.</p>
</section>
<section id="bug-hacking">
<h3>Bug-hacking<a class="headerlink" href="#bug-hacking" title="Permalink to this heading">#</a></h3>
<p>A particular concern is that not all software errors are created equal.
Imagine that a graduate student is comparing the performance of a new machine learning method that they developed with their implementation of a previous method.
Unbeknownst to them, their implementation of the previous method contains an error.
If the error results in poorer performance of the previous method (thus giving their new method the edge), then they are less likely to go looking for a bug than they might be if the error caused performance of the competing method to be inaccurately high.
We have referred to this before as “bug-hacking”, and this problem is nicely exemplified by the comic strip shown in <a class="reference internal" href="#bughacking-fig"><span class="std std-numref">Figure 3</span></a>.</p>
<figure class="align-default" id="bughacking-fig">
<a class="reference internal image-reference" href="http://www.phdcomics.com/comics/archive/phd033114s.gif"><img alt="PhD Comic about bug-hacking" src="http://www.phdcomics.com/comics/archive/phd033114s.gif" style="width: 500px;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 3 </span><span class="caption-text">A comic strip from PhD Comics demonstrating the concept of “bug-hacking”.
Image copyright Jorge Cham; Permission to reproduce pending.</span><a class="headerlink" href="#bughacking-fig" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
<p>There are multiple apparent examples of bug-hacking in the literature.
On example was identified by Mark Styczynski and his colleagues <span id="id15">[<a class="reference internal" href="bibliography.html#id22" title="Mark P Styczynski, Kyle L Jensen, Isidore Rigoutsos, and Gregory Stephanopoulos. Blosum62 miscalculations improve search performance. Nature Biotechnology, 26(3):274–275, 2008. URL: https://doi.org/10.1038/nbt0308-274, doi:10.1038/nbt0308-274.">Styczynski <em>et al.</em>, 2008</a>]</span> when they examined a set of substitution matrices known as the BLOSUM family that are comionly used in bioinformatics analyses.
A set of these matrices were initially created and shared in 1992 and widely used in the field for 15 years before Styczynski et al. discovered that they were in error.
These errors appeared to have significant impact on results, but interestingly the incorrect matrices actually performed <em>better</em> than the correct matrices in terms of the number of errors in biological sequence alignments.
It seems highly likely that a bug that had substantially reduced performance would have been identified much earlier.</p>
<p>Another example comes from our own field of neuroimaging.
A typical neuroimaging study collects data from hundreds of thousands of three-dimensional volumetric pixels (know as <em>voxels</em>) within the brain, and then performs statsitical tests at each of those locations.
This requires a correction for multiple tests to prevent the statistical error rate from skyrocketing simply due to the large number of tests.
There are a number of different methods that are implemented in different software packages, some of which rely upon mathematical theory and others of which rely upon resampling or simulation.
One of the commonly used open source software packages, AFNI, provided a tool called <em>3DClustSim</em> that used simulation to estimate a statistical correction for multiple comparisons.
This tool was commonly used in the neuroimaging literature, even by researchers who otherwise did not use the AFNI software, and the lore developed that 3DClustSim was less conservative than other tools.
When Anders Eklund and Tom Nichols <span id="id16">[<a class="reference internal" href="bibliography.html#id21" title="Anders Eklund, Thomas E Nichols, and Hans Knutsson. Cluster failure: why fmri inferences for spatial extent have inflated false-positive rates. Proc Natl Acad Sci U S A, 113(28):7900-5, Jul 2016. doi:10.1073/pnas.1602413113.">Eklund <em>et al.</em>, 2016</a>]</span> analyzed the performance of several different tools for multiple test correction, they identified a bug in the way that the 3DClustSim tool performed a particular rescaling operation, which led in some cases to inflated false positive rates.
This bug had existed in the code for 15 years, and almost certainly was being leveraged by researchers to obtain “better” results (i.e. results with more seeming discoveries).
Had the bug led to much more conservative results compared to other standard methods, it is likely that users would have complained and the problem would have been investigated; in the event, no users complained about getting more apparent discoveries in their analyses.</p>
</section>
<section id="how-not-to-fool-ourselves">
<h3>How not to fool ourselves<a class="headerlink" href="#how-not-to-fool-ourselves" title="Permalink to this heading">#</a></h3>
<p>In his <a class="reference external" href="https://calteches.library.caltech.edu/51/2/CargoCult.htm">1974 commencement address at Caltech</a>, the physicist Richard Feynman famously said “The first principle is that you must not fool yourself — and you are the easiest person to fool.” One of the most powerful ways that scientists have developed to prevent us from fooling ourselves is <em>blinding</em> - that is, preventing us from seeing or otherwise knowing information that could lead us to be biased towards our own hypotheses.
You may be familiar, for example, of the idea of a “double-blind” randomized controlled trial in medical research, in which participants are randomly assigned to a treatment of interest or a control condition (such as a placebo); the “double-blind” aspect of the trial refers to the fact that neither the patient nor the resarcher knows who has been assigned to the treatment versus control condition.
Assuming that blinding actually works (which can fail, for example, if the treatment has strong side effects), this can give results that are at much lower risk of bias compared to a trial in which the physician or patient know what their condition is.
In physics, researchers will regularly relabel or otherwise modify the data to prevent the researcher from knowing whether they are working with the real data versus some other version.
This kind of blinding helps researchers avoid fooling themselves.</p>
<p>A major concern in the development of software for data analysis is that the researcher will make choices that are data-dependent.
Andrew Gelman and Eric Loken <span id="id17">[<a class="reference internal" href="bibliography.html#id20">Gelman and Loken, 2019</a>]</span> referred to a “garden of forking paths”, in which the researcher makes seemingly innocuous data-driven decisions about the methods to apply for analysis, resulting in an injection of bias into the analysis.
One commonly recommended solution for this is “pre-registration”, in which the methods to be applied to the data are pre-specified before any contact is made with the data.
There are several platforms (including the Open Science Framework, <a class="reference external" href="http://ClinicalTrials.gov">ClinicalTrials.gov</a>, and <a class="reference external" href="http://AsPredicted.org">AsPredicted.org</a>, depending on the type of research) that can be used to pre-register analysis plans and code prior to their application to real data.
Pre-registration has been used in medical research for more than two decades, and its introduction was associated with a substantial reduction in the prevalence of positive outcomes in clinical trials, presumably reflecting the reduction in bias <span id="id18">[<a class="reference internal" href="bibliography.html#id19" title="Robert M Kaplan and Veronica L Irvin. Likelihood of null effects of large nhlbi clinical trials has increased over time. PLoS One, 10(8):e0132382, 2015. doi:10.1371/journal.pone.0132382.">Kaplan and Irvin, 2015</a>]</span>.
However, pre-registration can be challenging when the data are complex and the analytic methods are not clear from the outset.
How can a researcher avoid bias while still making sure that the analyses are optimal for their data? There are several possible solutions, whose applicability will depend upon the specific features of the data in question.</p>
<p>One solution is to set aside a portion of the data (which we call the “discovery” dataset) for code development, holding aside a “validation dataset” that remains locked away until the analysis code is fixed (and preferably pre-registered).
This allows the researcher to use the discovery dataset to develop the analysis code, ensuring that the code is well matched to the features of the dataset.
As long as contact with the validation dataset is scrupulously avoided during the discovery phase, this can prevent analyses of the validation dataset from being biased by the specific features of those data.
The main challenge of this approach comes about when the dataset is not large enough to split into two parts.
One adaptation of this approach is to use pilot data or data that were discarded in the initial phase of data cleaning (e.g. due to data quality issues) as the discovery sample, realizing that these data will likely differ in systematic ways from the validation set.</p>
<p><strong>TODO</strong>:</p>
<ul class="simple">
<li><p>Another solution is to modify the data in order to blind the researcher.  TBD</p></li>
<li><p>Yet another solution is to generate simulated data that are then used to develop the analysis code. TBD</p></li>
</ul>
</section>
</section>
<section id="beyond-reproducibility-getting-valid-answers">
<h2>Beyond reproducibility: Getting valid answers<a class="headerlink" href="#beyond-reproducibility-getting-valid-answers" title="Permalink to this heading">#</a></h2>
<p>Our discussion so far has focused on reproducibilty, but it is important to point out that a result can be completely reproducible yet wrong.
A degenerate example is a data analysis program that always outputs zeros for any analysis regardless of the data; it will be perfectly reproducible, with the same data or different data, yet also perfectly wrong! This distinction goes by various names; we will adopt the terminology of “reliability” versus “validity” that is commonly used in many fields including psychology.
A measurement is considered to be <em>reliable</em> if repeated measurements of the same type give similar answers; a measurement is perfectly reliable if it gives exactly the same answer each time it is performed, and increasing error variance leads to lower reliability.
On the other hand, a measurement is considered to be <em>valid</em> if it accurately indexes the underlying feature that it is intended to measure; that is, the measure is <em>unbiased</em> with respect to the ground truth.</p>
<p>The distinction between reliability and validity implies that we can’t simply focus on making our analyses reproducible; we also need to make sure that they reproducibly give a valid answer.
In Chapter XXX we will talk in much more detail about how to validate computational analyses using simulated data.</p>
</section>
<section id="guiding-principles-for-this-book">
<h2>Guiding principles for this book<a class="headerlink" href="#guiding-principles-for-this-book" title="Permalink to this heading">#</a></h2>
<p>The material that we will present in this book reflects a set of guiding principles:</p>
<ul class="simple">
<li><p>Scientific research increasingly relies upon code written by researchers with the help of AI agents.  Improving the quality of research code is a direct way to enhance the reliability of scientific research.</p></li>
<li><p>Scientists bear the ultimate responsibility for ensuring that their code provides answers that are both reliable and valid.  Fortunately, there are many software development tools that can help in this endeavor.</p></li>
<li><p>Scientists have a responsibility to make their work as open and reproducible as possible.  Open source software and platforms for open sharing of research objects including data and code are essential to making this happen.</p></li>
</ul>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./."
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="frontmatter.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">Better Code, Better Science</p>
      </div>
    </a>
    <a class="right-next"
       href="essential_tools_and_techniques.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Essential tools and techniques</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#why-poor-software-engineering-is-a-threat-to-science">Why poor software engineering is a threat to science</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#is-software-development-still-important-in-the-age-of-ai">Is software development still important in the age of AI?</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#why-better-code-can-mean-better-science">Why better code can mean better science</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#what-does-reproducibility-mean">What does “reproducibility” mean?</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#a-reproducibility-crisis">A reproducibility “crisis”</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#open-science-and-reproducibility">Open science and reproducibility</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#bug-hacking">Bug-hacking</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#how-not-to-fool-ourselves">How not to fool ourselves</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#beyond-reproducibility-getting-valid-answers">Beyond reproducibility: Getting valid answers</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#guiding-principles-for-this-book">Guiding principles for this book</a></li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Russell Poldrack et al.
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      © Copyright 2023.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b"></script>
<script src="_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>