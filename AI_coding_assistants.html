

<!DOCTYPE html>


<html lang="en" data-content_root="" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

    <title>Coding with AI &#8212; Better Code, Better Science</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="_static/styles/theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="_static/styles/bootstrap.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="_static/styles/pydata-sphinx-theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />

  
  <link href="_static/vendor/fontawesome/6.5.2/css/all.min.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.5.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.5.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.5.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="_static/pygments.css" />
    <link rel="stylesheet" type="text/css" href="_static/styles/sphinx-book-theme.css" />
    <link rel="stylesheet" type="text/css" href="_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-design.5ea377869091fd0449014c60fc090103.min.css" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b" />
<link rel="preload" as="script" href="_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b" />
  <script src="_static/vendor/fontawesome/6.5.2/js/all.min.js?digest=dfe6caa3a7d634c4db9b"></script>

    <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script>
    <script src="_static/jquery.js"></script>
    <script src="_static/underscore.js"></script>
    <script src="_static/_sphinx_javascript_frameworks_compat.js"></script>
    <script src="_static/doctools.js"></script>
    <script src="_static/sphinx_highlight.js"></script>
    <script src="_static/clipboard.min.js"></script>
    <script src="_static/copybutton.js"></script>
    <script src="_static/scripts/sphinx-book-theme.js"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="_static/togglebutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="_static/design-tabs.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script async="async" src="_static/sphinx-thebe.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'AI_coding_assistants';</script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Bibliography" href="bibliography.html" />
    <link rel="prev" title="Software testing" href="testing.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-primary-sidebar-checkbox"/>
  <label class="overlay overlay-primary" for="pst-primary-sidebar-checkbox"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-secondary-sidebar-checkbox"/>
  <label class="overlay overlay-secondary" for="pst-secondary-sidebar-checkbox"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  
    
  

<a class="navbar-brand logo" href="frontmatter.html">
  
  
  
  
  
  
    <p class="title logo__title">Better Code, Better Science</p>
  
</a></div>
        <div class="sidebar-primary-item">

 <script>
 document.write(`
   <button class="btn search-button-field search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
    <span class="search-button__default-text">Search</span>
    <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
   </button>
 `);
 </script></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="frontmatter.html">
                    Better Code, Better Science
                </a>
            </li>
        </ul>
        <ul class="current nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="introduction.html">Introduction</a></li>
<li class="toctree-l1"><a class="reference internal" href="essential_tools_and_techniques.html">Essential tools and techniques</a></li>
<li class="toctree-l1"><a class="reference internal" href="software_engineering.html">Principles of software engineering</a></li>
<li class="toctree-l1"><a class="reference internal" href="testing.html">Software testing</a></li>
<li class="toctree-l1 current active"><a class="current reference internal" href="#">Coding with AI</a></li>
<li class="toctree-l1"><a class="reference internal" href="bibliography.html">Bibliography</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><button class="sidebar-toggle primary-toggle btn btn-sm" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</button></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-source-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Source repositories">
    <i class="fab fa-github"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://github.com/poldrack/BetterCodeBetterScience" target="_blank"
   class="btn btn-sm btn-source-repository-button dropdown-item"
   title="Source repository"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="btn__text-container">Repository</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/poldrack/BetterCodeBetterScience/issues/new?title=Issue%20on%20page%20%2FAI_coding_assistants.html&body=Your%20issue%20content%20here." target="_blank"
   class="btn btn-sm btn-source-issues-button dropdown-item"
   title="Open an issue"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="btn__text-container">Open issue</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="_sources/AI_coding_assistants.md" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.md</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light"></i>
    <i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark"></i>
    <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"></i>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm pst-navbar-icon search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<button class="sidebar-toggle secondary-toggle btn btn-sm" title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</button>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Coding with AI</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#using-ai-coding-assistants">Using AI coding assistants</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#cautions-regarding-large-language-models">Cautions regarding large language models</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#ai-coding-approaches">AI coding approaches</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#chatbots">Chatbots</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#ide-assistants">IDE assistants</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#programmatic-access-via-api">Programmatic access via API</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#customizing-api-output">Customizing API output</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#agentic-coding-tools">Agentic coding tools</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#using-claude-code-an-example">Using Claude Code: An example</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#initializing-the-project">Initializing the project</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#initializing-the-claude-code-project">Initializing the Claude Code project</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#guiding-claude-through-the-development-process">Guiding Claude through the development process</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#test-development">Test development</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#core-implementation">Core implementation</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#performance-testing">Performance testing</a></li>
</ul>
</li>
</ul>
</li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="coding-with-ai">
<h1>Coding with AI<a class="headerlink" href="#coding-with-ai" title="Permalink to this heading">#</a></h1>
<section id="using-ai-coding-assistants">
<h2>Using AI coding assistants<a class="headerlink" href="#using-ai-coding-assistants" title="Permalink to this heading">#</a></h2>
<p>The advent of AI coding assistants based on large language models (LLMs) has fundamentally changed the landscape of computer programming.  Until Github released its Copilot AI assistant in 2021, most coders leaned heavily on internet searches, and at some point there was a meme that computer programming would be officially renamed “Googling Stack Overflow” (a popular question-and-answer site for programming questions).  With integrated AI coding assistants, the language model (which has ingested large amounts of text including Stack Overflow) can synthesize answers much more quickly than one could find them by searching online.</p>
<p>In the previous chapter we described the concepts behind the LLMs that coding assistants are based upon.  Early coding assistants (like OpenAI’s Codex) used models that had been trained specifically on large amounts of source code, but the developers of language models subsequently realized that training their models on code seemed to actually help them do a better job on non-coding tasks.  Subsequently, frontier models such as GPT-4o and Claude Sonnet 4 are now trained on large amounts of code and non-code text and can perform well on both coding and non-coding tasks.</p>
<p>Let’s see how code generation works by presenting ChatGPT (using GPT-4o) with the following prompt: “generate a python function to compute a multiple linear regression solution using linear algebra.”, and examining the resulting code:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">multiple_linear_regression</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Solves a multiple linear regression problem using the normal equation.</span>
<span class="sd">    </span>
<span class="sd">    Parameters:</span>
<span class="sd">    X (numpy array): A 2D numpy array where each row is a sample and each column is a feature.</span>
<span class="sd">    y (numpy array): A 1D numpy array representing the target variable for each sample.</span>
<span class="sd">    </span>
<span class="sd">    Returns:</span>
<span class="sd">    w (numpy array): The coefficients of the linear regression model.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># Add a column of ones to X for the intercept term</span>
    <span class="n">X_b</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">c_</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="mi">1</span><span class="p">)),</span> <span class="n">X</span><span class="p">]</span>
    
    <span class="c1"># Compute the coefficients using the normal equation</span>
    <span class="n">w</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">inv</span><span class="p">(</span><span class="n">X_b</span><span class="o">.</span><span class="n">T</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">X_b</span><span class="p">))</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">X_b</span><span class="o">.</span><span class="n">T</span><span class="p">)</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
    
    <span class="k">return</span> <span class="n">w</span>
</pre></div>
</div>
<p>Now let’s start another new chat window, and issue exactly the same prompt again. Here is the resulting code:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">multiple_linear_regression</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes the coefficients for a multiple linear regression using the normal equation.</span>
<span class="sd">    </span>
<span class="sd">    Parameters:</span>
<span class="sd">    X : numpy.ndarray</span>
<span class="sd">        The input feature matrix (each row is a data point, and each column is a feature).</span>
<span class="sd">    y : numpy.ndarray</span>
<span class="sd">        The target output vector.</span>

<span class="sd">    Returns:</span>
<span class="sd">    theta : numpy.ndarray</span>
<span class="sd">        The computed coefficients (including the intercept if X includes a column of ones).</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># Compute the normal equation: theta = (X^T X)^(-1) X^T y</span>
    <span class="n">X_transpose</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
    <span class="n">theta</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">inv</span><span class="p">(</span><span class="n">X_transpose</span> <span class="o">@</span> <span class="n">X</span><span class="p">)</span> <span class="o">@</span> <span class="n">X_transpose</span> <span class="o">@</span> <span class="n">y</span>
    
    <span class="k">return</span> <span class="n">theta</span>
</pre></div>
</div>
<p>This is very similar to the previous code, but you can see differences in the function, such as the presence of a docstring and use of a different variable name. There is one functionally important difference: The second function does not add an intercept (as the first one did), and thus would give incorrect results if there was not already an intercept in the model. This highlights the fact that there is a degree of randomness in the outputs of an LLM (as discussed in the previous chapter), but the results seem fairly close here in terms of the computation itself.</p>
<p>Now let’s see an example of in-context learning. As we discussed previously, this is a form of learning that occurs within a single chat session, through the buildup of information across the session that is used to condition the output at each point.  We start a new chat window, in which we first give the following prompts:  “why are type hints important when creating a python function?”, followed by “Please give an example of how to use type hints effectively”.  We then provide the same prompt as above for a linear algebra function, with the following result:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">linear_regression_normal_eq</span><span class="p">(</span><span class="n">X</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span> <span class="n">y</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Compute the solution to multiple linear regression using the normal equation.</span>

<span class="sd">    Parameters:</span>
<span class="sd">    - X (np.ndarray): The matrix of input features with shape (m, n), where m is the number of examples, and n is the number of features.</span>
<span class="sd">    - y (np.ndarray): The vector of target values with shape (m,).</span>

<span class="sd">    Returns:</span>
<span class="sd">    - np.ndarray: The vector of regression coefficients (theta) with shape (n,).</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># Add a column of ones to X for the intercept term (bias)</span>
    <span class="n">X_bias</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">c_</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="mi">1</span><span class="p">)),</span> <span class="n">X</span><span class="p">]</span>
    
    <span class="c1"># Normal equation: theta = (X^T X)^{-1} X^T y</span>
    <span class="n">theta</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">inv</span><span class="p">(</span><span class="n">X_bias</span><span class="o">.</span><span class="n">T</span> <span class="o">@</span> <span class="n">X_bias</span><span class="p">)</span> <span class="o">@</span> <span class="n">X_bias</span><span class="o">.</span><span class="n">T</span> <span class="o">@</span> <span class="n">y</span>
    
    <span class="k">return</span> <span class="n">theta</span>
</pre></div>
</div>
<p>Unlike the previous examples, the code now includes type hints.  It’s always a bad idea to generalize from a single result, so we ran these prompts through ChatGPT 10 times each (using the Openai API to generate them programmatically; see the <span class="xref myst">notebook</span>).  Here are the function signatures generated for each of the 10 runs without mentioning type hints:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">Run</span> <span class="mi">1</span><span class="p">:</span>  <span class="k">def</span> <span class="nf">multiple_linear_regression</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
<span class="n">Run</span> <span class="mi">2</span><span class="p">:</span>  <span class="k">def</span> <span class="nf">multiple_linear_regression</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">):</span>
<span class="n">Run</span> <span class="mi">3</span><span class="p">:</span>  <span class="k">def</span> <span class="nf">multiple_linear_regression</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
<span class="n">Run</span> <span class="mi">4</span><span class="p">:</span>  <span class="k">def</span> <span class="nf">multiple_linear_regression</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
<span class="n">Run</span> <span class="mi">5</span><span class="p">:</span>  <span class="k">def</span> <span class="nf">multiple_linear_regression</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
<span class="n">Run</span> <span class="mi">6</span><span class="p">:</span>  <span class="k">def</span> <span class="nf">multiple_linear_regression</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">):</span>
<span class="n">Run</span> <span class="mi">7</span><span class="p">:</span>  <span class="k">def</span> <span class="nf">multi_lin_reg</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
<span class="n">Run</span> <span class="mi">8</span><span class="p">:</span>  <span class="k">def</span> <span class="nf">multiple_linear_regression</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">):</span>
<span class="n">Run</span> <span class="mi">9</span><span class="p">:</span>  <span class="k">def</span> <span class="nf">multiple_linear_regression</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">):</span>
<span class="n">Run</span> <span class="mi">10</span><span class="p">:</span>  <span class="k">def</span> <span class="nf">multiple_linear_regression</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
</pre></div>
</div>
<p>The results here are very consistent, with all but one having exactly the same signature.  Here are the function signatures for each of the runs where the prompt to generate code was preceded by the question “why are type hints important when creating a python function?”:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">Run</span> <span class="mi">1</span><span class="p">:</span>  <span class="k">def</span> <span class="nf">multiple_linear_regression</span><span class="p">(</span><span class="n">X</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span> <span class="n">y</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">:</span>
<span class="n">Run</span> <span class="mi">2</span><span class="p">:</span>  <span class="k">def</span> <span class="nf">multiple_linear_regression</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">):</span>
<span class="n">Run</span> <span class="mi">3</span><span class="p">:</span>  <span class="k">def</span> <span class="nf">compute_average</span><span class="p">(</span><span class="n">numbers</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="nb">float</span><span class="p">:</span>
<span class="n">Run</span> <span class="mi">4</span><span class="p">:</span>  <span class="k">def</span> <span class="nf">compute_multiple_linear_regression</span><span class="p">(</span><span class="n">X</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span> <span class="n">y</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">:</span>
<span class="n">Run</span> <span class="mi">5</span><span class="p">:</span>  <span class="k">def</span> <span class="nf">compute_multiple_linear_regression</span><span class="p">(</span><span class="n">x</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span> <span class="n">y</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">:</span>
<span class="n">Run</span> <span class="mi">6</span><span class="p">:</span>  <span class="k">def</span> <span class="nf">compute_multiple_linear_regression</span><span class="p">(</span><span class="n">x_data</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">float</span><span class="p">],</span> <span class="n">y_data</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">float</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="nb">float</span><span class="p">]:</span>
<span class="n">Run</span> <span class="mi">7</span><span class="p">:</span>  <span class="k">def</span> <span class="nf">compute_linear_regression</span><span class="p">(</span><span class="n">X</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span> <span class="n">Y</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">):</span>
<span class="n">Run</span> <span class="mi">8</span><span class="p">:</span>  <span class="k">def</span> <span class="nf">mult_regression</span><span class="p">(</span><span class="n">X</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">,</span> <span class="n">y</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">:</span>
<span class="n">Run</span> <span class="mi">9</span><span class="p">:</span>  <span class="k">def</span> <span class="nf">compute_multiple_linear_regression</span><span class="p">(</span><span class="n">X</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">,</span> <span class="n">Y</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">)</span><span class="o">-&gt;</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">:</span>
<span class="n">Run</span> <span class="mi">10</span><span class="p">:</span>  <span class="k">def</span> <span class="nf">multilinear_regression</span><span class="p">(</span><span class="n">X</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span> <span class="n">Y</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">:</span>
</pre></div>
</div>
<p>Note a couple of interesting things here.  First, 9 out of the 10 signatures here include type hints, showing that introducing the idea of type hints into the context changed the result even using the same code generation prompt. Second, notice that we didn’t explicitly tell it to use type hints in our prompt; the simple mention of why they are a good thing in a previous prompt was enough to cause the model to use them.  Third, notice that the function signatures differ much more from one to another in terms of the names of the functions and variables. Fourth, notice that on Run 3 it seems to have generated incorrect code, which we can confirm by looking at the full function that was generated on that run:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">compute_average</span><span class="p">(</span><span class="n">numbers</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="nb">float</span><span class="p">:</span>
    <span class="k">return</span> <span class="nb">sum</span><span class="p">(</span><span class="n">numbers</span><span class="p">)</span> <span class="o">/</span> <span class="nb">len</span><span class="p">(</span><span class="n">numbers</span><span class="p">)</span>
</pre></div>
</div>
<p>In this case the LLM simply misunderstood the problem that was being solved, highlighting that one can’t simply take the results from LLMs at face value without checking them.  This misunderstanding may have occurred if the model had earlier generated a simple example in response to the type hints prompt, and then failed to update to the regression prompt.  This kind of perseverative error is not uncommon.</p>
</section>
<section id="cautions-regarding-large-language-models">
<h2>Cautions regarding large language models<a class="headerlink" href="#cautions-regarding-large-language-models" title="Permalink to this heading">#</a></h2>
<p>LLMs remain controversial for several reasons.  First, they are known to perpetuate the biases that exist in human language. A large body of research has shown that these models tend to demonstrate bias towards gender and racial minorities, and in general to perpetuate stereotypes that are present in human language.  The models have in some cases been fine-tuned to avoid this, but the biases nonetheless still persist.  Second, there are ongoing arguments regarding whether some models have misused copyrighted materials, or violated the license terms of open-source software.</p>
<p>Most importantly, there are continuing concerns regarding the tendency of LLMs to generate inaccurate material, which has come to be known as “hallucination”.  One of us (RP) has been continually amused at the inability of ChatGPT to distinguish between himself and his father, Wayne Poldrack, a retired accountant and not a neuroscientist.  Given the prompt “Who is Wayne Poldrack”, GPT-4o responds:</p>
<blockquote>
<div><p>Wayne Poldrack is a prominent neuroscientist and psychologist known for his work in cognitive neuroscience, neuroinformatics, and open science. He has made significant contributions to understanding brain function, particularly in areas like decision-making, learning, and memory, using neuroimaging techniques such as functional MRI (fMRI). Poldrack is also an advocate for open science and reproducibility in research, promoting the sharing of data and methods to improve scientific transparency.</p>
</div></blockquote>
<blockquote>
<div><p>One of his most influential contributions is the development of the OpenNeuro platform, which provides free and open access to brain imaging data, allowing researchers worldwide to share, analyze, and collaborate on neuroimaging studies. Poldrack has also been involved in efforts to improve data-sharing standards in the field and has published extensively on topics related to brain function and the methodology of neuroscience research.</p>
</div></blockquote>
<blockquote>
<div><p>As of recent years, he has been a professor at Stanford University, where he continues to lead research initiatives in cognitive neuroscience, and neuroimaging, and the use of data science techniques in understanding brain processes.</p>
</div></blockquote>
<p>When further prompted “Who is Russell Poldrack?”, it continues:</p>
<blockquote>
<div><p>It seems there might be some confusion here, as Wayne Poldrack is the well-known neuroscientist, and there is no widely recognized individual named “Russell Poldrack” in the fields of neuroscience, psychology, or related disciplines. If you’re referring to someone connected to Wayne Poldrack or seeking more specific information, feel free to clarify, and I can look into it further.</p>
</div></blockquote>
<p>On other occasions, ChatGPT has instead stated that Russell Poldrack and Wayne Poldrack are the same person, and even the latest Google Gemini 2.5 Pro provides a detailed description of a Wayne Poldrack who does not seem to exist. To restate the conclusion from the GPT-4 Technical Report stated in the Introduction: “Care should be taken when using the outputs of GPT-4, particularly in contexts where reliability is important.”</p>
<p>Fortunately, coding seems to be a best case scenario for the use of LLMs, since we can relatively easily write tests that can verify that the solutions generated by the system are correct.  This is the reason for our heavy focus on testing and test-driven development in this book, as you saw earlier in the book.</p>
</section>
<section id="ai-coding-approaches">
<h2>AI coding approaches<a class="headerlink" href="#ai-coding-approaches" title="Permalink to this heading">#</a></h2>
<p>As of the time of writing (Summer 2025) there are four primary approaches that one can use to take advantage of LLMs for coding: chatbots, integrated IDE assistants, API submission, and agentic coding tools.</p>
<section id="chatbots">
<h3>Chatbots<a class="headerlink" href="#chatbots" title="Permalink to this heading">#</a></h3>
<p>The first approach is the <em>chatbot</em> model, in which one uses an interactive chatbot interface (such as ChatGPT) to prompt the system.  This is the approach initially used for the coding prompts presented above, and can sometimes work well for generating code <em>de novo</em>, but I generally advise against it. The main reason for this is that it’s not a particularly efficient way to interact with the LLM; given that one will almost certainly be using a code editor/IDE, the code from the chatbot will need to be copied there anyway. As we will note below, use of LLMs integrated into the IDE are also more effective because they allow fine-grained control of which code files are placed into the context of the model.</p>
<p>Where I do often find chatbots useful is in reviewing code; I’ll discuss this later when we dive more deeply into code review.</p>
</section>
<section id="ide-assistants">
<h3>IDE assistants<a class="headerlink" href="#ide-assistants" title="Permalink to this heading">#</a></h3>
<p>A second and probably most common model is the <em>IDE</em> model, where one uses a coding assistant that is integrated into one’s IDE, such as Copilot for VSCode (my preferred choice).  This provides inline code completion suggestions as you type, as well as providing a chat window that allows the inclusion of selected code files into the model’s context.  The IDEs also now include access to <em>coding agents</em>, which I will describe in more detail below.</p>
<p>One way to prompt the system within an IDE is by writing code comments, which then trigger code generation.  The following is the output generated by Copilot within VSCode when given the prompt in the comment at the top:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># generate a python function to compute a multiple linear regression solution using linear algebra</span>

<span class="k">def</span> <span class="nf">multiple_linear_regression</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Compute the multiple linear regression solution using linear algebra.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    X : numpy.ndarray</span>
<span class="sd">        The input data matrix with shape (n_samples, n_features).</span>
<span class="sd">    y : numpy.ndarray</span>
<span class="sd">        The target data vector with shape (n_samples,).</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    numpy.ndarray</span>
<span class="sd">        The regression coefficients with shape (n_features,).</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># compute the regression coefficients</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">inv</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">T</span> <span class="o">@</span> <span class="n">X</span><span class="p">)</span> <span class="o">@</span> <span class="n">X</span><span class="o">.</span><span class="n">T</span> <span class="o">@</span> <span class="n">y</span>
    
</pre></div>
</div>
<p>Most IDEs now also present a chat window that provides access to LLMs that is integrated with the editor, providing the best of both worlds.  Importantly, the chatbot has the ability to load currently open code files into its context window, which allows it to give answers that are more directly relevant to the code.  For example, let’s say that we have a Pandas dataframe that was generated from a data file, and we wish to select a particular set of rows and then sort by one of the columns.  Starting with the following code:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>

<span class="n">astro_data</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s1">&#39;data/astro.csv&#39;</span><span class="p">)</span>
<span class="n">astro_data</span><span class="o">.</span><span class="n">columns</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;planet&#39;</span><span class="p">,</span> <span class="s1">&#39;distance&#39;</span><span class="p">,</span> <span class="s1">&#39;mass&#39;</span><span class="p">,</span> <span class="s1">&#39;diameter&#39;</span><span class="p">,</span> <span class="s1">&#39;moons&#39;</span><span class="p">]</span>
</pre></div>
</div>
<p>We then open the Copilot chat window within VSCode while this code is in the active window, and give it the following prompt: “select planets with only one moon and then sort by distance”, resulting in the addition of several lines:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>

<span class="n">astro_data</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s1">&#39;data/astro.csv&#39;</span><span class="p">)</span>
<span class="n">astro_data</span><span class="o">.</span><span class="n">columns</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;planet&#39;</span><span class="p">,</span> <span class="s1">&#39;distance&#39;</span><span class="p">,</span> <span class="s1">&#39;mass&#39;</span><span class="p">,</span> <span class="s1">&#39;diameter&#39;</span><span class="p">,</span> <span class="s1">&#39;moons&#39;</span><span class="p">]</span>

<span class="c1"># Filter planets with only one moon</span>
<span class="n">one_moon_planets</span> <span class="o">=</span> <span class="n">astro_data</span><span class="p">[</span><span class="n">astro_data</span><span class="p">[</span><span class="s1">&#39;moons&#39;</span><span class="p">]</span> <span class="o">==</span> <span class="mi">1</span><span class="p">]</span>

<span class="c1"># Sort by distance</span>
<span class="n">sorted_planets</span> <span class="o">=</span> <span class="n">one_moon_planets</span><span class="o">.</span><span class="n">sort_values</span><span class="p">(</span><span class="n">by</span><span class="o">=</span><span class="s1">&#39;distance&#39;</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="n">sorted_planets</span><span class="p">)</span>
</pre></div>
</div>
<p>Because the chat window has access to the code file, it was able to generate code that uses the same variable names as those in the existing code, saving time and prevent potential errors in renaming of variables.</p>
<p>When working with an existing codebase, the autocompletion feature of AI assistants provides yet another way that one can leverage their power seamlessly within the IDE.  In my experience, these tools are particularly good at autocompleting code for common coding problems where the code to be written is obvious but will take a bit of time for the coder to complete accurately.  In this way, these tools can remove some of the drudgery of coding, allowing the programmer to focus on more thoughtful aspects of coding.  They do of course make mistakes on occasion, so it’s always important to closely examine the autocompleted code and apply the relevant tests.  Personally I have found myself using autocompletion less and less often, as the chat tools built into the IDE have become increasingly powerful. I also find them rather visually cluttery and distracting when I am coding.</p>
</section>
<section id="programmatic-access-via-api">
<h3>Programmatic access via API<a class="headerlink" href="#programmatic-access-via-api" title="Permalink to this heading">#</a></h3>
<p>Whenever one needs to submit multiple prompts to a language model, it’s worth considering the use of programmatic access via API.  As an example, Jamie Cummins wrote in a <a class="reference external" href="https://bsky.app/profile/did:plc:iukzqhazgtp4mp2lfbitdtuz/post/3lsbyzwpq6c2y">Bluesky post</a> about a <a class="reference external" href="https://doi.org/10.1073/pnas.2501823122">published study</a> that seemingly performed about 900 experimental chats manually via ChatGPT, taking 4 people more than a week to complete.  Cummins pointed out in the thread that “if the authors had used the API, they could have run this study in about 4 hours”.  Similarly, in our first experiments with GPT-4 coding back in 2023, I initially used the ChatGPT interface, simply because I didn’t yet have access to the GPT-4 API, which was very scarce at the time.  Running the first set of 32 problems by hand took several hours, and there was no way that I was going to do the next set of experiments by hand, so I found someone who had access to the API, and we ran the remainder of the experiments using the API.  In addition to the time and labor of running things by hand, it is also a recipe for human error; automating as much as possible can help remove the chances of human errors.</p>
<p>You might be asking at this point, “What’s an API”?  The acronym stands for “Application Programming Interface”, which is a method by which one can programmatically send commands to and receive responses from a computer system, which could be local or remote<a class="footnote-reference brackets" href="#id2" id="id1">1</a>. To understand this better, let’s see how to send a chat command and receive a response from the Claude language model.  The full outline is in <a class="reference external" href="https://github.com/poldrack/BetterCodeBetterScience/blob/main/src/BetterCodeBetterScience/language_model_api_prompting.ipynb">the notebook</a>.  Coding agents are very good at generating code to perform API calls, so I used Claude Sonnet 4 to generate the example code in the notebook:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">anthropic</span>
<span class="kn">import</span> <span class="nn">os</span>

<span class="c1"># Set up the API client</span>
<span class="c1"># Requires setting your API key as an environment variable: ANTHROPIC</span>
<span class="n">client</span> <span class="o">=</span> <span class="n">anthropic</span><span class="o">.</span><span class="n">Anthropic</span><span class="p">(</span>
    <span class="n">api_key</span><span class="o">=</span><span class="n">os</span><span class="o">.</span><span class="n">getenv</span><span class="p">(</span><span class="s2">&quot;ANTHROPIC&quot;</span><span class="p">)</span>
<span class="p">)</span>
</pre></div>
</div>
<p>This code first imports the necessary libraries, including the <code class="docutils literal notranslate"><span class="pre">anthropic</span></code> module that provides functions to streamline interactions with the model.  It then sets up a <code class="docutils literal notranslate"><span class="pre">client</span></code> object, which has methods to allow prompting and receiving output from the model. Note that we have to specify an “API key” to use the API; this is a security token that tells the model which account should be charged for usage of the model.  Depending on the kind of account that you have, you may need to pay for API access on a per-token basis, or you may have a specific allocation of tokens to be used in a particular amount of time; check with your preferred model provider for more information on this.</p>
<p>It might be tempting to avoid the extra hassle of specifying the API key as an environment variable by simply pasting it directly into the code, but you should <strong>never</strong> do this.  Even if you think the code may be private, it’s all too easy for it to become public in the future, at which point someone could easily steal your key and rack up lots of charges.  See the section in Chapter 3 on <a class="reference external" href="https://poldrack.github.io/BetterCodeBetterScience/software_engineering.html#coding-portably">Coding Portably</a> for more on the ways to solve this problem.</p>
<p>Now that we have the client specified, we can submit a prompt and examine the result:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">model</span> <span class="o">=</span> <span class="s2">&quot;claude-3-5-haiku-latest&quot;</span>
<span class="n">max_tokens</span> <span class="o">=</span> <span class="mi">1000</span> 
<span class="n">prompt</span> <span class="o">=</span> <span class="s2">&quot;What is the capital of France?&quot;</span>

<span class="n">message</span> <span class="o">=</span> <span class="n">client</span><span class="o">.</span><span class="n">messages</span><span class="o">.</span><span class="n">create</span><span class="p">(</span>
    <span class="n">model</span><span class="o">=</span><span class="n">model</span><span class="p">,</span>
    <span class="n">max_tokens</span><span class="o">=</span><span class="n">max_tokens</span><span class="p">,</span>
    <span class="n">messages</span><span class="o">=</span><span class="p">[</span>
        <span class="p">{</span><span class="s2">&quot;role&quot;</span><span class="p">:</span> <span class="s2">&quot;user&quot;</span><span class="p">,</span> <span class="s2">&quot;content&quot;</span><span class="p">:</span> <span class="n">prompt</span><span class="p">}</span>
    <span class="p">]</span>
<span class="p">)</span>
</pre></div>
</div>
<p>Examining the content of the <code class="docutils literal notranslate"><span class="pre">message</span></code> object, we see that it contains information about the API call and resource usage as well as a response:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">Message</span><span class="p">(</span>
    <span class="nb">id</span><span class="o">=</span><span class="s1">&#39;msg_016H1QzGNPKdsLmXRZog78kU&#39;</span><span class="p">,</span>
    <span class="n">content</span><span class="o">=</span><span class="p">[</span>
        <span class="n">TextBlock</span><span class="p">(</span>
            <span class="n">citations</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
            <span class="n">text</span><span class="o">=</span><span class="s1">&#39;The capital of France is Paris.&#39;</span><span class="p">,</span>
            <span class="nb">type</span><span class="o">=</span><span class="s1">&#39;text&#39;</span>
        <span class="p">)</span>
    <span class="p">],</span>
    <span class="n">model</span><span class="o">=</span><span class="s1">&#39;claude-3-5-haiku-20241022&#39;</span><span class="p">,</span>
    <span class="n">role</span><span class="o">=</span><span class="s1">&#39;assistant&#39;</span><span class="p">,</span>
    <span class="n">stop_reason</span><span class="o">=</span><span class="s1">&#39;end_turn&#39;</span><span class="p">,</span>
    <span class="n">stop_sequence</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="nb">type</span><span class="o">=</span><span class="s1">&#39;message&#39;</span><span class="p">,</span>
    <span class="n">usage</span><span class="o">=</span><span class="n">Usage</span><span class="p">(</span>
        <span class="n">cache_creation_input_tokens</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
        <span class="n">cache_read_input_tokens</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
        <span class="n">input_tokens</span><span class="o">=</span><span class="mi">14</span><span class="p">,</span>
        <span class="n">output_tokens</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span>
        <span class="n">server_tool_use</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">service_tier</span><span class="o">=</span><span class="s1">&#39;standard&#39;</span>
    <span class="p">)</span>
<span class="p">)</span>
</pre></div>
</div>
<p>The key part of the response is in the <code class="docutils literal notranslate"><span class="pre">content</span></code> field, which contains the answer:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="n">message</span><span class="o">.</span><span class="n">content</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">text</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="s2">&quot;The capital of France is Paris.&quot;</span>
</pre></div>
</div>
<section id="customizing-api-output">
<h4>Customizing API output<a class="headerlink" href="#customizing-api-output" title="Permalink to this heading">#</a></h4>
<p>By default, the API will simply return text, just as a chatbot would. However, it’s possible to instruct the model to return results in a format that is much easier to programmatically process.  The preferred format for this is generally JSON (JavaScript Object Notation), which has very similar structure to a Python dictionary. Let’s see how we could get the previous example to return a JSON object containing just the name of the capital.  Here we will use a function called <code class="docutils literal notranslate"><span class="pre">send_prompt_to_claude()</span></code> that wraps the call to the model object and returns the text from the result:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">BetterCodeBetterScience.llm_utils</span> <span class="kn">import</span> <span class="n">send_prompt_to_claude</span>

<span class="n">json_prompt</span> <span class="o">=</span> <span class="s2">&quot;&quot;&quot;</span>
<span class="s2">What is the capital of France? </span>

<span class="s2">Please return your response as a JSON object with the following structure:</span>
<span class="s2">{</span>
<span class="s2">    &quot;capital&quot;: &quot;city_name&quot;,</span>
<span class="s2">    &quot;country&quot;: &quot;country_name&quot;</span>
<span class="s2">}</span>
<span class="s2">&quot;&quot;&quot;</span>

<span class="n">result</span> <span class="o">=</span> <span class="n">send_prompt_to_claude</span><span class="p">(</span><span class="n">json_prompt</span><span class="p">,</span> <span class="n">client</span><span class="p">)</span>
<span class="n">result</span>
</pre></div>
</div>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="s1">&#39;{\n    &quot;capital&quot;: &quot;Paris&quot;,\n    &quot;country&quot;: &quot;France&quot;\n}&#39;</span>
</pre></div>
</div>
<p>The result is returned as a JSON object that has been encoded as a string, so we need to convert it from a string to a JSON object:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">json</span>

<span class="n">result_dict</span> <span class="o">=</span> <span class="n">json</span><span class="o">.</span><span class="n">loads</span><span class="p">(</span><span class="n">result</span><span class="p">)</span>
<span class="n">result_dict</span>
</pre></div>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="p">{</span><span class="s1">&#39;capital&#39;</span><span class="p">:</span> <span class="s1">&#39;Paris&#39;</span><span class="p">,</span> <span class="s1">&#39;country&#39;</span><span class="p">:</span> <span class="s1">&#39;France&#39;</span><span class="p">}</span>
</pre></div>
</div>
<p>The output is now in a standard Python dictionary format.  We can easily use this pattern to expand to multiple calls to the API.  Let’s say that we wanted to get the capitals for ten different countries. There are two ways that we might do this.  First, we might loop through ten API calls with each country individually:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">countries</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;France&quot;</span><span class="p">,</span> <span class="s2">&quot;Germany&quot;</span><span class="p">,</span> <span class="s2">&quot;Spain&quot;</span><span class="p">,</span> <span class="s2">&quot;Italy&quot;</span><span class="p">,</span> <span class="s2">&quot;Portugal&quot;</span><span class="p">,</span> 
             <span class="s2">&quot;Netherlands&quot;</span><span class="p">,</span> <span class="s2">&quot;Belgium&quot;</span><span class="p">,</span> <span class="s2">&quot;Sweden&quot;</span><span class="p">,</span> <span class="s2">&quot;Norway&quot;</span><span class="p">,</span> <span class="s2">&quot;Finland&quot;</span><span class="p">]</span>

<span class="k">for</span> <span class="n">country</span> <span class="ow">in</span> <span class="n">countries</span><span class="p">:</span>
    <span class="n">json_prompt</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;&quot;&quot;</span>
<span class="s2">    What is the capital of </span><span class="si">{</span><span class="n">country</span><span class="si">}</span><span class="s2">? </span>

<span class="s2">    Please return your response as a JSON object with the following structure:</span>
<span class="s2">    </span><span class="se">{{</span>
<span class="s2">        &quot;capital&quot;: &quot;city_name&quot;,</span>
<span class="s2">        &quot;country&quot;: &quot;country_name&quot;</span>
<span class="s2">    </span><span class="se">}}</span>
<span class="s2">    &quot;&quot;&quot;</span>
    <span class="n">result</span> <span class="o">=</span> <span class="n">send_prompt_to_claude</span><span class="p">(</span><span class="n">json_prompt</span><span class="p">,</span> <span class="n">client</span><span class="p">)</span>
    <span class="n">result_dict</span> <span class="o">=</span> <span class="n">json</span><span class="o">.</span><span class="n">loads</span><span class="p">(</span><span class="n">result</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">result_dict</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="p">{</span><span class="s1">&#39;capital&#39;</span><span class="p">:</span> <span class="s1">&#39;Paris&#39;</span><span class="p">,</span> <span class="s1">&#39;country&#39;</span><span class="p">:</span> <span class="s1">&#39;France&#39;</span><span class="p">}</span>
<span class="p">{</span><span class="s1">&#39;capital&#39;</span><span class="p">:</span> <span class="s1">&#39;Berlin&#39;</span><span class="p">,</span> <span class="s1">&#39;country&#39;</span><span class="p">:</span> <span class="s1">&#39;Germany&#39;</span><span class="p">}</span>
<span class="p">{</span><span class="s1">&#39;capital&#39;</span><span class="p">:</span> <span class="s1">&#39;Madrid&#39;</span><span class="p">,</span> <span class="s1">&#39;country&#39;</span><span class="p">:</span> <span class="s1">&#39;Spain&#39;</span><span class="p">}</span>
<span class="p">{</span><span class="s1">&#39;capital&#39;</span><span class="p">:</span> <span class="s1">&#39;Rome&#39;</span><span class="p">,</span> <span class="s1">&#39;country&#39;</span><span class="p">:</span> <span class="s1">&#39;Italy&#39;</span><span class="p">}</span>
<span class="p">{</span><span class="s1">&#39;capital&#39;</span><span class="p">:</span> <span class="s1">&#39;Lisbon&#39;</span><span class="p">,</span> <span class="s1">&#39;country&#39;</span><span class="p">:</span> <span class="s1">&#39;Portugal&#39;</span><span class="p">}</span>
<span class="p">{</span><span class="s1">&#39;capital&#39;</span><span class="p">:</span> <span class="s1">&#39;Amsterdam&#39;</span><span class="p">,</span> <span class="s1">&#39;country&#39;</span><span class="p">:</span> <span class="s1">&#39;Netherlands&#39;</span><span class="p">}</span>
<span class="p">{</span><span class="s1">&#39;capital&#39;</span><span class="p">:</span> <span class="s1">&#39;Brussels&#39;</span><span class="p">,</span> <span class="s1">&#39;country&#39;</span><span class="p">:</span> <span class="s1">&#39;Belgium&#39;</span><span class="p">}</span>
<span class="p">{</span><span class="s1">&#39;capital&#39;</span><span class="p">:</span> <span class="s1">&#39;Stockholm&#39;</span><span class="p">,</span> <span class="s1">&#39;country&#39;</span><span class="p">:</span> <span class="s1">&#39;Sweden&#39;</span><span class="p">}</span>
<span class="p">{</span><span class="s1">&#39;capital&#39;</span><span class="p">:</span> <span class="s1">&#39;Oslo&#39;</span><span class="p">,</span> <span class="s1">&#39;country&#39;</span><span class="p">:</span> <span class="s1">&#39;Norway&#39;</span><span class="p">}</span>
<span class="p">{</span><span class="s1">&#39;capital&#39;</span><span class="p">:</span> <span class="s1">&#39;Helsinki&#39;</span><span class="p">,</span> <span class="s1">&#39;country&#39;</span><span class="p">:</span> <span class="s1">&#39;Finland&#39;</span><span class="p">}</span>
</pre></div>
</div>
<p>Alternatively, we could submit all of the countries together in a single prompt. Here is the first prompt I tried:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">json_prompt_all</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;&quot;&quot;</span>
<span class="s2">Here is a list of countries:</span>
<span class="si">{</span><span class="s1">&#39;, &#39;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">countries</span><span class="p">)</span><span class="si">}</span>

<span class="s2">For each country, please provide the capital city </span>
<span class="s2">in a JSON object with the country name as the key </span>
<span class="s2">and the capital city as the value.  </span>
<span class="s2">&quot;&quot;&quot;</span>
<span class="n">result_all</span><span class="p">,</span> <span class="n">ntokens_prompt</span> <span class="o">=</span> <span class="n">send_prompt_to_claude</span><span class="p">(</span>
    <span class="n">json_prompt_all</span><span class="p">,</span> <span class="n">client</span><span class="p">,</span> <span class="n">return_tokens</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
<p>The output was not exactly what I was looking for, as it included extra text that caused the JSON conversion to fail:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>&#39;Here\&#39;s the JSON object with the countries and their respective capital cities:\n\n{\n    &quot;France&quot;: &quot;Paris&quot;,\n    &quot;Germany&quot;: &quot;Berlin&quot;,\n    &quot;Spain&quot;: &quot;Madrid&quot;,\n 
  &quot;Italy&quot;: &quot;Rome&quot;,\n    &quot;Portugal&quot;: &quot;Lisbon&quot;,\n    &quot;Netherlands&quot;: &quot;Amsterdam&quot;,\n
  &quot;Belgium&quot;: &quot;Brussels&quot;,\n    &quot;Sweden&quot;: &quot;Stockholm&quot;,\n    &quot;Norway&quot;: &quot;Oslo&quot;,\n
  &quot;Finland&quot;: &quot;Helsinki&quot;\n}&#39;
</pre></div>
</div>
<p>This highlights an important aspect of prompting: One must often be much more explicit and detailed than you expect.  As the folks at Anthropic said in their <a class="reference external" href="https://www.anthropic.com/engineering/claude-code-best-practices?curius=2107">guide to best practices for coding using Claude Code</a> (a product discussed further below): “Claude can infer intent, but it can’t read minds. Specificity leads to better alignment with expectations.”  In this case, we change the prompt to include an explicit directive to only return the JSON object:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">json_prompt_all</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;&quot;&quot;</span>
<span class="s2">Here is a list of countries:</span>
<span class="si">{</span><span class="s1">&#39;, &#39;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">countries</span><span class="p">)</span><span class="si">}</span>

<span class="s2">For each country, please provide the capital city in a </span>
<span class="s2">JSON object with the country name as the key and the </span>
<span class="s2">capital city as the value.  </span>

<span class="s2">IMPORTANT: Return only the JSON object without any additional text.</span>
<span class="s2">&quot;&quot;&quot;</span>
<span class="n">result_all</span><span class="p">,</span> <span class="n">ntokens_prompt</span> <span class="o">=</span> <span class="n">send_prompt_to_claude</span><span class="p">(</span>
    <span class="n">json_prompt_all</span><span class="p">,</span> <span class="n">client</span><span class="p">,</span> <span class="n">return_tokens</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="s1">&#39;{</span><span class="se">\n</span><span class="s1">    &quot;France&quot;: &quot;Paris&quot;,</span><span class="se">\n</span><span class="s1">    &quot;Germany&quot;: &quot;Berlin&quot;,</span><span class="se">\n</span><span class="s1">    &quot;Spain&quot;: &quot;Madrid&quot;,</span><span class="se">\n</span>
  <span class="s2">&quot;Italy&quot;</span><span class="p">:</span> <span class="s2">&quot;Rome&quot;</span><span class="p">,</span>\<span class="n">n</span>    <span class="s2">&quot;Portugal&quot;</span><span class="p">:</span> <span class="s2">&quot;Lisbon&quot;</span><span class="p">,</span>\<span class="n">n</span>    <span class="s2">&quot;Netherlands&quot;</span><span class="p">:</span> <span class="s2">&quot;Amsterdam&quot;</span><span class="p">,</span>\<span class="n">n</span>
  <span class="s2">&quot;Belgium&quot;</span><span class="p">:</span> <span class="s2">&quot;Brussels&quot;</span><span class="p">,</span>\<span class="n">n</span>    <span class="s2">&quot;Sweden&quot;</span><span class="p">:</span> <span class="s2">&quot;Stockholm&quot;</span><span class="p">,</span>\<span class="n">n</span>    <span class="s2">&quot;Norway&quot;</span><span class="p">:</span> <span class="s2">&quot;Oslo&quot;</span><span class="p">,</span>\<span class="n">n</span>
  <span class="s2">&quot;Finland&quot;</span><span class="p">:</span> <span class="s2">&quot;Helsinki&quot;</span>\<span class="n">n</span><span class="p">}</span><span class="s1">&#39;</span>
</pre></div>
</div>
<p>Why might we prefer one of these solutions to the other?  One reason has to do with the amount of LLM resources required by each. If you look back at the full output of the client above, you will see that it includes fields called <code class="docutils literal notranslate"><span class="pre">input_tokens</span></code> and <code class="docutils literal notranslate"><span class="pre">output_tokens</span></code> that quantify the amount of information fed into and out of the model.  Because LLM costs are generally based on the number of tokens used, we would like to minimize this.  If we add these up, we see that the looping solution uses a total of 832 tokens, while the single-prompt solution uses only 172 tokens. At this scale this wouldn’t make a difference, but for large analyses this could result in major cost differences for the two analyses.  Note, however, that the difference between these models in part reflects the short nature of the prompt, which means that most of the tokens being passed are what one might consider to be overhead tokens which are required for any prompt (such as the system prompt).  As the length of the user prompt increases, the proportional difference between looping and a single compound prompt will decrease.</p>
<p>It’s also important to note that there is a point at which very long prompts may begin to degrade performance.  In particular, LLM researchers have identified a phenomenon that has come to be called <em>context rot</em>, in which performance of the model is degraded as the amount of information in context grows. <a class="reference external" href="https://research.trychroma.com/context-rot">Analyses of performance as a function of context</a> have shown that model performance can begin to degrade on some benchmarks when the context extends beyond 1000 tokens and can sometimes degrade very badly as the context goes beyond 100,000 tokens.    Later in this chapter we will discuss <em>retrieval-augmented generation</em>, which is a method that can help alleviate the impact of context rot by focusing the context on the most relevant information for the task at hand.</p>
</section>
</section>
<section id="agentic-coding-tools">
<h3>Agentic coding tools<a class="headerlink" href="#agentic-coding-tools" title="Permalink to this heading">#</a></h3>
<p>The fourth approach uses tools that have <em>agentic</em> capabilities, which means that they have larger goals and can call upon other tools to help accomplish those goals.  Rather than simply using a language model to generate code based on a prompt, a coding agent is a language model (usually a <em>thinking</em> model) that can take in information (including direct prompts, files, web searches, and input from other tools), synthesize that information to figure out how to solve a goal, and then execute on that plan.  The landscape of agentic coding tools is developing very rapidly, so anything I say here will likely be outdated very soon, but hopefully the general points will remain relevant for some time.  In this chapter I will use <a class="reference external" href="https://docs.anthropic.com/en/docs/claude-code/overview">Claude Code</a>, which is at the time of writing of one of the most popular and powerful agentic coding tools.  I will only scratch the surface of its capabilities, but this discussion should noentheless should give you a good feel for how these tools can be used.</p>
<p>Claude Code works through the command line interface (CLI), which makes it very different from the tools that are accessed via IDEs or web interfaces:</p>
<p><img alt="Claude Code startup screen" src="_images/claudecode_1.png" /></p>
<p>However, Claude Code can also integrate with IDEs such as VSCode.  Shortcuts in Claude Code begin with a forward slash (“/”); when we type a forward slash, we receive a listing of selected shortcuts:</p>
<p><img alt="Claude Code shortcuts" src="_images/claudecode_2.png" /></p>
</section>
<section id="using-claude-code-an-example">
<h3>Using Claude Code: An example<a class="headerlink" href="#using-claude-code-an-example" title="Permalink to this heading">#</a></h3>
<p>The best way to learn how Claude Code works is to see it in action, so here I will walk through an example of using it to create a new application.  As we work through the example I’ll highlight important best practices for using coding agents.  An important note: If you follow along and perform all of the same commands, you will not necessarily see the same results, given the stochastic nature of large language models.</p>
<p>In this example, we will build an application that computes column-wise correlations between two matrices using an optimized approach based on the Einstein summation notation and leveraging GPU acceleration when possible.  This is a problem that one would often encounter when trying to compute correlations between a large number of recordings. While standard methods for computing correlations are fine for small numbers of variables, these methods can become very slow for large correlation matrices.</p>
<section id="initializing-the-project">
<h4>Initializing the project<a class="headerlink" href="#initializing-the-project" title="Permalink to this heading">#</a></h4>
<p>We will use <code class="docutils literal notranslate"><span class="pre">uv</span></code> to manage dependencies for this project, so we first create the project with <code class="docutils literal notranslate"><span class="pre">uv</span> <span class="pre">init</span> <span class="pre">--package</span> <span class="pre">einsumcorr</span></code> and then create and load the virtual environment (from within the new directory) with <code class="docutils literal notranslate"><span class="pre">uv</span> <span class="pre">venv</span></code> and <code class="docutils literal notranslate"><span class="pre">source</span> <span class="pre">.venv/bin/activate</span></code>.  I also like to remove the default contents of the <code class="docutils literal notranslate"><span class="pre">__init__.py</span></code> file created by <code class="docutils literal notranslate"><span class="pre">uv</span></code> and replace it with an empty file, using:</p>
<p><code class="docutils literal notranslate"><span class="pre">rm</span> <span class="pre">src/einsumcorr/__init__.py;</span> <span class="pre">touch</span> <span class="pre">src/einsumcorr/__init__.py</span></code></p>
<p>We also initialize a new Git repository and add several files:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="o">&gt;</span> <span class="n">git</span> <span class="n">init</span>
<span class="o">&gt;</span> <span class="n">git</span> <span class="n">add</span> <span class="n">pyproject</span><span class="o">.</span><span class="n">toml</span> <span class="n">README</span><span class="o">.</span><span class="n">md</span> <span class="n">src</span><span class="o">/</span><span class="n">einsumcorr</span><span class="o">/</span><span class="fm">__init__</span><span class="o">.</span><span class="n">py</span>
<span class="o">&gt;</span> <span class="n">git</span> <span class="n">commit</span> <span class="o">-</span><span class="n">m</span><span class="s2">&quot;initial add&quot;</span>
</pre></div>
</div>
</section>
<section id="initializing-the-claude-code-project">
<h4>Initializing the Claude Code project<a class="headerlink" href="#initializing-the-claude-code-project" title="Permalink to this heading">#</a></h4>
<p>We can now run <code class="docutils literal notranslate"><span class="pre">claude</span></code> within the project directory and run the <code class="docutils literal notranslate"><span class="pre">/init</span></code> command which will generate a skeletal <code class="docutils literal notranslate"><span class="pre">CLAUDE.md</span></code> file for us. At this point we exit Claude Code and edit the <code class="docutils literal notranslate"><span class="pre">CLAUDE.md</span></code> file to include our specifications and development preferences. You can see the full file <a class="reference external" href="https://github.com/poldrack/einsumcorr/blob/main/CLAUDE.md">here</a>.</p>
</section>
<section id="guiding-claude-through-the-development-process">
<h4>Guiding Claude through the development process<a class="headerlink" href="#guiding-claude-through-the-development-process" title="Permalink to this heading">#</a></h4>
<p>Now that we have the project defined, we can have Claude read the updated specs and present a plan for development:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>User: Look at the updated CLAUDE.md and present me with a development plan.

⏺ I&#39;ll analyze the updated CLAUDE.md file and create a development plan for
   the einsumcorr package.

⏺ Development Plan for einsumcorr

  Based on the CLAUDE.md specifications, here&#39;s the development plan
  following TDD principles:

  Phase 1: Setup &amp; Dependencies

  - Install required packages: opt_einsum, PyTorch, numpy, pytest
  - Configure pyproject.toml with all dependencies

  Phase 2: Test Development (TDD)

  - Create comprehensive test suite covering:
    - Single matrix correlation (self-correlation)
    - Two matrix cross-correlation
    - Edge cases (empty matrices, single columns)
    - GPU acceleration verification
    - Comparison with standard numpy correlations
    - Smoke test for main() function

  Phase 3: Core Implementation

  - Implement optcorr function using einsum notation
  - Handle single vs dual matrix inputs
  - Ensure proper matrix centering and normalization
  - Use opt_einsum with PyTorch backend

  Phase 4: GPU Acceleration

  - Auto-detect CUDA or MPS availability
  - Implement automatic device selection
  - Handle fallback to CPU if GPU unavailable

  Phase 5: CLI &amp; Entry Point

  - Create main() function for command-line usage
  - Set up entry point in pyproject.toml
  - Add basic argument parsing

  Phase 6: Validation

  - Run all tests to ensure functionality
  - Verify results match standard correlation methods
  - Test package installation and CLI entry point

  The key technical challenges will be:
  1. Correctly implementing correlation using einsum notation
  2. Ensuring numerical stability and accuracy
  3. Efficient GPU utilization when available
  4. Maintaining compatibility with standard numpy correlation results
</pre></div>
</div>
<p>This looks like a very reasonable plan, so we will have Claude get started with the first setup step:</p>
<p><code class="docutils literal notranslate"><span class="pre">User: Please</span> <span class="pre">perform</span> <span class="pre">Phase</span> <span class="pre">1:</span> <span class="pre">Setup</span> <span class="pre">&amp;</span> <span class="pre">Dependencies</span></code></p>
<p>After approving several commands, we get the following message:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>⏺ Phase 1 completed successfully. Dependencies installed:
  - PyTorch 2.8.0
  - opt-einsum 3.4.0
  - NumPy 2.3.2
  - pytest 8.4.1 &amp; pytest-cov 6.2.1 (dev dependencies)

  MPS (Metal Performance Shaders) GPU acceleration is available on this
  system.
</pre></div>
</div>
<p>Claude successfully identified the MPS acceleration on my Apple M3 laptop. Before moving to the first coding step, we will commit all of the changes so far, by simply giving the command <code class="docutils literal notranslate"><span class="pre">commit</span></code> and then approving its actions.</p>
</section>
<section id="test-development">
<h4>Test development<a class="headerlink" href="#test-development" title="Permalink to this heading">#</a></h4>
<p>We can now have it proceed with the test development phase. I prefer to perform each step within its own <code class="docutils literal notranslate"><span class="pre">git</span></code> branch, so that it’s easier to revert to a clean state if needed.  We can ask Claude to do this using <code class="docutils literal notranslate"><span class="pre">check</span> <span class="pre">out</span> <span class="pre">a</span> <span class="pre">new</span> <span class="pre">branch</span> <span class="pre">called</span> <span class="pre">dev/phase2</span></code> which creates and checks out a new branch.  We can now ask it to perform the test development phase:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>User: perform Phase 2: Test Development (TDD)

... (lots of interim output deleted)

⏺ Phase 2 completed. Created comprehensive test suite with:

  - test_optcorr.py: 17 tests covering:
    - Shape validation for single and dual matrix inputs
    - Self-correlation properties (diagonal ones, symmetry)
    - Correlation value range [-1, 1]
    - Edge cases (single column, empty matrix, mismatched rows)
    - NaN handling
    - Comparison with numpy.corrcoef
    - GPU acceleration verification
    - Deterministic results
  - test_main.py: 6 tests for CLI functionality:
    - Smoke test for main function
    - Command-line argument handling
    - File I/O operations
    - Error handling

  All 21 tests are currently failing with NotImplementedError as expected
  in TDD approach.
</pre></div>
</div>
</section>
<section id="core-implementation">
<h4>Core implementation<a class="headerlink" href="#core-implementation" title="Permalink to this heading">#</a></h4>
<p>Looking at the tests that were generated by Claude, they all seem to be fairly reasonable and together they seem to cover most of the cases that one would want to test.  Thus, we can move forward with the next phase, which implements the core routines.  We first merge our development branch into main, using the Claude command <code class="docutils literal notranslate"><span class="pre">merge</span> <span class="pre">branch</span> <span class="pre">into</span> <span class="pre">main</span></code> and then ask it to create a new branch for the next phase using <code class="docutils literal notranslate"><span class="pre">check</span> <span class="pre">out</span> <span class="pre">a</span> <span class="pre">new</span> <span class="pre">branch</span> <span class="pre">called</span> <span class="pre">dev/phase3</span></code>.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>User: Proceed with Phase 3: Core Implementation

... (lots of interim output deleted)

⏺ Phase 3 completed successfully!

  Implementation Summary

  ✅ Core optcorr function implemented with:
  - Einstein summation notation using opt_einsum
  - PyTorch backend for efficient computation
  - Automatic GPU detection (CUDA/MPS)
  - Float32/64 handling for device compatibility
  - Full input validation and error handling

  ✅ CLI interface created with:
  - Argument parsing for single/dual matrix input
  - CSV file I/O support
  - Configurable output options
  - Help documentation

  ✅ All 21 tests passing:
  - Shape validation
  - Correlation properties
  - Comparison with numpy.corrcoef
  - GPU acceleration support
  - CLI functionality

  The package now provides efficient columnwise correlation computation
  using Einstein summation with automatic GPU acceleration when available.
</pre></div>
</div>
<p>All of the tests now run!  Examination of the primary function shows that seems like very reasonable code.  It’s a good idea to check and make sure that any changes to the tests don’t seem to be problematic:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="o">&gt;</span> <span class="n">git</span> <span class="n">diff</span> <span class="n">tests</span><span class="o">/</span><span class="n">test_optcorr</span><span class="o">.</span><span class="n">py</span>

 <span class="k">def</span> <span class="nf">test_comparison_with_numpy_large_matrix</span><span class="p">(</span><span class="n">large_matrix</span><span class="p">):</span>
<span class="w">     </span><span class="sd">&quot;&quot;&quot;Test that optcorr matches numpy.corrcoef for larger matrices.&quot;&quot;&quot;</span>
<span class="o">-</span>    <span class="n">compare_with_numpy_corrcoef</span><span class="p">(</span><span class="n">large_matrix</span><span class="p">)</span>
<span class="o">+</span>    <span class="c1"># Use slightly higher tolerance for larger matrices with float32 on MPS</span>
<span class="o">+</span>    <span class="n">compare_with_numpy_corrcoef</span><span class="p">(</span><span class="n">large_matrix</span><span class="p">,</span> <span class="n">rtol</span><span class="o">=</span><span class="mf">5e-4</span><span class="p">)</span>
</pre></div>
</div>
<p>Here it seems that it had to slightly loosen the tolerance for the comparison between our new method and the numpy method, which seems fine.</p>
<p>While the initial plan had originally separated out the addition of GPU support into a separate fourth phase, it seems that Claude included it in Phase 3. We can ask it to make sure:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>User: It seems that you have already completed the remaining phases. 
Please provide an updated plan for any remaining work to be done, 
before we proceed any further

⏺ Updated Plan

  Minor Tasks Remaining:

  1. Fix code warning: Remove unused dtype variable in optcorr.py
  2. Add performance benchmark: Create a simple benchmark comparing einsum
  vs standard correlation
  3. Update README: Add usage examples and documentation
  4. Final cleanup: Commit all changes

  These are minor refinements - the core implementation is complete and
  fully functional. Would you like me to proceed with these cleanup tasks?
</pre></div>
</div>
<p>We can ask it to complete the remaining tasks:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>User: complete remaining tasks

... (lots of interim output deleted)

⏺ All remaining tasks completed successfully!

  Summary of completed work:

  ✅ Fixed code warning - Removed unused dtype variable
  ✅ Added performance benchmarks - Created test_performance.py with timing
   comparisons
  ✅ Updated README - Added comprehensive documentation with usage examples
  ✅ Final commit - All changes committed to dev/phase3 branch

  The einsumcorr package is now fully implemented with:
  - Efficient Einstein summation correlation computation
  - Automatic GPU acceleration (MPS detected on this system)
  - Complete test coverage (25 tests passing)
  - CLI tool with file I/O support
  - Comprehensive documentation
</pre></div>
</div>
<p>Based on this, we can commit and merge this branch to main using <code class="docutils literal notranslate"><span class="pre">commit,</span> <span class="pre">then</span> <span class="pre">merge</span> <span class="pre">into</span> <span class="pre">main</span></code>.</p>
</section>
<section id="performance-testing">
<h4>Performance testing<a class="headerlink" href="#performance-testing" title="Permalink to this heading">#</a></h4>
<p>Even though we didn’t ask for it, Claude added a set of performance tests to compare our new method to <code class="docutils literal notranslate"><span class="pre">numpy.corrcoef()</span></code>.  In its first pass, it seems that our method is very slow compared to <code class="docutils literal notranslate"><span class="pre">numpy</span></code>:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="o">&gt;</span> <span class="n">uv</span> <span class="n">run</span> <span class="n">pytest</span> <span class="n">tests</span><span class="o">/</span><span class="n">test_performance</span><span class="o">.</span><span class="n">py</span> <span class="o">-</span><span class="n">v</span> <span class="o">-</span><span class="n">s</span>
<span class="o">=============================</span> <span class="n">test</span> <span class="n">session</span> <span class="n">starts</span> <span class="o">==============================</span>

<span class="n">tests</span><span class="o">/</span><span class="n">test_performance</span><span class="o">.</span><span class="n">py</span><span class="p">::</span><span class="n">test_performance_comparison</span><span class="p">[</span><span class="n">size0</span><span class="p">]</span>
<span class="n">Matrix</span> <span class="n">size</span> <span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="mi">10</span><span class="p">):</span> <span class="n">optcorr</span><span class="o">=</span><span class="mf">0.0472</span><span class="n">s</span><span class="p">,</span> <span class="n">numpy</span><span class="o">=</span><span class="mf">0.0001</span><span class="n">s</span><span class="p">,</span> <span class="n">speedup</span><span class="o">=</span><span class="mf">0.00</span><span class="n">x</span>
<span class="n">PASSED</span>
<span class="n">tests</span><span class="o">/</span><span class="n">test_performance</span><span class="o">.</span><span class="n">py</span><span class="p">::</span><span class="n">test_performance_comparison</span><span class="p">[</span><span class="n">size1</span><span class="p">]</span>
<span class="n">Matrix</span> <span class="n">size</span> <span class="p">(</span><span class="mi">500</span><span class="p">,</span> <span class="mi">20</span><span class="p">):</span> <span class="n">optcorr</span><span class="o">=</span><span class="mf">0.0130</span><span class="n">s</span><span class="p">,</span> <span class="n">numpy</span><span class="o">=</span><span class="mf">0.0001</span><span class="n">s</span><span class="p">,</span> <span class="n">speedup</span><span class="o">=</span><span class="mf">0.01</span><span class="n">x</span>
<span class="n">PASSED</span>
<span class="n">tests</span><span class="o">/</span><span class="n">test_performance</span><span class="o">.</span><span class="n">py</span><span class="p">::</span><span class="n">test_performance_comparison</span><span class="p">[</span><span class="n">size2</span><span class="p">]</span>
<span class="n">Matrix</span> <span class="n">size</span> <span class="p">(</span><span class="mi">1000</span><span class="p">,</span> <span class="mi">30</span><span class="p">):</span> <span class="n">optcorr</span><span class="o">=</span><span class="mf">0.0112</span><span class="n">s</span><span class="p">,</span> <span class="n">numpy</span><span class="o">=</span><span class="mf">0.0001</span><span class="n">s</span><span class="p">,</span> <span class="n">speedup</span><span class="o">=</span><span class="mf">0.01</span><span class="n">x</span>
<span class="n">PASSED</span>
<span class="n">tests</span><span class="o">/</span><span class="n">test_performance</span><span class="o">.</span><span class="n">py</span><span class="p">::</span><span class="n">test_gpu_speedup_if_available</span>
<span class="n">Large</span> <span class="n">matrix</span> <span class="p">(</span><span class="mi">2000</span><span class="n">x50</span><span class="p">)</span> <span class="n">correlation</span> <span class="n">computed</span> <span class="ow">in</span> <span class="mf">0.0120</span><span class="n">s</span>
<span class="n">PASSED</span>

<span class="o">==============================</span> <span class="mi">4</span> <span class="n">passed</span> <span class="ow">in</span> <span class="mf">0.53</span><span class="n">s</span> <span class="o">===============================</span>
</pre></div>
</div>
<p>In general, there is some overhead for using the Einstein summation notation method, so it will probably only help when the size of the matrix (particularly the number of columns) is large.  To explore this, let’s ask Claude to generate a Jupyter notebook where we can look at how performance changes as a function of the matrix size.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>User: Generate a Jupyter notebook that I can use to explore the relationship 
between matrix size and performance.  It seems that the number of columns 
matters more than the number of rows for performance, so generate performance 
values for a range of column dimensionality up to 2000, and plot timing as 
a function of column dimensionality.

⏺ I&#39;ve created a comprehensive Jupyter notebook performance_analysis.ipynb
  that explores the relationship between matrix dimensions and performance.
   The notebook includes:

  Key Features:

  1. Systematic Testing:
    - Column dimensionality: 5 to 2000 columns (fixed at 1000 rows)
    - Row dimensionality: 100 to 10,000 rows (fixed at 100 columns)
  2. Performance Metrics:
    - Timing comparisons between optcorr and numpy.corrcoef
    - Speedup calculations
    - Accuracy verification
  3. Visualizations:
    - Performance vs column/row dimensions
    - Speedup analysis
    - Computational complexity scaling
    - Log-scale plots for better trend visibility
  4. Analysis:
    - Empirical confirmation of O(n_cols²) scaling
    - GPU acceleration effectiveness across problem sizes
    - Detailed performance tables and summary statistics

  The notebook will help you understand:
  - How column dimensionality quadratically affects performance
  - When GPU acceleration provides benefits vs overhead costs
  - Scaling characteristics of Einstein summation vs standard methods
  - Numerical accuracy across all tested dimensions

  To run it, you&#39;ll need to install additional dependencies:
  uv add matplotlib pandas seaborn jupyter

  Then launch with:
  jupyter notebook performance_analysis.ipynb
</pre></div>
</div>
<p>The <a class="reference internal" href="#performance_analysis.ipynb"><span class="xref myst">resulting notebook</span></a> provides a nice exploration of the performance of our function, showing that it only provides speedup for datasets with more than about 2500 columns, with a relatively minimal impact of the number of rows.  Based on this, we can have our function default to using <code class="docutils literal notranslate"><span class="pre">numpy</span></code> if there are are fewer than 2500 rows (modifiable by an argument):</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>User: It seems that our optimization only helps if there are more than about 2500 
columns in the dataset.  So, let&#39;s add an argument to the function that specifies 
a minimum number of columns, under which the function will default to using numpy.

... (lots of interim output deleted)

⏺ Excellent! All tests pass. Now let&#39;s add a test to verify the new parameter 
works correctly:

... (lots of interim output deleted)

⏺ Summary

  I&#39;ve successfully added adaptive backend selection to the einsumcorr
  function:

  Key Changes:

  1. New Parameter: Added min_cols_for_gpu=2500 parameter to optcorr()
  function
  2. Automatic Backend Selection:
    - Uses numpy.corrcoef for matrices with &lt; 2500 total columns
    - Uses GPU/einsum for larger matrices
    - User can customize the threshold
  3. CLI Integration: Added --min-cols-for-gpu option to command-line
  interface
  4. Bug Fixes: Fixed single-column matrix handling in numpy backend
  5. Testing: Added comprehensive test for the new functionality

  Benefits:

  - Better Performance: Small matrices now use the faster numpy
  implementation
  - Flexible: Users can adjust the threshold based on their specific
  hardware/use case
  - Backward Compatible: Default behavior preserved with sensible defaults
  - Transparent: Same API, automatic optimization under the hood

  The function now automatically chooses the best backend based on matrix
  size, providing optimal performance across different use cases while
  maintaining the same interface.

</pre></div>
</div>
<p>The code that it generated for this looked reasonable, but running the notebook with the new code I didn’t actually see the expected change in performance for smaller matrices. In looking at the code, I saw a couple of issues that I asked Claude to fix:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>User: There are a couple of issues with the code from the last round. First,  the cutoff
is currently being based on the sum of the number of columns in the two matrices, 
when it should really be based on the maximum of the two column numbers. Second, 
if the total cols is less than the the min_cols_for_gpu and only one matrix was 
provided, then the call to _numpy_corrcoef should only include x.

⏺ You&#39;re absolutely right! Let me fix both issues:

  1. The cutoff should be based on the maximum number of columns, not the
  sum
  2. When falling back to numpy with a single matrix, we should only pass x\

... (lots of interim output deleted)

⏺ Summary

  I&#39;ve successfully fixed both issues you identified:

  ✅ Issue 1: Column Count Logic

  - Before: Used total_cols = x.shape[1] + (0 if y is x else y.shape[1])
  (sum)
  - After: Uses max_cols = max(x.shape[1], y.shape[1]) (maximum)
  - Rationale: The maximum column count determines computational
  complexity, not the sum

  ✅ Issue 2: Single Matrix Case

  - Before: Always passed both x and y to _numpy_corrcoef(x, y)
  - After: Only passes x when y is x: _numpy_corrcoef(x) vs
  _numpy_corrcoef(x, y)
  - Rationale: Avoids redundant parameters and ensures correct
  numpy.corrcoef behavior

  ✅ Additional Improvements

  - Updated docstring to clarify max column behavior
  - Added comprehensive test test_min_cols_for_gpu_uses_max_columns() to
  verify the max logic
  - All 27 tests passing

  Example Behavior

  # Two matrices: 20 columns vs 5 columns
  # Uses max(20, 5) = 20 for threshold comparison
  x = np.random.randn(100, 20)  # 20 columns
  y = np.random.randn(100, 5)   # 5 columns

  # threshold=15: 20 &gt; 15 → uses GPU
  # threshold=25: 20 &lt; 25 → uses numpy

  The adaptive backend selection now works correctly and efficiently
  chooses the optimal implementation based on the largest matrix dimension.

</pre></div>
</div>
<p>This works, but the results from our function still don’t match the numpy function for smaller matrices.  Let’s see if Claude can figure out what’s up:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>User: Something odd is happening.  When I run the performance tests in the 
notebook I expect to see that performance of optcorr() should be basically 
identical to performance of numpy_corrcoef for matrices below the threshold 
size.  However, the optcorr() results are still much slower for the smaller 
matrices, just as they were before we added the adaptive backend selelction.  
Can you figure out what&#39;s going on here?


⏺ You&#39;re absolutely right! There&#39;s definitely something wrong if optcorr()
  is still slow for small matrices when it should be using the numpy
  backend. Let me investigate what&#39;s happening.

...

⏺ I see a small overhead but let me check if there&#39;s an issue with the
  logic. Let me trace through what&#39;s actually happening:

...

⏺ The logic looks correct. Let me check if the issue is with import
  overhead or the way we&#39;re calling it. Let me look at what might be
  causing the performance difference:

...

⏺ I found part of the issue! There&#39;s overhead in the _numpy_corrcoef
  function. Let me check what&#39;s causing it:

...

⏺ The overhead is from the additional checks in _numpy_corrcoef. But
  there&#39;s another issue - let me trace through the actual optcorr function
  to see what&#39;s happening:

...

⏺ Good! This shows the adaptive backend is working correctly:
  - GPU path: 3222x slower (as expected for small matrices)
  - Numpy path: only 1.66x slower (reasonable overhead)

  The 1.66x overhead for the numpy path is from the input validation and
  wrapper function calls. This is actually pretty good!

  But let me check if there&#39;s something wrong in your notebook. The issue
  might be that the performance tests are inadvertently using the GPU path.
   Let me check the notebook logic:

...

 The adaptive selection is working perfectly! The small overhead (1.3-2x
  slower) for small matrices is expected due to:

  1. Function call overhead from the wrapper
  2. Input validation (type checking, NaN detection, shape validation)
  3. Reshaping logic for 1D arrays
  4. Additional checks in _numpy_corrcoef

  This ~1.5-2x overhead is actually quite reasonable for the benefits of
  having a unified API with automatic optimization. The key insight is that
   at 3000+ columns, the GPU version becomes 1.78x-5.5x faster than numpy,
  which is exactly what we want!

  The adaptive backend is working correctly! The &quot;much slower&quot; behavior you
   were seeing before was likely when all matrices were using the GPU path.
   Now small matrices use numpy (with reasonable overhead) and large
  matrices use GPU acceleration.
</pre></div>
</div>
<p>In this case, Claude seems to think that the difference for the smaller matrices is due to additional checking that happens in our function, which seems like a reasonable explanation.</p>
<p>In summary, this example provided an overview of the power of agentic coding tools like Claude Code, as well as highlighting the level of code analysis that the current models are capable of achieving.</p>
<hr class="footnotes docutils" />
<dl class="footnote brackets">
<dt class="label" id="id2"><span class="brackets"><a class="fn-backref" href="#id1">1</a></span></dt>
<dd><p>Confusingly, the term “API” is used in two different ways in different contexts.  In this chapter we are using it to refer to an actual system that one can interact with to send and receive messages.  However, in other contexts the term is used to refer to a specification for how to interact with a system.  For example, many software packages present an “API Reference” (for example, <a class="reference external" href="https://scikit-learn.org/stable/api/index.html">scikit-learn</a>), which specifies the interfaces to all of the classes and functions in the package.  It’s important to distinguish these two uses of the term to avoid confusion.</p>
</dd>
</dl>
</section>
</section>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./."
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="testing.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">Software testing</p>
      </div>
    </a>
    <a class="right-next"
       href="bibliography.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Bibliography</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#using-ai-coding-assistants">Using AI coding assistants</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#cautions-regarding-large-language-models">Cautions regarding large language models</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#ai-coding-approaches">AI coding approaches</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#chatbots">Chatbots</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#ide-assistants">IDE assistants</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#programmatic-access-via-api">Programmatic access via API</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#customizing-api-output">Customizing API output</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#agentic-coding-tools">Agentic coding tools</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#using-claude-code-an-example">Using Claude Code: An example</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#initializing-the-project">Initializing the project</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#initializing-the-claude-code-project">Initializing the Claude Code project</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#guiding-claude-through-the-development-process">Guiding Claude through the development process</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#test-development">Test development</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#core-implementation">Core implementation</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#performance-testing">Performance testing</a></li>
</ul>
</li>
</ul>
</li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Russell Poldrack et al.
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      © Copyright 2023.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b"></script>
<script src="_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>