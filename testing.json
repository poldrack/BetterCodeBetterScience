{"version":3,"kind":"Article","sha256":"4a9b742bc9345eb5c8c65ceb8dad06fabfc4e1910f1d479abbc57e1a600808fb","slug":"testing","location":"/testing.md","dependencies":[],"frontmatter":{"title":"Software testing","content_includes_title":false,"authors":[{"nameParsed":{"literal":"Russell Poldrack et al.","given":"Russell Poldrack","non_dropping_particle":"et","family":"al."},"name":"Russell Poldrack et al.","id":"contributors-myst-generated-uid-0"}],"github":"https://github.com/poldrack/BetterCodeBetterScience","source_url":"https://github.com/poldrack/BetterCodeBetterScience/blob/main/book/testing.md","edit_url":"https://github.com/poldrack/BetterCodeBetterScience/edit/main/book/testing.md","thumbnail":"/build/github_actions_failu-25d4b04ead335da5128a6f4198042f8f.png","exports":[{"format":"md","filename":"testing.md","url":"/build/testing-30a9d44a75e23ee6db9ea2b767caea82.md"}]},"mdast":{"type":"root","children":[{"type":"block","children":[{"type":"paragraph","position":{"start":{"line":4,"column":1},"end":{"line":4,"column":1}},"children":[{"type":"text","value":"Tests define the expected behavior of code, and detect when the code doesn’t match that expected behavior.","position":{"start":{"line":4,"column":1},"end":{"line":4,"column":1}},"key":"uXc809zkhy"}],"key":"zJ7I6N2L8t"},{"type":"paragraph","position":{"start":{"line":6,"column":1},"end":{"line":10,"column":1}},"children":[{"type":"text","value":"One useful analogy for software testing comes from the biosciences.\nThink for a moment about the rapid COVID-19 tests that we all came to know during the pandemic.\nThese tests had two lines, one of which was a ","position":{"start":{"line":6,"column":1},"end":{"line":6,"column":1}},"key":"Z8xNzoRmOl"},{"type":"emphasis","position":{"start":{"line":6,"column":1},"end":{"line":6,"column":1}},"children":[{"type":"text","value":"control","position":{"start":{"line":6,"column":1},"end":{"line":6,"column":1}},"key":"MTDvXQQLVL"}],"key":"xI3DPBTjrF"},{"type":"text","value":" line; if this line didn’t show up, then that meant that the test was not functioning as expected.\nThis is known as a ","position":{"start":{"line":6,"column":1},"end":{"line":6,"column":1}},"key":"LeUA70okwg"},{"type":"emphasis","position":{"start":{"line":6,"column":1},"end":{"line":6,"column":1}},"children":[{"type":"text","value":"positive control","position":{"start":{"line":6,"column":1},"end":{"line":6,"column":1}},"key":"rjpXYNrvjn"}],"key":"yexdfPORR7"},{"type":"text","value":" because it assesses the test’s ability to identify a positive response","position":{"start":{"line":6,"column":1},"end":{"line":6,"column":1}},"key":"gboCG6a8d5"},{"type":"footnoteReference","identifier":"1","label":"1","position":{"start":{"line":6,"column":1},"end":{"line":6,"column":1}},"enumerator":"1","key":"BaPvuv6VGs"},{"type":"text","value":".\nOther tests also include ","position":{"start":{"line":6,"column":1},"end":{"line":6,"column":1}},"key":"yNhHhC4uA4"},{"type":"emphasis","position":{"start":{"line":6,"column":1},"end":{"line":6,"column":1}},"children":[{"type":"text","value":"negative controls","position":{"start":{"line":6,"column":1},"end":{"line":6,"column":1}},"key":"rcbbmZlOW7"}],"key":"z7iT7pdaDl"},{"type":"text","value":", which ensure that the test returns a negative result when it should.","position":{"start":{"line":6,"column":1},"end":{"line":6,"column":1}},"key":"UbqIao4zWc"}],"key":"U0jcPRXkJC"},{"type":"paragraph","position":{"start":{"line":12,"column":1},"end":{"line":14,"column":1}},"children":[{"type":"text","value":"By analogy, we can think of software tests as being either positive or negative controls for the expected outcome of a software component.\nA positive test assesses whether, given a particular valid input, the component returns the correct output.\nA negative test assesses whether, in the absence of valid input, the component correctly returns the appropriate error message or null result.","position":{"start":{"line":12,"column":1},"end":{"line":12,"column":1}},"key":"WF3B17tfBo"}],"key":"QoVbr3VpX2"},{"type":"heading","depth":2,"position":{"start":{"line":16,"column":1},"end":{"line":16,"column":1}},"children":[{"type":"text","value":"Why use software tests?","position":{"start":{"line":16,"column":1},"end":{"line":16,"column":1}},"key":"RSsWyXgskW"}],"identifier":"why-use-software-tests","label":"Why use software tests?","html_id":"why-use-software-tests","implicit":true,"key":"q7mkcpCD47"},{"type":"paragraph","position":{"start":{"line":18,"column":1},"end":{"line":20,"column":1}},"children":[{"type":"text","value":"The most obvious reason to write tests for code is to make sure that the answers that the code gives you are correct.\nThis becomes increasingly important as AI assistants write more of the code, to the degree that testing is becoming ","position":{"start":{"line":18,"column":1},"end":{"line":18,"column":1}},"key":"uOAKTzkeEq"},{"type":"emphasis","position":{"start":{"line":18,"column":1},"end":{"line":18,"column":1}},"children":[{"type":"text","value":"more important","position":{"start":{"line":18,"column":1},"end":{"line":18,"column":1}},"key":"xHuwpqHo6H"}],"key":"rN4NxmudDu"},{"type":"text","value":" than code generation as a skill for generating good scientific code.\nBut creating correct code is far from the only reason for writing tests.","position":{"start":{"line":18,"column":1},"end":{"line":18,"column":1}},"key":"H3K4r70sJh"}],"key":"Fiy9LrEBeT"},{"type":"paragraph","position":{"start":{"line":22,"column":1},"end":{"line":25,"column":1}},"children":[{"type":"text","value":"A second reason for testing was highlighted in our earlier discussion of test-driven development.\nTests can provide the coder with a measure of task completion; when the tests pass, the job is done, other than refactoring the code to make it cleaner and more robust.\nWriting tests make one think harder about what exactly they want/need the code to do, and to specify those goals in as clear a way as possible.\nFocusing on tests can help keep the coder’s “eyes on the MVP prize” and prevent generating too much extraneous code (“gold plating”).","position":{"start":{"line":22,"column":1},"end":{"line":22,"column":1}},"key":"pLRaZSHORm"}],"key":"jZUHBumcRA"},{"type":"paragraph","position":{"start":{"line":27,"column":1},"end":{"line":30,"column":1}},"children":[{"type":"text","value":"A third reason to write tests is that they can help drive modularity in the code.\nIt’s much easier to write tests for a simple function that does a single thing than for a complex function with many different roles.\nTesting can also help drive modularity by causing you to think more clearly about what a function does when developing the test; the inability to easily write a test for a function can suggest that the function might be overly complex and should be refactored.\nIn this way, writing tests can give us useful insights into the structure of the code.","position":{"start":{"line":27,"column":1},"end":{"line":27,"column":1}},"key":"pSLnVzHyk7"}],"key":"KoecW2Bta2"},{"type":"paragraph","position":{"start":{"line":32,"column":1},"end":{"line":35,"column":1}},"children":[{"type":"text","value":"A final reason to write tests is that they make it much easier to make changes to the code.\nWithout a robust test suite, one is always left worried that changing some aspect of the code will have unexpected effects on its former behavior (known as a “regression”).\nTests can provide you with the comfort you need to make changes, knowing that you will detect any untoward effects your changes might have.\nThis includes refactoring, where the changes are not meant to modify the function but simply to make the code more robust and readable.","position":{"start":{"line":32,"column":1},"end":{"line":32,"column":1}},"key":"oeZBuh1obW"}],"key":"rjAOy8Pw7s"},{"type":"heading","depth":2,"position":{"start":{"line":38,"column":1},"end":{"line":38,"column":1}},"children":[{"type":"text","value":"Types of tests","position":{"start":{"line":38,"column":1},"end":{"line":38,"column":1}},"key":"qnoc6pAAEJ"}],"identifier":"types-of-tests","label":"Types of tests","html_id":"types-of-tests","implicit":true,"key":"Jnehl9u4cU"},{"type":"heading","depth":3,"position":{"start":{"line":40,"column":1},"end":{"line":40,"column":1}},"children":[{"type":"text","value":"Unit tests","position":{"start":{"line":40,"column":1},"end":{"line":40,"column":1}},"key":"X67HVrnqRH"}],"identifier":"unit-tests","label":"Unit tests","html_id":"unit-tests","implicit":true,"key":"Y4hAp7RgGO"},{"type":"paragraph","position":{"start":{"line":42,"column":1},"end":{"line":48,"column":1}},"children":[{"type":"text","value":"Unit tests are the bread and butter of software testing.\nThey are meant to assess whether individual software components (in the case of Python, functions, classes, and methods) perform as expected.\nThis includes both assessing whether the component performs as it is supposed to perform given a particular input, but also assessing whether it performs correctly under boundary conditions or problematic conditions, where the correct response is often to raise an exception.\nA major goal of unit testing in the latter case is preventing “garbage in, garbage out” behavior.\nFor example, say that we are testing a function that takes in two matrices, and that the size of these matrices along their first dimension is assumed to match.\nIn this case, we would want to test to make sure that if the function is provided with two matrices that mismatch in their first dimension, the function will respond by raising an exception rather than by giving back an answer that is incorrect or nonsensical (such as ","position":{"start":{"line":42,"column":1},"end":{"line":42,"column":1}},"key":"y7p1k0p2nJ"},{"type":"emphasis","position":{"start":{"line":42,"column":1},"end":{"line":42,"column":1}},"children":[{"type":"text","value":"NaN","position":{"start":{"line":42,"column":1},"end":{"line":42,"column":1}},"key":"oWWLmBrWay"}],"key":"eDJupSLzfE"},{"type":"text","value":", or “not a number”).\nThat is, we want to aim for “garbage in, exception out” behavior.","position":{"start":{"line":42,"column":1},"end":{"line":42,"column":1}},"key":"u8QvkwtG9U"}],"key":"gg8QUqezgH"},{"type":"heading","depth":3,"position":{"start":{"line":50,"column":1},"end":{"line":50,"column":1}},"children":[{"type":"text","value":"Integration tests","position":{"start":{"line":50,"column":1},"end":{"line":50,"column":1}},"key":"TQWCQpbDqI"}],"identifier":"integration-tests","label":"Integration tests","html_id":"integration-tests","implicit":true,"key":"Y2DMms130v"},{"type":"paragraph","position":{"start":{"line":52,"column":1},"end":{"line":52,"column":1}},"children":[{"type":"text","value":"As the name suggests, an integration test assesses whether the entire application works as it should, integrating all of the components that were tested via unit testing.","position":{"start":{"line":52,"column":1},"end":{"line":52,"column":1}},"key":"ff526U2jD9"}],"key":"kpMc1lEgtg"},{"type":"paragraph","position":{"start":{"line":54,"column":1},"end":{"line":60,"column":1}},"children":[{"type":"text","value":"One simple type of integration test is a “smoke test”.\nThis name ","position":{"start":{"line":54,"column":1},"end":{"line":54,"column":1}},"key":"YVuQg0Gz9F"},{"type":"link","url":"https://learn.microsoft.com/en-us/previous-versions/ms182613(v=vs.80)","position":{"start":{"line":54,"column":1},"end":{"line":54,"column":1}},"children":[{"type":"text","value":"apparently","position":{"start":{"line":54,"column":1},"end":{"line":54,"column":1}},"key":"A9Dzthhl25"}],"urlSource":"https://learn.microsoft.com/en-us/previous-versions/ms182613(v=vs.80)","key":"p6o1hMk1du"},{"type":"text","value":" derives from the computer hardware industry, where one often performs an initial sanity test on an electronic component by plugging it in and seeing if it smokes.\nIn coding, a smoke test is a simple sanity check meant to ensure that the entire application runs without crashing.\nThis is usually accomplished by running a top-level function that exercises the entire application.\nSmoke tests are useful for quickly identifying major problems, but they don’t actually test whether the application performs its function correctly.\nThey can be especially useful for large applications, where the full test suite may take hours to run.\nAn initial smoke test can determine whether something is broken downstream, saving lots of wasted testing time.","position":{"start":{"line":54,"column":1},"end":{"line":54,"column":1}},"key":"AulrW5JKwV"}],"key":"KayOmXJXti"},{"type":"paragraph","position":{"start":{"line":62,"column":1},"end":{"line":63,"column":1}},"children":[{"type":"text","value":"Full integration tests assess the function of the entire application; one can think of them as unit tests where the unit is the entire application.\nJust as with unit tests, we want integration tests that both confirm proper operation under intended conditions, as well as confirming proper behavior (such as exiting with an error message) under improper conditions.","position":{"start":{"line":62,"column":1},"end":{"line":62,"column":1}},"key":"NGdYHmUm4U"}],"key":"uv94NuC19z"},{"type":"heading","depth":2,"position":{"start":{"line":65,"column":1},"end":{"line":65,"column":1}},"children":[{"type":"text","value":"The anatomy of a test","position":{"start":{"line":65,"column":1},"end":{"line":65,"column":1}},"key":"x2vnyTmNYr"}],"identifier":"the-anatomy-of-a-test","label":"The anatomy of a test","html_id":"the-anatomy-of-a-test","implicit":true,"key":"vkwI38EJaG"},{"type":"paragraph","position":{"start":{"line":67,"column":1},"end":{"line":68,"column":1}},"children":[{"type":"text","value":"A test is generally structured as a function that executes without raising an exception as long as the code behaves in an expected way.\nLet’s say that we want to generate a function that returns the escape velocity of a planet:","position":{"start":{"line":67,"column":1},"end":{"line":67,"column":1}},"key":"tcXs3QIZPD"}],"key":"P6x8eMxb1t"},{"type":"code","lang":"python","value":"import math\nimport numpy as np\n\ndef escape_velocity(mass: float, radius: float, G=6.67430e-11):\n    \"\"\"\n    Calculate the escape velocity from a celestial body, given its mass and radius.\n\n    Args:\n    mass (float): Mass of the celestial body in kg.\n    radius (float): Radius of the celestial body in meters.\n\n    Returns:\n    float: Escape velocity in m/s.\n    \"\"\"\n    \n    return math.sqrt(2 * G * mass / radius)","position":{"start":{"line":70,"column":1},"end":{"line":87,"column":1}},"key":"BJWIGfJcKg"},{"type":"paragraph","position":{"start":{"line":89,"column":1},"end":{"line":89,"column":1}},"children":[{"type":"text","value":"We can then generate a test to determine whether the value returned by our function matches the known value for a given planet:","position":{"start":{"line":89,"column":1},"end":{"line":89,"column":1}},"key":"uEBBFhHIN2"}],"key":"ruKMeCla7Z"},{"type":"code","lang":"python","value":"def test_escape_velocity():\n    \"\"\"\n    Test the escape_velocity function with known values.\n    \"\"\"\n    mass_earth = 5.972e24  # Earth mass in kg\n    radius_earth = 6.371e6  # Earth radius in meters\n    ev_expected = 11186.0  # Expected escape velocity for Earth in m/s\n    ev_computed = escape_velocity(mass_earth, radius_earth)\n    assert np.allclose(ev_expected, ev_computed), \"Test failed!\"","position":{"start":{"line":91,"column":1},"end":{"line":101,"column":1}},"key":"Y5iR9xvXSd"},{"type":"paragraph","position":{"start":{"line":103,"column":1},"end":{"line":103,"column":1}},"children":[{"type":"text","value":"We can run this using ","position":{"start":{"line":103,"column":1},"end":{"line":103,"column":1}},"key":"R8GTustR0a"},{"type":"inlineCode","value":"pytest","position":{"start":{"line":103,"column":1},"end":{"line":103,"column":1}},"key":"jBPA3pnX03"},{"type":"text","value":" (more about this later), which tells us that the test passes:","position":{"start":{"line":103,"column":1},"end":{"line":103,"column":1}},"key":"oddkoBU8Od"}],"key":"kZyVymqTjY"},{"type":"code","lang":"bash","value":"❯ pytest src/BetterCodeBetterScience/escape_velocity.py\n====================== test session starts ======================\n\nsrc/BetterCodeBetterScience/escape_velocity.py ..          [100%]\n\n======================= 1 passed in 0.10s =======================","position":{"start":{"line":105,"column":1},"end":{"line":112,"column":1}},"key":"xZv6Cb1HT7"},{"type":"paragraph","position":{"start":{"line":115,"column":1},"end":{"line":116,"column":1}},"children":[{"type":"text","value":"If the returned value didn’t match the known value (within a given level of tolerance, which is handled by ","position":{"start":{"line":115,"column":1},"end":{"line":115,"column":1}},"key":"N7Ib6kMf6l"},{"type":"inlineCode","value":"np.allclose()","position":{"start":{"line":115,"column":1},"end":{"line":115,"column":1}},"key":"WbxTALKNcC"},{"type":"text","value":"), then the assertion will fail and raise an exception, causing the test to fail.\nFor example, if we had mis-specified the expected value as 1186.0, we would have seen an error like this:","position":{"start":{"line":115,"column":1},"end":{"line":115,"column":1}},"key":"KkwrKpRkGY"}],"key":"FiRtQ4iPkx"},{"type":"code","lang":"bash","value":"❯ pytest src/BetterCodeBetterScience/escape_velocity.py\n====================== test session starts ======================\n\nsrc/BetterCodeBetterScience/escape_velocity.py F          [100%]\n\n=========================== FAILURES ===========================\n_____________________ test_escape_velocity _____________________\n\n    def test_escape_velocity():\n        \"\"\"\n        Test the escape_velocity function with known values.\n        \"\"\"\n        mass_earth = 5.972e24  # Earth mass in kg\n        radius_earth = 6.371e6  # Earth radius in meters\n        ev_expected = 1186.0 # 11186.0  # Expected escape velocity for Earth in m/s\n        ev_computed = escape_velocity(mass_earth, radius_earth)\n>       assert np.allclose(ev_expected, ev_computed), \"Test failed!\"\nE       AssertionError: Test failed!\nE       assert False\nE        +  where False = <function allclose at 0x101403370>(1186.0, 11185.97789184991)\nE        +    where <function allclose at 0x101403370> = np.allclose\n\nsrc/BetterCodeBetterScience/escape_velocity.py:26: AssertionError\n===================== short test summary info =====================\nFAILED src/BetterCodeBetterScience/escape_velocity.py::test_escape_velocity - AssertionError: Test failed!\n======================== 1 failed in 0.11s ========================","position":{"start":{"line":118,"column":1},"end":{"line":145,"column":1}},"key":"tSt4tl3IK4"},{"type":"paragraph","position":{"start":{"line":147,"column":1},"end":{"line":149,"column":1}},"children":[{"type":"text","value":"It’s also important to make sure that an exception is raised whenever it should be.\nFor example, the version of the ","position":{"start":{"line":147,"column":1},"end":{"line":147,"column":1}},"key":"lKmfCc8fqO"},{"type":"inlineCode","value":"escape_velocity()","position":{"start":{"line":147,"column":1},"end":{"line":147,"column":1}},"key":"DkLsT8DpTV"},{"type":"text","value":" function above did not check to make sure that the mass and radius arguments had positive values, which means that it would give nonsensical results when passed a negative mass or radius value.\nTo address this we should add code to the function that causes it to raise an exception if either of the arguments is negative:","position":{"start":{"line":147,"column":1},"end":{"line":147,"column":1}},"key":"gN6FGg3FR7"}],"key":"iEVuivFk1i"},{"type":"code","lang":"python","value":"def escape_velocity(mass: float, radius: float, G=6.67430e-11):\n    \"\"\"\n    Calculate the escape velocity from a celestial body, given its mass and radius.\n\n    Args:\n    mass (float): Mass of the celestial body in kg.\n    radius (float): Radius of the celestial body in meters.\n\n    Returns:\n    float: Escape velocity in m/s.\n    \"\"\"\n    if mass <= 0 or radius <= 0:\n        raise ValueError(\"Mass and radius must be positive values.\")\n    return math.sqrt(2 * G * mass / radius)\n","position":{"start":{"line":151,"column":1},"end":{"line":167,"column":1}},"key":"LJ7CSOiq2q"},{"type":"paragraph","position":{"start":{"line":169,"column":1},"end":{"line":170,"column":1}},"children":[{"type":"text","value":"We can then specify a test that checks whether the function properly raises an exception when passed a negative value.\nTo do this we can use a feature of the ","position":{"start":{"line":169,"column":1},"end":{"line":169,"column":1}},"key":"ZcaAbzZLIU"},{"type":"inlineCode","value":"pytest","position":{"start":{"line":169,"column":1},"end":{"line":169,"column":1}},"key":"zLbTlakTBY"},{"type":"text","value":" package (","position":{"start":{"line":169,"column":1},"end":{"line":169,"column":1}},"key":"ovEiaXrsaR"},{"type":"inlineCode","value":"pytest.raises","position":{"start":{"line":169,"column":1},"end":{"line":169,"column":1}},"key":"MGn2UeVAhQ"},{"type":"text","value":") that passes only if the specified exception is raised:","position":{"start":{"line":169,"column":1},"end":{"line":169,"column":1}},"key":"tJPBktvJM6"}],"key":"FPqcQg7FDg"},{"type":"code","lang":"python","value":"def test_escape_velocity_negative():\n    \"\"\"\n    Make sure the function raises ValueError for negative mass or radius.\n    \"\"\"\n    with pytest.raises(ValueError):\n        escape_velocity(-5.972e24, 6.371e6)","position":{"start":{"line":172,"column":1},"end":{"line":179,"column":1}},"key":"BnbEEoZ2n0"},{"type":"heading","depth":2,"position":{"start":{"line":182,"column":1},"end":{"line":182,"column":1}},"children":[{"type":"text","value":"When to write tests","position":{"start":{"line":182,"column":1},"end":{"line":182,"column":1}},"key":"nsR22fyKzC"}],"identifier":"when-to-write-tests","label":"When to write tests","html_id":"when-to-write-tests","implicit":true,"key":"aSSDB2Yzzf"},{"type":"paragraph","position":{"start":{"line":184,"column":1},"end":{"line":186,"column":1}},"children":[{"type":"text","value":"Too often researchers decide to write tests after they have written an entire codebase.\nHaving any tests is certainly better than having no tests, but integrating testing into ones development workflow from the start can help improve the development experience and ultimately lead to better and more maintainable software.\nIn Chapter 1 we mentioned the idea of ","position":{"start":{"line":184,"column":1},"end":{"line":184,"column":1}},"key":"N7oIZnExUT"},{"type":"emphasis","position":{"start":{"line":184,"column":1},"end":{"line":184,"column":1}},"children":[{"type":"text","value":"test-driven development","position":{"start":{"line":184,"column":1},"end":{"line":184,"column":1}},"key":"oAPssvpIRP"}],"key":"ebWXnixams"},{"type":"text","value":", which we outline in more detail below, but we first discuss a simple approach to introducing testing into the development process.","position":{"start":{"line":184,"column":1},"end":{"line":184,"column":1}},"key":"yHBGRNRIbT"}],"key":"ozlxnhnO2B"},{"type":"heading","depth":3,"position":{"start":{"line":188,"column":1},"end":{"line":188,"column":1}},"children":[{"type":"text","value":"Bug-driven testing: Any time you encounter a bug, write a test","position":{"start":{"line":188,"column":1},"end":{"line":188,"column":1}},"key":"RwUAHiGUAv"}],"identifier":"bug-driven-testing-any-time-you-encounter-a-bug-write-a-test","label":"Bug-driven testing: Any time you encounter a bug, write a test","html_id":"bug-driven-testing-any-time-you-encounter-a-bug-write-a-test","implicit":true,"key":"qelYMmL5yg"},{"type":"paragraph","position":{"start":{"line":190,"column":1},"end":{"line":192,"column":1}},"children":[{"type":"text","value":"An easy way to introduce testing into the development process is to write a new test any time one encounters a bug, which we refer to as ","position":{"start":{"line":190,"column":1},"end":{"line":190,"column":1}},"key":"svoWzrkwPh"},{"type":"emphasis","position":{"start":{"line":190,"column":1},"end":{"line":190,"column":1}},"children":[{"type":"text","value":"bug-driven testing","position":{"start":{"line":190,"column":1},"end":{"line":190,"column":1}},"key":"UZLgo7OjYl"}],"key":"GLemuzQBp8"},{"type":"text","value":".\nThis makes it easy to then work on fixing the bug, since the test will determine when the bug has been fixed.\nIn addition, the test will detect if future changes reintroduce the bug.","position":{"start":{"line":190,"column":1},"end":{"line":190,"column":1}},"key":"cqZ7d6KCcI"}],"key":"wzOzcNexHp"},{"type":"paragraph","position":{"start":{"line":194,"column":1},"end":{"line":194,"column":1}},"children":[{"type":"text","value":"As an example, take the following function:","position":{"start":{"line":194,"column":1},"end":{"line":194,"column":1}},"key":"S3X8J6n9i1"}],"key":"zEwzsUa0Vr"},{"type":"code","lang":"python","value":"def find_outliers(data: List[float], threshold: float = 2.0) -> List[int]:\n    \"\"\"\n    Find outliers in a dataset using z-score method.\n    \n    Parameters\n    ----------\n    data : List[float]\n        List of numerical values.\n    threshold : float, default=2.0\n        Number of standard deviations from the mean to consider a value as an outlier.\n    \n    Returns\n    -------\n    List[int]\n        List of indices of outliers in the data.\n    \"\"\"\n    \n    mean = sum(data) / len(data)\n    variance = sum((x - mean) ** 2 for x in data) / len(data)\n    std = variance ** 0.5\n    \n    # Bug: division by zero when std is 0 (all values are identical)\n    # This only happens when all data points are the same\n    outliers = []\n    for i, value in enumerate(data):\n        z_score = abs(value - mean) / std \n        if z_score > threshold:\n            outliers.append(i)\n    \n    return outliers","position":{"start":{"line":196,"column":1},"end":{"line":227,"column":1}},"key":"pbNxhKGo7m"},{"type":"paragraph","position":{"start":{"line":229,"column":1},"end":{"line":229,"column":1}},"children":[{"type":"text","value":"This code works to properly identify outliers:","position":{"start":{"line":229,"column":1},"end":{"line":229,"column":1}},"key":"ZO3kHgyWW6"}],"key":"I2FF7lJ0u9"},{"type":"code","lang":"python","value":"In : data = [1, 2, 3, 1000, 4, 5, 6]\n\nIn : find_outliers(data)\nOut: [3]","position":{"start":{"line":231,"column":1},"end":{"line":236,"column":1}},"key":"RzebEWt1pb"},{"type":"paragraph","position":{"start":{"line":238,"column":1},"end":{"line":238,"column":1}},"children":[{"type":"text","value":"However, it fails due to a division by zero if all of the values are equal:","position":{"start":{"line":238,"column":1},"end":{"line":238,"column":1}},"key":"npAKvgd4cm"}],"key":"U6se09Wxas"},{"type":"code","lang":"python","value":"In : data = [1, 1, 1, 1, 1]\n\nIn : find_outliers(data)\n---------------------------------------------------------------------------\nZeroDivisionError                         Traceback (most recent call last)\nCell In[21], line 1\n----> 1 find_outliers(data)\n\nCell In[9], line 26, in find_outliers(data, threshold)\n     24 outliers = []\n     25 for i, value in enumerate(data):\n---> 26     z_score = abs(value - mean) / std \n     27     if z_score > threshold:\n     28         outliers.append(i)\n\nZeroDivisionError: float division by zero\n","position":{"start":{"line":240,"column":1},"end":{"line":258,"column":1}},"key":"tvsE6yTyg9"},{"type":"paragraph","position":{"start":{"line":260,"column":1},"end":{"line":261,"column":1}},"children":[{"type":"text","value":"Our intended behavior if all of the values are equal is to return an empty list, since there are by definition no outliers.\nBut before we do this, let’s create a couple of tests to check for the intended behavior and provide useful error messages if the test fails:","position":{"start":{"line":260,"column":1},"end":{"line":260,"column":1}},"key":"LANeEG7T6w"}],"key":"nqcTxOnpQm"},{"type":"code","lang":"python","value":"def test_find_outliers_normal_case():\n    data = [1, 2, 3, 4, 5, 100]  # 100 is clearly an outlier\n    outliers = find_outliers(data, threshold=2.0)\n    \n    # Should find the outlier at index 5\n    assert 5 in outliers, f\"Failed to detect outlier: {outliers}\"\n    assert len(outliers) == 1, f'Expected exactly one outlier, got: {len(outliers)}'\n\n\ndef test_find_outliers_identical_values():\n    data = [5, 5, 5, 5, 5]  # All identical values\n    \n    outliers = find_outliers(data, threshold=2.0)\n    assert outliers == [], f\"Expected no outliers for identical values, got {outliers}\"","position":{"start":{"line":263,"column":1},"end":{"line":278,"column":1}},"key":"x5ZsD4b3sR"},{"type":"paragraph","position":{"start":{"line":280,"column":1},"end":{"line":280,"column":1}},"children":[{"type":"text","value":"Running this with the original function definition, we see that it fails:","position":{"start":{"line":280,"column":1},"end":{"line":280,"column":1}},"key":"qlF1gJXLdg"}],"key":"nHPZjcI8Xy"},{"type":"code","lang":"python","value":"❯ pytest src/BetterCodeBetterScience/bug_driven_testing.py\n=========================== test session starts ===========================\ncollected 2 items\n\nsrc/BetterCodeBetterScience/bug_driven_testing.py .F                [100%]\n\n================================ FAILURES =================================\n___________________ test_find_outliers_identical_values ___________________\n\n    def test_find_outliers_identical_values():\n        data = [5, 5, 5, 5, 5]  # All identical values\n\n>       outliers = find_outliers(data, threshold=2.0)\n\nsrc/BetterCodeBetterScience/bug_driven_testing.py:50:\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n\ndata = [5, 5, 5, 5, 5], threshold = 2.0\n\n    def find_outliers(data: List[float], threshold: float = 2.0) -> List[int]:\n        \"\"\"\n        Find outliers in a dataset using z-score method.\n\n        Parameters\n        ----------\n        data : List[float]\n            List of numerical values.\n        threshold : float, default=2.0\n            Number of standard deviations from the mean to consider a value as an outlier.\n\n        Returns\n        -------\n        List[int]\n            List of indices of outliers in the data.\n        \"\"\"\n\n        mean = sum(data) / len(data)\n        variance = sum((x - mean) ** 2 for x in data) / len(data)\n        std = variance ** 0.5\n\n        # Bug: division by zero when std is 0 (all values are identical)\n        # This only happens when all data points are the same\n        outliers = []\n        for i, value in enumerate(data):\n>           z_score = abs(value - mean) / std  # Bug: std can be 0!\nE           ZeroDivisionError: float division by zero\n\nsrc/BetterCodeBetterScience/bug_driven_testing.py:31: ZeroDivisionError\n========================= short test summary info =========================\nFAILED src/BetterCodeBetterScience/bug_driven_testing.py::test_find_outliers_identical_values\n - ZeroDivisionError: float division by zero\n======================= 1 failed, 1 passed in 0.10s =======================","position":{"start":{"line":282,"column":1},"end":{"line":335,"column":1}},"key":"wCnxFmQVNM"},{"type":"paragraph","position":{"start":{"line":337,"column":1},"end":{"line":337,"column":1}},"children":[{"type":"text","value":"We can now fix the code by returning an empty list if zero standard deviation is detected:","position":{"start":{"line":337,"column":1},"end":{"line":337,"column":1}},"key":"mSji0Z4mpM"}],"key":"nNbXG6yP2R"},{"type":"code","lang":"python","value":"    ...\n    if std == 0:\n        # If standard deviation is zero, all values are identical, so no outliers\n        return []","position":{"start":{"line":339,"column":1},"end":{"line":344,"column":1}},"key":"p1IvcitrRo"},{"type":"paragraph","position":{"start":{"line":346,"column":1},"end":{"line":347,"column":1}},"children":[{"type":"text","value":"Here we add a comment to explain the intention of the statement.\nRunning the tests now will show that the problem is fixed:","position":{"start":{"line":346,"column":1},"end":{"line":346,"column":1}},"key":"MJQKqoQ9TT"}],"key":"KFb0VJBPeN"},{"type":"code","lang":"python","value":"❯ pytest src/BetterCodeBetterScience/bug_driven_testing.py\n=========================== test session starts ===========================\ncollected 2 items\n\nsrc/BetterCodeBetterScience/bug_driven_testing.py ..                [100%]\n\n============================ 2 passed in 0.08s ============================\n","position":{"start":{"line":349,"column":1},"end":{"line":358,"column":1}},"key":"D0Pyreob6x"},{"type":"paragraph","position":{"start":{"line":360,"column":1},"end":{"line":360,"column":1}},"children":[{"type":"text","value":"Now we can continue coding with confidence that if we happen to accidentally reintroduce the bug, it will be caught.","position":{"start":{"line":360,"column":1},"end":{"line":360,"column":1}},"key":"DwsOe3fNgI"}],"key":"DlsqNGdkOW"},{"type":"heading","depth":2,"position":{"start":{"line":362,"column":1},"end":{"line":362,"column":1}},"children":[{"type":"text","value":"The structure of a good test","position":{"start":{"line":362,"column":1},"end":{"line":362,"column":1}},"key":"WmaCdslQrx"}],"identifier":"the-structure-of-a-good-test","label":"The structure of a good test","html_id":"the-structure-of-a-good-test","implicit":true,"key":"lm2XQb2B34"},{"type":"paragraph","position":{"start":{"line":364,"column":1},"end":{"line":364,"column":1}},"children":[{"type":"text","value":"A commonly used scheme for writing a test is “given/when/then”:","position":{"start":{"line":364,"column":1},"end":{"line":364,"column":1}},"key":"hhD24vutwO"}],"key":"aQjqMAz25r"},{"type":"list","ordered":false,"spread":false,"position":{"start":{"line":366,"column":1},"end":{"line":369,"column":1}},"children":[{"type":"listItem","spread":true,"position":{"start":{"line":366,"column":1},"end":{"line":366,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"given some particular situation as background","position":{"start":{"line":366,"column":1},"end":{"line":366,"column":1}},"key":"SpfA2S4PKD"}],"key":"I2rGWdKrnA"}],"key":"KoX8Htn5JR"},{"type":"listItem","spread":true,"position":{"start":{"line":367,"column":1},"end":{"line":367,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"when something happens (such as a particular input)","position":{"start":{"line":367,"column":1},"end":{"line":367,"column":1}},"key":"cks5yyjEmT"}],"key":"O4V0WKdeZ7"}],"key":"l82AgaOIdp"},{"type":"listItem","spread":true,"position":{"start":{"line":368,"column":1},"end":{"line":369,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"then something else should happen (such as a particular output or exception)","position":{"start":{"line":368,"column":1},"end":{"line":368,"column":1}},"key":"bopUqL3MjC"}],"key":"C8XldSqxyg"}],"key":"h2b6gLEssF"}],"key":"qpmSmO56um"},{"type":"paragraph","position":{"start":{"line":370,"column":1},"end":{"line":372,"column":1}},"children":[{"type":"text","value":"Importantly, a test should only test one thing at a time.\nThis doesn’t mean that the test should necessarily only test for one specific error at a time; rather, it means that the test should assess a specific situation (“given/when”), and then assess all of the possible outcomes that are necessary to ensure that the component functions properly (“then”).\nYou can see this in the test for zero standard deviation that we generated in the earlier example, which actually tested for two conditions (the intended value being present in the list, and the list having a length of one) that together define the condition that we are interested in testing for.","position":{"start":{"line":370,"column":1},"end":{"line":370,"column":1}},"key":"DtQMLnYGHu"}],"key":"CO3xdVrLRO"},{"type":"paragraph","position":{"start":{"line":374,"column":1},"end":{"line":374,"column":1}},"children":[{"type":"text","value":"How do we test that the output of a function is correct given the input? There are different answers for different situations:","position":{"start":{"line":374,"column":1},"end":{"line":374,"column":1}},"key":"GZbYJR2SdO"}],"key":"JMoUaCmXnW"},{"type":"list","ordered":false,"spread":false,"position":{"start":{"line":376,"column":1},"end":{"line":380,"column":1}},"children":[{"type":"listItem","spread":true,"position":{"start":{"line":376,"column":1},"end":{"line":376,"column":1}},"children":[{"type":"paragraph","children":[{"type":"emphasis","position":{"start":{"line":376,"column":1},"end":{"line":376,"column":1}},"children":[{"type":"text","value":"commonly known answer","position":{"start":{"line":376,"column":1},"end":{"line":376,"column":1}},"key":"X21V2lRvEF"}],"key":"wJ2V5qxLhN"},{"type":"text","value":": Sometimes we possess inputs where the output is known.  For example, if we were creating a function that computes the circumference of a circle, then we know that the output for an input radius of 1 should be 2 * pi.  This is generally only the case for very simple functions.","position":{"start":{"line":376,"column":1},"end":{"line":376,"column":1}},"key":"dVBviLfMyF"}],"key":"M84A5tVe2I"}],"key":"Mp2sJH2nqG"},{"type":"listItem","spread":true,"position":{"start":{"line":377,"column":1},"end":{"line":377,"column":1}},"children":[{"type":"paragraph","children":[{"type":"emphasis","position":{"start":{"line":377,"column":1},"end":{"line":377,"column":1}},"children":[{"type":"text","value":"reference implementation","position":{"start":{"line":377,"column":1},"end":{"line":377,"column":1}},"key":"pOUYG6Igxb"}],"key":"pbk0KLpwRm"},{"type":"text","value":": In other cases we may have a standard implementation of an algorithm that we can compare against.  While in general it’s not a good idea to reimplement code that already exists in a standard library, in come cases we may want to extend existing code but also check that the basic version still works as planned.","position":{"start":{"line":377,"column":1},"end":{"line":377,"column":1}},"key":"LC6ZeOnqEC"}],"key":"btRs9HUXQC"}],"key":"iPLADi1fZH"},{"type":"listItem","spread":true,"position":{"start":{"line":378,"column":1},"end":{"line":378,"column":1}},"children":[{"type":"paragraph","children":[{"type":"emphasis","position":{"start":{"line":378,"column":1},"end":{"line":378,"column":1}},"children":[{"type":"text","value":"parallel implementation","position":{"start":{"line":378,"column":1},"end":{"line":378,"column":1}},"key":"fn83z6Qe0l"}],"key":"RbZVQ9e6yS"},{"type":"text","value":": Some times we don’t have a reference implementation, but we can code up another parallel implementation to compare our code to.  It’s important that this isn’t just a copy of the code used in the function; in that case, it’s really not a test at all!","position":{"start":{"line":378,"column":1},"end":{"line":378,"column":1}},"key":"TtbwaRgPXK"}],"key":"kuGwQ4gmvR"}],"key":"nUWpKuPiJm"},{"type":"listItem","spread":true,"position":{"start":{"line":379,"column":1},"end":{"line":380,"column":1}},"children":[{"type":"paragraph","children":[{"type":"emphasis","position":{"start":{"line":379,"column":1},"end":{"line":379,"column":1}},"children":[{"type":"text","value":"behavioral test","position":{"start":{"line":379,"column":1},"end":{"line":379,"column":1}},"key":"QcLZERBzWZ"}],"key":"ROoPQN5i2n"},{"type":"text","value":": Sometimes the best we can do is to run the code repeatedly and ensure that it behaves as expected on average.  For example, if a function outputs a numerical value and we know the expected distribution of that value given a particular input, we can ensure that the result matches that distribution with a high probability.  Such ","position":{"start":{"line":379,"column":1},"end":{"line":379,"column":1}},"key":"wvDESfIfSZ"},{"type":"emphasis","position":{"start":{"line":379,"column":1},"end":{"line":379,"column":1}},"children":[{"type":"text","value":"probabilistic tests","position":{"start":{"line":379,"column":1},"end":{"line":379,"column":1}},"key":"C7G5s0kgl2"}],"key":"HhAxtyH4Td"},{"type":"text","value":" are not optimal in the sense that they can occasionally fail even when the code is correct, but they are sometimes the best we can do.","position":{"start":{"line":379,"column":1},"end":{"line":379,"column":1}},"key":"dTOcEgWaFP"}],"key":"Q6UHokvHeX"}],"key":"EWR7fvItlY"}],"key":"hG4oPjETcW"},{"type":"heading","depth":3,"position":{"start":{"line":381,"column":1},"end":{"line":381,"column":1}},"children":[{"type":"text","value":"Test against the interface, not the implementation","position":{"start":{"line":381,"column":1},"end":{"line":381,"column":1}},"key":"hhh8PdRL9w"}],"identifier":"test-against-the-interface-not-the-implementation","label":"Test against the interface, not the implementation","html_id":"test-against-the-interface-not-the-implementation","implicit":true,"key":"aFUqkF0D3h"},{"type":"paragraph","position":{"start":{"line":383,"column":1},"end":{"line":384,"column":1}},"children":[{"type":"text","value":"A good test shouldn’t know about the internal implementation details of the function that it is testing, and changes in the internal code that do not modify the input-output relationship should not affect the test.\nThat is, from the standpoint of the test, a function should be a “black box”.","position":{"start":{"line":383,"column":1},"end":{"line":383,"column":1}},"key":"PVAllYaZ9R"}],"key":"JdiBEuMU6Q"},{"type":"paragraph","position":{"start":{"line":386,"column":1},"end":{"line":387,"column":1}},"children":[{"type":"text","value":"The most common way in which a test can violate this principle is by accessing the internal variables of a class that it is testing.\nFor example, we might generate a class that performs a scaling operation on a numpy matrix:","position":{"start":{"line":386,"column":1},"end":{"line":386,"column":1}},"key":"B8nIWwawzF"}],"key":"pcHB3kxp4X"},{"type":"code","lang":"python","value":"class SimpleScaler:\n    def __init__(self):\n        self.transformed_ = None\n\n    def fit(self, X):\n        self.mean_ = X.mean(axis=0)\n        self.std_ = X.std(axis=0)\n\n    def transform(self, X):\n        self.transformed_ = (X - self.mean_) / self.std_\n        return self.transformed_\n\n    def fit_transform(self, X):\n        self.fit(X)\n        return self.transform(X)","position":{"start":{"line":389,"column":1},"end":{"line":405,"column":1}},"key":"Zm95r0dco0"},{"type":"paragraph","position":{"start":{"line":407,"column":1},"end":{"line":407,"column":1}},"children":[{"type":"text","value":"We could write a test that checks the values returned by the ","position":{"start":{"line":407,"column":1},"end":{"line":407,"column":1}},"key":"lrhRnzOXQy"},{"type":"inlineCode","value":"fit_transform()","position":{"start":{"line":407,"column":1},"end":{"line":407,"column":1}},"key":"mELubfUUy7"},{"type":"text","value":" method, treating the the class as a black box:","position":{"start":{"line":407,"column":1},"end":{"line":407,"column":1}},"key":"Fwof0xaUKd"}],"key":"sMSnOdyQ5q"},{"type":"code","lang":"python","value":"def test_simple_scaler_interface():\n    X = np.array([[1, 2], [3, 4], [5, 6]])\n    scaler = SimpleScaler()\n    \n    # Test the interface without accessing internals\n    transformed_X = scaler.fit_transform(X)\n    assert np.allclose(transformed_X.mean(axis=0), np.array([0, 0]))\n    assert np.allclose(transformed_X.std(axis=0), np.array([1, 1]))","position":{"start":{"line":409,"column":1},"end":{"line":418,"column":1}},"key":"c4AgnYuEoh"},{"type":"paragraph","position":{"start":{"line":420,"column":1},"end":{"line":420,"column":1}},"children":[{"type":"text","value":"Alternatively one might use knowledge of the internals of the class to test the transformed value:","position":{"start":{"line":420,"column":1},"end":{"line":420,"column":1}},"key":"gbfvmRksUE"}],"key":"yaIOcx8XCc"},{"type":"code","lang":"python","value":"def test_simple_scaler_internals():\n\n    X = np.array([[1, 2], [3, 4], [5, 6]])\n    scaler = SimpleScaler()\n    _ = scaler.fit_transform(X)\n    \n    # Test that the transformed data is correct using the internal\n    assert np.allclose(scaler.transformed_.mean(axis=0), np.array([0, 0]))\n    assert np.allclose(scaler.transformed_.std(axis=0), np.array([1, 1]))\n","position":{"start":{"line":422,"column":1},"end":{"line":433,"column":1}},"key":"blpV9IRjGD"},{"type":"paragraph","position":{"start":{"line":435,"column":1},"end":{"line":437,"column":1}},"children":[{"type":"text","value":"Both of these tests pass against the class definition shown above.\nHowever, if we were to change the way that the transformation is performed (for example, we decide to use the ","position":{"start":{"line":435,"column":1},"end":{"line":435,"column":1}},"key":"I6Uc7bei24"},{"type":"inlineCode","value":"StandardScaler","position":{"start":{"line":435,"column":1},"end":{"line":435,"column":1}},"key":"EEsoLSbfLZ"},{"type":"text","value":" function from ","position":{"start":{"line":435,"column":1},"end":{"line":435,"column":1}},"key":"KC8GDMDO1z"},{"type":"inlineCode","value":"scikit-learn","position":{"start":{"line":435,"column":1},"end":{"line":435,"column":1}},"key":"DD5ismPVj0"},{"type":"text","value":" instead of writing our own), then the implementation-aware tests are likely to fail unless the sample internal variable names are used.\nIn general we should only interact with a function or class via its explicit interfaces.","position":{"start":{"line":435,"column":1},"end":{"line":435,"column":1}},"key":"x91IDCzgA3"}],"key":"nIt99F5cKf"},{"type":"heading","depth":3,"position":{"start":{"line":439,"column":1},"end":{"line":439,"column":1}},"children":[{"type":"text","value":"Tests should be independent","position":{"start":{"line":439,"column":1},"end":{"line":439,"column":1}},"key":"Zs9uM9AjHx"}],"identifier":"tests-should-be-independent","label":"Tests should be independent","html_id":"tests-should-be-independent","implicit":true,"key":"QZuXYwBFOe"},{"type":"paragraph","position":{"start":{"line":441,"column":1},"end":{"line":446,"column":1}},"children":[{"type":"text","value":"In scientific computing it’s common to compose many different operations into a workflow.\nIf we want to test the workflow, then the tests of later steps in the workflow must necessarily rely upon earlier steps.\nWe could in theory write a set of tests that operate on a shared object, but the tests would fail if executed in an incorrect order, even if the code was correct.\nSimilarly, a failure on an early test would cause cascading failures in later tests, even if their code was correct.\nThe use of ordered tests also prevents the parallel execution of tests, which may slow down testing for complex projects.\nFor these reasons, we should always aim to create tests that can be executed independently.","position":{"start":{"line":441,"column":1},"end":{"line":441,"column":1}},"key":"nLwBEkhXBM"}],"key":"T1Rqrwj6lM"},{"type":"paragraph","position":{"start":{"line":448,"column":1},"end":{"line":449,"column":1}},"children":[{"type":"text","value":"Here is an example where coupling between tests could cause failures.\nFirst we generate two functions that make changes in place to a data frame:","position":{"start":{"line":448,"column":1},"end":{"line":448,"column":1}},"key":"Wuh1jP53VJ"}],"key":"RqL5MUJqcV"},{"type":"code","lang":"python","value":"def split_names(df):\n    df['firstname'] = df['name'].apply(lambda x: x.split()[0])\n    df['lastname'] = df['name'].apply(lambda x: x.split()[1])\n\ndef get_initials(df):\n    df['initials'] = df['firstname'].str[0] + df['lastname'].str[0]\n","position":{"start":{"line":451,"column":1},"end":{"line":459,"column":1}},"key":"XgIxlVXYZi"},{"type":"paragraph","position":{"start":{"line":461,"column":1},"end":{"line":462,"column":1}},"children":[{"type":"text","value":"In this case, the ","position":{"start":{"line":461,"column":1},"end":{"line":461,"column":1}},"key":"N8mYNzXSpB"},{"type":"inlineCode","value":"get_initials()","position":{"start":{"line":461,"column":1},"end":{"line":461,"column":1}},"key":"I0SL9jqG33"},{"type":"text","value":" function relies upon the ","position":{"start":{"line":461,"column":1},"end":{"line":461,"column":1}},"key":"mJGQzmPksK"},{"type":"inlineCode","value":"split_names()","position":{"start":{"line":461,"column":1},"end":{"line":461,"column":1}},"key":"ErXGOzXTFO"},{"type":"text","value":" function having been run, since otherwise the necessary columns won’t exist in the data frame.\nWe can then create tests for each of these, and a data frame that they can both use:","position":{"start":{"line":461,"column":1},"end":{"line":461,"column":1}},"key":"OixFj9iaP2"}],"key":"JQPtcDQVJY"},{"type":"code","lang":"python","value":"people_df = pd.DataFrame({'name': ['Alice Smith', 'Bob Howard', 'Charlie Ashe']}) \n\ndef test_split_names():\n    split_names(people_df)\n    assert people_df['firstname'].tolist() == ['Alice', 'Bob', 'Charlie']\n    assert people_df['lastname'].tolist() == ['Smith', 'Howard', 'Ashe']\n\ndef test_get_initials():\n    get_initials(people_df)\n    assert people_df['initials'].tolist() == ['AS', 'BH', 'CA']","position":{"start":{"line":464,"column":1},"end":{"line":475,"column":1}},"key":"JKCqJhNwYW"},{"type":"paragraph","position":{"start":{"line":477,"column":1},"end":{"line":477,"column":1}},"children":[{"type":"text","value":"These tests run correctly, but the same tests fail if we change their order such that ","position":{"start":{"line":477,"column":1},"end":{"line":477,"column":1}},"key":"ITLh0S2A2Q"},{"type":"inlineCode","value":"test_get_intials()","position":{"start":{"line":477,"column":1},"end":{"line":477,"column":1}},"key":"c6zpe1Ct1I"},{"type":"text","value":" runs first, because the necessary columns (","position":{"start":{"line":477,"column":1},"end":{"line":477,"column":1}},"key":"FlArB87HpB"},{"type":"inlineCode","value":"firstname","position":{"start":{"line":477,"column":1},"end":{"line":477,"column":1}},"key":"HrrwvSWR7u"},{"type":"text","value":" and ","position":{"start":{"line":477,"column":1},"end":{"line":477,"column":1}},"key":"OI1wJaP0zM"},{"type":"inlineCode","value":"lastname","position":{"start":{"line":477,"column":1},"end":{"line":477,"column":1}},"key":"nzgHB68cF1"},{"type":"text","value":") have not yet been created.","position":{"start":{"line":477,"column":1},"end":{"line":477,"column":1}},"key":"JBrzEfBHtr"}],"key":"IfQcXExbtN"},{"type":"paragraph","position":{"start":{"line":479,"column":1},"end":{"line":479,"column":1}},"children":[{"type":"text","value":"One simple way to deal with this is to set up all of the necessary structure locally within each test:","position":{"start":{"line":479,"column":1},"end":{"line":479,"column":1}},"key":"bRwVSNyzP2"}],"key":"BGa7qXywFd"},{"type":"code","lang":"python","value":"\ndef get_people_df():\n    return pd.DataFrame({'name': ['Alice Smith', 'Bob Howard', 'Charlie Ashe']}) \n\ndef test_split_names_fullsetup():\n    local_people_df = get_people_df()\n    split_names(local_people_df)\n    assert local_people_df['firstname'].tolist() == ['Alice', 'Bob', 'Charlie']\n    assert local_people_df['lastname'].tolist() == ['Smith', 'Howard', 'Ashe']\n\ndef test_get_initials_fullsetup():\n    local_people_df = get_people_df()\n    split_names(local_people_df)\n    get_initials(local_people_df)\n    assert local_people_df['initials'].tolist() == ['AS', 'BH', 'CA']","position":{"start":{"line":481,"column":1},"end":{"line":497,"column":1}},"key":"ddUnVjcC3w"},{"type":"paragraph","position":{"start":{"line":499,"column":1},"end":{"line":500,"column":1}},"children":[{"type":"text","value":"For simple functions like these this would not cause too much computational overhead, but for computationally intensive functions we would like to be able to reuse the results from the first time each function is run.\nIn a later section we will discuss the use of ","position":{"start":{"line":499,"column":1},"end":{"line":499,"column":1}},"key":"jflKUy4HvN"},{"type":"emphasis","position":{"start":{"line":499,"column":1},"end":{"line":499,"column":1}},"children":[{"type":"text","value":"fixtures","position":{"start":{"line":499,"column":1},"end":{"line":499,"column":1}},"key":"k5OJH92hby"}],"key":"F5mNaZ0rRd"},{"type":"text","value":" which allow this kind of reuse across tests while avoiding the ordering problems that we saw above when using a global variable across tests.","position":{"start":{"line":499,"column":1},"end":{"line":499,"column":1}},"key":"hMBBy24w44"}],"key":"ygpDLlyoFb"},{"type":"heading","depth":2,"position":{"start":{"line":502,"column":1},"end":{"line":502,"column":1}},"children":[{"type":"text","value":"Testing frameworks","position":{"start":{"line":502,"column":1},"end":{"line":502,"column":1}},"key":"D74qxGmTKF"}],"identifier":"testing-frameworks","label":"Testing frameworks","html_id":"testing-frameworks","implicit":true,"key":"WigEBOyZ8j"},{"type":"paragraph","position":{"start":{"line":504,"column":1},"end":{"line":505,"column":1}},"children":[{"type":"text","value":"One could write tests without the help of any specialized packages, but we generally use a testing framework to help automate the activities of testing.\nThere are several testing frameworks for Python; we will focus on the popular and flexible ","position":{"start":{"line":504,"column":1},"end":{"line":504,"column":1}},"key":"DSs8eOR98R"},{"type":"inlineCode","value":"pytest","position":{"start":{"line":504,"column":1},"end":{"line":504,"column":1}},"key":"OVcTzzufZo"},{"type":"text","value":" framework.","position":{"start":{"line":504,"column":1},"end":{"line":504,"column":1}},"key":"Jrvcft0ls6"}],"key":"OGX2iJKEc1"},{"type":"paragraph","position":{"start":{"line":507,"column":1},"end":{"line":508,"column":1}},"children":[{"type":"text","value":"We will start with a very simple example: a function that generates the Euclidean distance between two points.\nCopilot generates the following for us based on the prompt in the comment:","position":{"start":{"line":507,"column":1},"end":{"line":507,"column":1}},"key":"exUIeHvkai"}],"key":"WppGzpNgIT"},{"type":"code","lang":"python","value":"# generate a function that calculates the distance between two points\n# where each point is defined as a tuple of two numbers\n\nimport math\n\ndef distance(p1, p2):\n    \"\"\"Calculate the distance between two points\"\"\"\n    x1, y1 = p1\n    x2, y2 = p2\n    return math.sqrt((x2 - x1)**2 + (y2 - y1)**2)","position":{"start":{"line":510,"column":1},"end":{"line":521,"column":1}},"key":"mPioPWsOIV"},{"type":"paragraph","position":{"start":{"line":523,"column":1},"end":{"line":524,"column":1}},"children":[{"type":"text","value":"Now we would like to generate some tests for this code to make sure that it works properly.\nIf we ask Copilot to generate some tests, it does a seeming decent job:","position":{"start":{"line":523,"column":1},"end":{"line":523,"column":1}},"key":"YgZkOv7LVC"}],"key":"ooLTFfZ4ko"},{"type":"code","lang":"python","value":"def test_distance_zero():\n    assert distance((0, 0), (0, 0)) == 0\n\ndef test_distance_positive_coordinates():\n    assert distance((1, 2), (4, 6)) == 5\n\ndef test_distance_negative_coordinates():\n    assert distance((-1, -2), (-4, -6)) == 5\n\ndef test_distance_mixed_coordinates():\n    assert distance((1, -2), (-4, 6)) == math.sqrt(125)\n\ndef test_distance_same_x():\n    assert distance((3, 4), (3, 8)) == 4\n\ndef test_distance_same_y():\n    assert distance((3, 4), (7, 4)) == 4","position":{"start":{"line":526,"column":1},"end":{"line":544,"column":1}},"key":"LakC0GHdtI"},{"type":"paragraph","position":{"start":{"line":546,"column":1},"end":{"line":546,"column":1}},"children":[{"type":"text","value":"Now that we have our tests, we can run them using the ","position":{"start":{"line":546,"column":1},"end":{"line":546,"column":1}},"key":"YYjq8SdBH7"},{"type":"inlineCode","value":"pytest","position":{"start":{"line":546,"column":1},"end":{"line":546,"column":1}},"key":"c68eZe95If"},{"type":"text","value":" command:","position":{"start":{"line":546,"column":1},"end":{"line":546,"column":1}},"key":"x8Hof6ynYL"}],"key":"zo2dvsSto2"},{"type":"code","lang":"bash","value":"pytest src/BetterCodeBetterScience/distance_testing","position":{"start":{"line":548,"column":1},"end":{"line":550,"column":1}},"key":"JLDifQN3Gg"},{"type":"paragraph","position":{"start":{"line":552,"column":1},"end":{"line":553,"column":1}},"children":[{"type":"text","value":"This command will cause pytest to search (by default) for any files named ","position":{"start":{"line":552,"column":1},"end":{"line":552,"column":1}},"key":"tueCLXV1Dp"},{"type":"inlineCode","value":"test_*.py","position":{"start":{"line":552,"column":1},"end":{"line":552,"column":1}},"key":"Rku6pUI9fu"},{"type":"text","value":" or ","position":{"start":{"line":552,"column":1},"end":{"line":552,"column":1}},"key":"HGo0Rts9Ko"},{"type":"inlineCode","value":"*_test.py","position":{"start":{"line":552,"column":1},"end":{"line":552,"column":1}},"key":"jVCKZSdKHL"},{"type":"text","value":" in the relevant path, and the select any functions whose name starts with the prefix “test”.\nRunning those tests, we get an error:","position":{"start":{"line":552,"column":1},"end":{"line":552,"column":1}},"key":"apFZ6cFFXx"}],"key":"wHHhfSy56n"},{"type":"code","lang":"bash","value":">       assert distance((1, -2), (-4, 6)) == math.sqrt(125)\nE       assert 9.433981132056603 == 11.180339887498949\nE        +  where 9.433981132056603 = distance((1, -2), (-4, 6))\nE        +  and   11.180339887498949 = <built-in function sqrt>(125)\nE        +    where <built-in function sqrt> = math.sqrt\n","position":{"start":{"line":556,"column":1},"end":{"line":563,"column":1}},"key":"CUoFUE5Y3l"},{"type":"paragraph","position":{"start":{"line":565,"column":1},"end":{"line":567,"column":1}},"children":[{"type":"text","value":"Here we see that the value returned by our function is different from the one expected by the test; in this case, the test value generated by Copilot is incorrect.\nIn our research, it was not uncommon for ChatGPT to generate incorrect test values, so these must always be checked by a domain expert.\nOnce we fix the expected value for that test (the square root of 89), then we can rerun the tests and see that they have passed:","position":{"start":{"line":565,"column":1},"end":{"line":565,"column":1}},"key":"SZsAAz179o"}],"key":"bFmaCSHGNi"},{"type":"code","lang":"bash","value":"python -m pytest pytest src/BetterCodeBetterScience/distance_testing\n==================== test session starts =====================                                     \n\nsrc/codingforscience/simple_testing/test_distance.py . [ 16%]\n.....                                                  [100%]\n\n===================== 6 passed in 0.06s ======================\n","position":{"start":{"line":569,"column":1},"end":{"line":578,"column":1}},"key":"SvHrtV2Tlf"},{"type":"heading","depth":3,"position":{"start":{"line":580,"column":1},"end":{"line":580,"column":1}},"children":[{"type":"text","value":"Potential problems with AI-generated tests","position":{"start":{"line":580,"column":1},"end":{"line":580,"column":1}},"key":"C7dcLuwzWk"}],"identifier":"potential-problems-with-ai-generated-tests","label":"Potential problems with AI-generated tests","html_id":"potential-problems-with-ai-generated-tests","implicit":true,"key":"PccY8tR1rR"},{"type":"paragraph","position":{"start":{"line":582,"column":1},"end":{"line":583,"column":1}},"children":[{"type":"text","value":"If we are going to rely upon AI tools to generate our tests, we need to be sure that the tests are correct.\nOne of my early forays into AI-driven test generation uncovered an interesting example of how this can go wrong.","position":{"start":{"line":582,"column":1},"end":{"line":582,"column":1}},"key":"h8NroiSjla"}],"key":"F8sTcJcvj6"},{"type":"paragraph","position":{"start":{"line":585,"column":1},"end":{"line":586,"column":1}},"children":[{"type":"text","value":"In our early project that examined the performance of GPT-4 for coding ","position":{"start":{"line":585,"column":1},"end":{"line":585,"column":1}},"key":"wk1iul9Z4J"},{"type":"citeGroup","kind":"parenthetical","position":{"start":{"line":585,"column":1},"end":{"line":585,"column":1}},"children":[{"type":"cite","identifier":"poldrack:2023aa","label":"Poldrack:2023aa","kind":"parenthetical","position":{"start":{"line":585,"column":73},"end":{"line":585,"column":89}},"children":[{"type":"text","value":"Poldrack ","key":"jMaqWtf5mL"},{"type":"emphasis","children":[{"type":"text","value":"et al.","key":"o5KTUVx7Ej"}],"key":"zQxjlsxBoc"},{"type":"text","value":", 2023","key":"Lk2eoWblnp"}],"enumerator":"1","key":"IgRA2s2RGU"}],"key":"oyVLa4tNRP"},{"type":"text","value":", one of the analyses that we performed first asked GPT-4 to do was to generate a set of functions related to common problems in several scientific domains, and then to generate tests to make sure that the function performed correctly.\nOne of the functions that was generated was the escape velocity function shown above, for which GPT-4 generated the ","position":{"start":{"line":585,"column":1},"end":{"line":585,"column":1}},"key":"glXTD0RAwA"},{"type":"link","url":"https://github.com/poldrack/ai-coding-experiments/blob/main/data/conceptual_prompting/testdirs/conceptual_prompting06/test_answer.py","position":{"start":{"line":585,"column":1},"end":{"line":585,"column":1}},"children":[{"type":"text","value":"following test","position":{"start":{"line":585,"column":1},"end":{"line":585,"column":1}},"key":"E0xEZM0mry"}],"urlSource":"https://github.com/poldrack/ai-coding-experiments/blob/main/data/conceptual_prompting/testdirs/conceptual_prompting06/test_answer.py","data":{"kind":"file","org":"poldrack","repo":"ai-coding-experiments","reference":"main","file":"data/conceptual_prompting/testdirs/conceptual_prompting06/test_answer.py","raw":"https://raw.githubusercontent.com/poldrack/ai-coding-experiments/main/data/conceptual_prompting/testdirs/conceptual_prompting06/test_answer.py"},"internal":false,"protocol":"github","key":"sKFoBmDuaU"},{"type":"text","value":":","position":{"start":{"line":585,"column":1},"end":{"line":585,"column":1}},"key":"LPG5gfxMoA"}],"key":"RVGOrjABsa"},{"type":"code","lang":"python","value":"def test_escape_velocity():\n\n    mass_earth = 5.972e24\n    radius_earth = 6.371e6\n    result = escape_velocity(mass_earth, radius_earth)\n    assert pytest.approx(result, rel=1e-3) == 11186.25\n\n    mass_mars = 6.4171e23\n    radius_mars = 3.3895e6\n    result = escape_velocity(mass_mars, radius_mars)\n    assert pytest.approx(result, rel=1e-3) == 5027.34\n\n    mass_jupiter = 1.8982e27\n    radius_jupiter = 6.9911e7\n    result = escape_velocity(mass_jupiter, radius_jupiter)\n    assert pytest.approx(result, rel=1e-3) == 59564.97","position":{"start":{"line":589,"column":1},"end":{"line":606,"column":1}},"key":"thRSrqKS7z"},{"type":"paragraph","position":{"start":{"line":608,"column":1},"end":{"line":608,"column":1}},"children":[{"type":"text","value":"When we run this test (renaming it ","position":{"start":{"line":608,"column":1},"end":{"line":608,"column":1}},"key":"ecQgTKmHxr"},{"type":"inlineCode","value":"test_escape_velocity_gpt4","position":{"start":{"line":608,"column":1},"end":{"line":608,"column":1}},"key":"mblvE7QS95"},{"type":"text","value":"), we see that one of the tests fails:","position":{"start":{"line":608,"column":1},"end":{"line":608,"column":1}},"key":"nv8uOJBVwm"}],"key":"vFNfKbHXhQ"},{"type":"code","lang":"bash","value":"❯ pytest src/BetterCodeBetterScience/escape_velocity.py::test_escape_velocity_gpt4\n==================================== test session starts ====================================\nplatform darwin -- Python 3.12.0, pytest-8.4.1, pluggy-1.5.0\nrootdir: /Users/poldrack/Dropbox/code/BetterCodeBetterScience\nconfigfile: pyproject.toml\nplugins: cov-5.0.0, anyio-4.6.0, hypothesis-6.115.3, mock-3.14.0\ncollected 1 item\n\nsrc/BetterCodeBetterScience/escape_velocity.py F                                      [100%]\n\n========================================= FAILURES ==========================================\n_________________________________ test_escape_velocity_gpt4 _________________________________\n\n    def test_escape_velocity_gpt4():\n\n        mass_earth = 5.972e24\n        radius_earth = 6.371e6\n        result = escape_velocity(mass_earth, radius_earth)\n        assert pytest.approx(result, rel=1e-3) == 11186.25\n\n        mass_mars = 6.4171e23\n        radius_mars = 3.3895e6\n        result = escape_velocity(mass_mars, radius_mars)\n        assert pytest.approx(result, rel=1e-3) == 5027.34\n\n        mass_jupiter = 1.8982e27\n        radius_jupiter = 6.9911e7\n        result = escape_velocity(mass_jupiter, radius_jupiter)\n>       assert pytest.approx(result, rel=1e-3) == 59564.97\nE       assert 60202.716344497014 ± 60.2027 == 59564.97\nE\nE         comparison failed\nE         Obtained: 59564.97\nE         Expected: 60202.716344497014 ± 60.2027\n\nsrc/BetterCodeBetterScience/escape_velocity.py:52: AssertionError\n================================== short test summary info ==================================\nFAILED src/BetterCodeBetterScience/escape_velocity.py::test_escape_velocity_gpt4 - assert 60202.716344497014 ± 60.2027 == 59564.97\n===================================== 1 failed in 0.12s =====================================","position":{"start":{"line":610,"column":1},"end":{"line":650,"column":1}},"key":"Dcie3F44ve"},{"type":"paragraph","position":{"start":{"line":652,"column":1},"end":{"line":659,"column":1}},"children":[{"type":"text","value":"It seems that the first two assertions pass but the third one, for Jupiter, fails.\nThis failure took a bit of digging to fully understand.\nIn this case, the code and test value are both correct, depending on where you stand on Jupiter! The problem is that planets are ","position":{"start":{"line":652,"column":1},"end":{"line":652,"column":1}},"key":"mhY68Fvrje"},{"type":"emphasis","position":{"start":{"line":652,"column":1},"end":{"line":652,"column":1}},"children":[{"type":"text","value":"oblate","position":{"start":{"line":652,"column":1},"end":{"line":652,"column":1}},"key":"IydH84McUh"}],"key":"VXVMu3BXA3"},{"type":"text","value":", meaning that they are slightly flattened such that the radius around the equator is higher than at other points.\nNASA’s ","position":{"start":{"line":652,"column":1},"end":{"line":652,"column":1}},"key":"y2OLYAN5Ke"},{"type":"link","url":"https://nssdc.gsfc.nasa.gov/planetary/factsheet/jupiterfact.html","position":{"start":{"line":652,"column":1},"end":{"line":652,"column":1}},"children":[{"type":"text","value":"Jupiter fact sheet","position":{"start":{"line":652,"column":1},"end":{"line":652,"column":1}},"key":"ZLXc9Zc9E6"}],"urlSource":"https://nssdc.gsfc.nasa.gov/planetary/factsheet/jupiterfact.html","key":"aDOwN0N8zo"},{"type":"text","value":" claims an escape velocity of 59.5 km/s, which seems to be the source of the test value.\nThis is correct when computed using the equatorial radius of 71492 km.\nHowever, the radius given for Jupiter in GPT-4’s test (69911 km) is the volumetric mean radius rather than the equatorial radius, and the value generated by the code (60.2 km/s) is correct when computed using the volumetric mean radius.\nThus, the test failed not due to any problems with the code itself, but due to a mismatch in assumptions regarding the combination of test values.\nThis example highlights the importance of understanding and checking the tests that are generated by AI coding tools.","position":{"start":{"line":652,"column":1},"end":{"line":652,"column":1}},"key":"cm3vzgSgIP"}],"key":"U5RCrrS6KQ"},{"type":"heading","depth":2,"position":{"start":{"line":661,"column":1},"end":{"line":661,"column":1}},"children":[{"type":"text","value":"Test-driven development and AI-assisted coding","position":{"start":{"line":661,"column":1},"end":{"line":661,"column":1}},"key":"X5zmi8Isej"}],"identifier":"test-driven-development-and-ai-assisted-coding","label":"Test-driven development and AI-assisted coding","html_id":"test-driven-development-and-ai-assisted-coding","implicit":true,"key":"QKJpF01CMx"},{"type":"paragraph","position":{"start":{"line":663,"column":1},"end":{"line":665,"column":1}},"children":[{"type":"text","value":"Here we will dive into a more realistic example of an application that one might develop using AI assistance, specifically looking at how we could develop the application using a test-driven development (TDD) approach.\nWe will develop a Python application that takes in a query for the PubMed database and returns a data frame containing the number of database records matching that query for each year.\nWe start by decomposing the problem and sketching out the main set of functions that we will need to develop, with understandable names for each:","position":{"start":{"line":663,"column":1},"end":{"line":663,"column":1}},"key":"RbE8LtHfNI"}],"key":"W85zOlpv9n"},{"type":"list","ordered":false,"spread":false,"position":{"start":{"line":667,"column":1},"end":{"line":672,"column":1}},"children":[{"type":"listItem","spread":true,"position":{"start":{"line":667,"column":1},"end":{"line":667,"column":1}},"children":[{"type":"paragraph","children":[{"type":"inlineCode","value":"get_PubmedIDs_for_query","position":{"start":{"line":667,"column":1},"end":{"line":667,"column":1}},"key":"v7HzaUHcYo"},{"type":"text","value":": A function that will search pubmed for a given query and return a list of pubmed IDs","position":{"start":{"line":667,"column":1},"end":{"line":667,"column":1}},"key":"PkcyNIYVNl"}],"key":"qvNkEuY4oK"}],"key":"ZoZdTjcmVz"},{"type":"listItem","spread":true,"position":{"start":{"line":668,"column":1},"end":{"line":668,"column":1}},"children":[{"type":"paragraph","children":[{"type":"inlineCode","value":"get_record_from_PubmedID","position":{"start":{"line":668,"column":1},"end":{"line":668,"column":1}},"key":"kaG7WWcWnI"},{"type":"text","value":": A function that will retrieve the record for a given pubmed ID","position":{"start":{"line":668,"column":1},"end":{"line":668,"column":1}},"key":"Z5SHw1lNJh"}],"key":"WRv7xBrutQ"}],"key":"kwgtpVmo7V"},{"type":"listItem","spread":true,"position":{"start":{"line":669,"column":1},"end":{"line":669,"column":1}},"children":[{"type":"paragraph","children":[{"type":"inlineCode","value":"parse_year_from_Pubmed_record","position":{"start":{"line":669,"column":1},"end":{"line":669,"column":1}},"key":"KAmuPzLKzq"},{"type":"text","value":": A function that will parse a record to extract the year of publication","position":{"start":{"line":669,"column":1},"end":{"line":669,"column":1}},"key":"gpX6jhFeaL"}],"key":"nWrfiKJUNy"}],"key":"rIn4C3FNX7"},{"type":"listItem","spread":true,"position":{"start":{"line":670,"column":1},"end":{"line":670,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"A function that will summarize the number of records per year","position":{"start":{"line":670,"column":1},"end":{"line":670,"column":1}},"key":"wrcdINUxKS"}],"key":"yKu3wUiJ9b"}],"key":"FOpM51fEYc"},{"type":"listItem","spread":true,"position":{"start":{"line":671,"column":1},"end":{"line":672,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"The main function that will take in a query and return a data frame with the number of records per year for the query","position":{"start":{"line":671,"column":1},"end":{"line":671,"column":1}},"key":"QAPrgqHzOt"}],"key":"prcUDpS7CJ"}],"key":"paA5BTdDZn"}],"key":"QvQLhWFQfk"},{"type":"paragraph","position":{"start":{"line":673,"column":1},"end":{"line":676,"column":1}},"children":[{"type":"text","value":"We start by creating ","position":{"start":{"line":673,"column":1},"end":{"line":673,"column":1}},"key":"MgyND1bvrS"},{"type":"inlineCode","value":"get_PubmedIDs_for_query","position":{"start":{"line":673,"column":1},"end":{"line":673,"column":1}},"key":"R06XdxLe2d"},{"type":"text","value":".\nWe could use the ","position":{"start":{"line":673,"column":1},"end":{"line":673,"column":1}},"key":"eBnFqe4pxK"},{"type":"inlineCode","value":"Biopython.Entrez","position":{"start":{"line":673,"column":1},"end":{"line":673,"column":1}},"key":"iU6b1Aq3sn"},{"type":"text","value":" module to perform this search, but Biopython is a relatively large module that could introduce technical debt.\nInstead, we will directly retrieve the result using the Entrez API and the built-in ","position":{"start":{"line":673,"column":1},"end":{"line":673,"column":1}},"key":"v81vU8OxV1"},{"type":"inlineCode","value":"requests","position":{"start":{"line":673,"column":1},"end":{"line":673,"column":1}},"key":"cbG1fUlWat"},{"type":"text","value":" module.\nNote that for all of the code shown here we will not include docstrings, but they are available in the code within the repository.","position":{"start":{"line":673,"column":1},"end":{"line":673,"column":1}},"key":"MWzb7ov3Bt"}],"key":"wbXfD4wTn9"},{"type":"paragraph","position":{"start":{"line":678,"column":1},"end":{"line":680,"column":1}},"children":[{"type":"text","value":"If we are using the TDD approach, we would first want to develop a set of tests to make sure that our function is working correctly.\nThe following three tests specify several different outcomes that we might expect.\nFirst, we give a query that is known to give a valid result, and test whether it in fact gives such a result:","position":{"start":{"line":678,"column":1},"end":{"line":678,"column":1}},"key":"tmBwoWpdN1"}],"key":"YOzIfJOthn"},{"type":"code","lang":"python","value":"def test_get_PubmedIDs_for_query_check_valid():\n    query = \"friston-k AND 'free energy'\"\n    ids = get_PubmedIDs_for_query(query)\n\n    # make sure that a list is returned\n    assert isinstance(ids, list)       \n    # make sure the list is not empty\n    assert len(ids) > 0                 ","position":{"start":{"line":682,"column":1},"end":{"line":691,"column":1}},"key":"U8raCrARUn"},{"type":"paragraph","position":{"start":{"line":693,"column":1},"end":{"line":693,"column":1}},"children":[{"type":"text","value":"Second, we give a query with a known empty result, and make sure it returns an empty list:","position":{"start":{"line":693,"column":1},"end":{"line":693,"column":1}},"key":"slR4hWGYWw"}],"key":"xRDIvDYRqj"},{"type":"code","lang":"python","value":"def test_get_PubmedIDs_for_query_check_empty():\n    query = \"friston-k AND 'fizzbuzz'\"\n    ids = get_PubmedIDs_for_query(query)\n\n    # make sure that a list is returned\n    assert isinstance(ids, list)   \n    # make sure the resulting list is empty\n    assert len(ids) == 0","position":{"start":{"line":695,"column":1},"end":{"line":704,"column":1}},"key":"YIQQjkqjIC"},{"type":"paragraph","position":{"start":{"line":707,"column":1},"end":{"line":708,"column":1}},"children":[{"type":"text","value":"With the minimal tests in place, we then move to writing the code for the module.\nWe first create an empty function to ensure that the tests fail:","position":{"start":{"line":707,"column":1},"end":{"line":707,"column":1}},"key":"q95wa6ymtl"}],"key":"E6CMDFRvJM"},{"type":"code","lang":"python","value":"def get_PubmedIDs_for_query(query: str, \n                            retmax: int = None,\n                            esearch_url: str = None) -> list:\n    return None","position":{"start":{"line":710,"column":1},"end":{"line":715,"column":1}},"key":"x8AOBEBrWS"},{"type":"paragraph","position":{"start":{"line":717,"column":1},"end":{"line":717,"column":1}},"children":[{"type":"text","value":"The test result shows that all of the tests fail:","position":{"start":{"line":717,"column":1},"end":{"line":717,"column":1}},"key":"WPzGfl18qS"}],"key":"ZwzKsU2PMA"},{"type":"code","lang":"bash","value":"❯ python -m pytest -v tests/textmining\n================================== test session starts ===================================\n...\ntests/textmining/test_textmining.py::test_get_PubmedIDs_for_query_check_valid FAILED [ 50%]\ntests/textmining/test_textmining.py::test_get_PubmedIDs_for_query_check_empty FAILED [100%]\n\n======================================== FAILURES ========================================\n________________________ test_get_PubmedIDs_for_query_check_valid ________________________\n\nids = None\n\n    def test_get_PubmedIDs_for_query_check_valid(ids):\n>       assert isinstance(ids, list)\nE       assert False\nE        +  where False = isinstance(None, list)\n\ntests/textmining/test_textmining.py:32: AssertionError\n________________________ test_get_PubmedIDs_for_query_check_empty ________________________\n\n    def test_get_PubmedIDs_for_query_check_empty():\n        query = \"friston-k AND 'fizzbuzz'\"\n        ids = get_PubmedIDs_for_query(query)\n>       assert len(ids) == 0\n               ^^^^^^^^\nE       TypeError: object of type 'NoneType' has no len()\n\ntests/textmining/test_textmining.py:39: TypeError\n================================ short test summary info =================================\nFAILED tests/textmining/test_textmining.py::test_get_PubmedIDs_for_query_check_valid - assert False\nFAILED tests/textmining/test_textmining.py::test_get_PubmedIDs_for_query_check_empty - TypeError: object of type 'NoneType' has no len()\n=================================== 2 failed in 0.12s ====================================","position":{"start":{"line":719,"column":1},"end":{"line":751,"column":1}},"key":"t5eYvo88xF"},{"type":"paragraph","position":{"start":{"line":753,"column":1},"end":{"line":753,"column":1}},"children":[{"type":"text","value":"Now we work with Copilot write the code to make the tests pass:","position":{"start":{"line":753,"column":1},"end":{"line":753,"column":1}},"key":"b42qHabP4A"}],"key":"MeSTq2YnSO"},{"type":"code","lang":"python","value":"# define the eutils base URL globally for the module\n# - not best practice but probably ok here\nBASE_URL = \"https://eutils.ncbi.nlm.nih.gov/entrez/eutils\"\n\n\ndef get_PubmedIDs_for_query(\n    query: str, retmax: int = None, esearch_url: str = None\n) -> list:\n    \"\"\"\n    Search database for a given query and return a list of IDs.\n    :param query: str, the query to search for\n    :param retmax: int, the maximum number of results to return\n    :base_url: str, the base url for the pubmed search\n    :return: list, a list of pubmed IDs\n    \"\"\"\n    # define the base url for the pubmed search\n    if esearch_url is None:\n        esearch_url = f\"{BASE_URL}/esearch.fcgi\"\n\n    params = format_pubmed_query_params(query, retmax=retmax)\n\n    response = requests.get(esearch_url, params=params)\n\n    return get_idlist_from_response(response)\n\n\ndef format_pubmed_query_params(query: str, retmax: int = 10000) -> str:\n    \"\"\"\n    Format a query for use with the pubmed api.\n    :param query: str, the query to format\n    :return: dict, the formatted query dict\n    \"\"\"\n\n    # define the parameters for the search\n    return {\"db\": \"pubmed\", \"term\": query, \"retmode\": \"json\", \"retmax\": retmax}\n\n\ndef get_idlist_from_response(response: requests.Response) -> list:\n    if response.status_code == 200:\n        # extract the pubmed IDs from the response\n        ids = response.json()[\"esearchresult\"][\"idlist\"]\n        return ids\n    else:\n        raise ValueError(\"Bad request\")","position":{"start":{"line":755,"column":1},"end":{"line":800,"column":1}},"key":"LoI8evNLcr"},{"type":"paragraph","position":{"start":{"line":802,"column":1},"end":{"line":805,"column":1}},"children":[{"type":"text","value":"Note that we have split parts of the functionality into separate functions in order to make the code more understandable.\nRunning the tests, we see that both of them pass.\nAssuming that our tests cover all possible outcomes of interest, we can consider our function complete.\nWe can also add additional tests to cover additional functions that we generated; we won’t go into the details here, but you can see them on the Github repo.","position":{"start":{"line":802,"column":1},"end":{"line":802,"column":1}},"key":"Mol3KwoE3i"}],"key":"VMwLnyIvmo"},{"type":"heading","depth":2,"position":{"start":{"line":808,"column":1},"end":{"line":808,"column":1}},"children":[{"type":"text","value":"Test coverage","position":{"start":{"line":808,"column":1},"end":{"line":808,"column":1}},"key":"alnsVnWz0c"}],"identifier":"test-coverage","label":"Test coverage","html_id":"test-coverage","implicit":true,"key":"eapVZYn2Y4"},{"type":"paragraph","position":{"start":{"line":810,"column":1},"end":{"line":811,"column":1}},"children":[{"type":"text","value":"It can be useful to know if there are any portions of our code that are not being exercised by our tests, which is known as ","position":{"start":{"line":810,"column":1},"end":{"line":810,"column":1}},"key":"RUCygja9Cw"},{"type":"emphasis","position":{"start":{"line":810,"column":1},"end":{"line":810,"column":1}},"children":[{"type":"text","value":"code coverage","position":{"start":{"line":810,"column":1},"end":{"line":810,"column":1}},"key":"rs7FVYlsno"}],"key":"HdBpSmtihS"},{"type":"text","value":".\nThe ","position":{"start":{"line":810,"column":1},"end":{"line":810,"column":1}},"key":"bmKMF0bZFe"},{"type":"inlineCode","value":"pytest-cov","position":{"start":{"line":810,"column":1},"end":{"line":810,"column":1}},"key":"IFu8lzqGlp"},{"type":"text","value":" extension for the ","position":{"start":{"line":810,"column":1},"end":{"line":810,"column":1}},"key":"JWO5FMz6ZX"},{"type":"inlineCode","value":"pytest","position":{"start":{"line":810,"column":1},"end":{"line":810,"column":1}},"key":"aM5RKbI4aR"},{"type":"text","value":" testing package can provide us with a report of test coverage for these tests:","position":{"start":{"line":810,"column":1},"end":{"line":810,"column":1}},"key":"zrYtN1uK3g"}],"key":"vbra31Umar"},{"type":"code","lang":"bash","value":"---------- coverage: platform darwin, python 3.12.0-final-0 ----------\nName                                                   Stmts   Miss  Cover   Missing\n------------------------------------------------------------------------------------\nsrc/BetterCodeBetterScience/textmining/textmining.py      30      1    97%   70\n------------------------------------------------------------------------------------\nTOTAL                                                     30      1    97%","position":{"start":{"line":813,"column":1},"end":{"line":820,"column":1}},"key":"GpciPPKMRE"},{"type":"paragraph","position":{"start":{"line":822,"column":1},"end":{"line":823,"column":1}},"children":[{"type":"text","value":"This report shows that of the 30 statements in our code, one of them is not covered by the tests.\nWhen we look at the missing code (denoted as being on line 70), we see that the missing line is this one from ","position":{"start":{"line":822,"column":1},"end":{"line":822,"column":1}},"key":"npwkx6eK4N"},{"type":"inlineCode","value":"get_idlist_from_response","position":{"start":{"line":822,"column":1},"end":{"line":822,"column":1}},"key":"aSSnmbJbKU"},{"type":"text","value":":","position":{"start":{"line":822,"column":1},"end":{"line":822,"column":1}},"key":"Rnjw4Iw02z"}],"key":"v8u4Nuj7at"},{"type":"code","lang":"python","value":"    else:\n        # raise an exception if the search didn't return a usable response\n        raise ValueError(\"Bad request\")","position":{"start":{"line":825,"column":1},"end":{"line":829,"column":1}},"key":"PnJ7GUlqls"},{"type":"paragraph","position":{"start":{"line":831,"column":1},"end":{"line":833,"column":1}},"children":[{"type":"text","value":"Since none of our test cases caused a bad request to occur, this line never gets executed in the tests.\nWe can address this by adding a test that makes sure that an exception is raised if an invalid base url is provided.\nTo check for an exception, we need to use the ","position":{"start":{"line":831,"column":1},"end":{"line":831,"column":1}},"key":"sDv1HxdUKp"},{"type":"inlineCode","value":"pytest.raises","position":{"start":{"line":831,"column":1},"end":{"line":831,"column":1}},"key":"BAlTCa96So"},{"type":"text","value":" context manager:","position":{"start":{"line":831,"column":1},"end":{"line":831,"column":1}},"key":"Owq9utOMAh"}],"key":"E0SvXImwaq"},{"type":"code","lang":"python","value":"def test_get_PubmedIDs_for_query_check_badurl():\n    query = \"friston-k AND 'free energy'\"\n    # bad url\n    base_url = 'https://eutils.ncbi.nlm.nih.gov/entrez/eutils/esearch.f'\n    \n    # make sure that the function raises an exception\n    with pytest.raises(Exception):\n        ids = get_PubmedIDs_for_query(query, base_url=base_url)\n    ","position":{"start":{"line":835,"column":1},"end":{"line":845,"column":1}},"key":"Due8AJSh0T"},{"type":"paragraph","position":{"start":{"line":847,"column":1},"end":{"line":850,"column":1}},"children":[{"type":"text","value":"After adding this test, we see that we now have 100% coverage.\nIt’s important not to get too hung up on test coverage; rather than always aspiring to 100% coverage, it’s important to make sure that the most likely possible situations are tested.\nJust because you have 100% coverage doesn’t mean that your code is perfectly tested, since there could always be situations that you haven’t checked for.\nAnd spending too much time testing for unlikely problems can divert your efforts from other most useful activities.","position":{"start":{"line":847,"column":1},"end":{"line":847,"column":1}},"key":"p1zZeuCmnu"}],"key":"ttRiVdJEE2"},{"type":"heading","depth":2,"position":{"start":{"line":853,"column":1},"end":{"line":853,"column":1}},"children":[{"type":"text","value":"Test fixtures","position":{"start":{"line":853,"column":1},"end":{"line":853,"column":1}},"key":"xaqz0rdQJ1"}],"identifier":"test-fixtures","label":"Test fixtures","html_id":"test-fixtures","implicit":true,"key":"niWi0KLa1o"},{"type":"paragraph","position":{"start":{"line":855,"column":1},"end":{"line":857,"column":1}},"children":[{"type":"text","value":"Sometimes we need to use a the same data for multiple tests.\nRather than duplicating potentially time-consuming processes across each of the tests, it is often preferable to create a single instance of the object that can be used across multiple tests, which is known as a ","position":{"start":{"line":855,"column":1},"end":{"line":855,"column":1}},"key":"i09P0ZCZ8E"},{"type":"emphasis","position":{"start":{"line":855,"column":1},"end":{"line":855,"column":1}},"children":[{"type":"text","value":"test fixture","position":{"start":{"line":855,"column":1},"end":{"line":855,"column":1}},"key":"WcDMkYbmPX"}],"key":"GQlXRjvegH"},{"type":"text","value":".\nThis also helps maintain isolation between tests, since the order of tests shouldn’t matter if an appropriate fixture is generated as soon as it’s needed.","position":{"start":{"line":855,"column":1},"end":{"line":855,"column":1}},"key":"hx7bnDzjRe"}],"key":"LapDuObyiy"},{"type":"paragraph","position":{"start":{"line":859,"column":1},"end":{"line":863,"column":1}},"children":[{"type":"text","value":"For our example above, it’s likely that we will need to reuse the list of pubmed IDs from the search to perform various tests on the subsequent functions.\nWe can create a single version of this list of IDs by creating a fixture.\nIn the ","position":{"start":{"line":859,"column":1},"end":{"line":859,"column":1}},"key":"rRUnxZ12Lp"},{"type":"inlineCode","value":"pytest","position":{"start":{"line":859,"column":1},"end":{"line":859,"column":1}},"key":"e7eLN2Ym9k"},{"type":"text","value":" framework we do this using a special Python operator called a ","position":{"start":{"line":859,"column":1},"end":{"line":859,"column":1}},"key":"U6Wm0XAMbx"},{"type":"emphasis","position":{"start":{"line":859,"column":1},"end":{"line":859,"column":1}},"children":[{"type":"text","value":"decorator","position":{"start":{"line":859,"column":1},"end":{"line":859,"column":1}},"key":"zONTV2dwbX"}],"key":"BQelhDhLhm"},{"type":"text","value":", which is denoted by the symbol ","position":{"start":{"line":859,"column":1},"end":{"line":859,"column":1}},"key":"XfTwNnrY7c"},{"type":"inlineCode","value":"@","position":{"start":{"line":859,"column":1},"end":{"line":859,"column":1}},"key":"Jh8JlOfTii"},{"type":"text","value":" as a prefix.\nA decorator is function that takes another function as input, modifies its functionality, and returns another function; you don’t need to understand in detail how decorators work for this particular usage.\nTo refactor our tests above, we would first create the fixture by decorating the function that generates the fixture with the ","position":{"start":{"line":859,"column":1},"end":{"line":859,"column":1}},"key":"xRybD7lrT9"},{"type":"inlineCode","value":"@pytest.fixture","position":{"start":{"line":859,"column":1},"end":{"line":859,"column":1}},"key":"DH9g26OjLO"},{"type":"text","value":" decorator, setting the ","position":{"start":{"line":859,"column":1},"end":{"line":859,"column":1}},"key":"khmfDl3SZc"},{"type":"inlineCode","value":"scope","position":{"start":{"line":859,"column":1},"end":{"line":859,"column":1}},"key":"lEGljVZNlh"},{"type":"text","value":" variable to “session” so that the fixture is only generated once within the session:","position":{"start":{"line":859,"column":1},"end":{"line":859,"column":1}},"key":"yisyezgNAD"}],"key":"bTLtRYmShe"},{"type":"code","lang":"python","value":"@pytest.fixture(scope=\"session\")\ndef ids():\n    query = \"friston-k AND 'free energy'\"\n    ids = get_PubmedIDs_for_query(query)\n    return ids","position":{"start":{"line":865,"column":1},"end":{"line":871,"column":1}},"key":"PEpB7AzkRH"},{"type":"paragraph","position":{"start":{"line":873,"column":1},"end":{"line":873,"column":1}},"children":[{"type":"text","value":"We can then refactor our tests for a valid query to use the fixture by passing it as an argument to the test function:","position":{"start":{"line":873,"column":1},"end":{"line":873,"column":1}},"key":"XzHpYY8b7B"}],"key":"tH3scwOfzp"},{"type":"code","lang":"python","value":"def test_get_PubmedIDs_for_query_check_valid(ids):\n    assert isinstance(ids, list)\n    assert len(ids) > 0","position":{"start":{"line":875,"column":1},"end":{"line":879,"column":1}},"key":"YIUCyzFAZC"},{"type":"paragraph","position":{"start":{"line":881,"column":1},"end":{"line":884,"column":1}},"children":[{"type":"text","value":"The result is the same, but we now have a set of ids that we can reuse in subsequent tests, so that we don’t have to make repeated queries.\nIt’s important to note while using a session-scoped fixture: If any of the subsequent tests modify the fixture, those modifications will persist, which will break the isolation between tests.\nWe could prevent this by removing the ","position":{"start":{"line":881,"column":1},"end":{"line":881,"column":1}},"key":"avNPz2u02w"},{"type":"inlineCode","value":"scope=\"session\"","position":{"start":{"line":881,"column":1},"end":{"line":881,"column":1}},"key":"Hbav9AJyuP"},{"type":"text","value":" argument, which would then default to the standard scope which is within a specific function.\nIf you wish to use session-scoped fixtures and need to modify them within the test function, then it is best to first create a copy of the fixture object (e.g. ","position":{"start":{"line":881,"column":1},"end":{"line":881,"column":1}},"key":"TtiCKP978V"},{"type":"inlineCode","value":"my_ids = ids.copy()","position":{"start":{"line":881,"column":1},"end":{"line":881,"column":1}},"key":"rgeVKt0Jbj"},{"type":"text","value":") so that the global fixture object won’t be modified.","position":{"start":{"line":881,"column":1},"end":{"line":881,"column":1}},"key":"dIHWw8D1m9"}],"key":"t1S2XuexQw"},{"type":"heading","depth":2,"position":{"start":{"line":886,"column":1},"end":{"line":886,"column":1}},"children":[{"type":"text","value":"Mocking","position":{"start":{"line":886,"column":1},"end":{"line":886,"column":1}},"key":"YaLj44OH8y"}],"identifier":"mocking","label":"Mocking","html_id":"mocking","implicit":true,"key":"JREBrXRQrM"},{"type":"paragraph","position":{"start":{"line":888,"column":1},"end":{"line":891,"column":1}},"children":[{"type":"text","value":"Sometimes tests require infrastructure that is outside of the control of the tester.\nIn the example above, we are assuming that the Pubmed API is working correctly for our tests to run; if we were to try to run these tests without an internet connection, they would fail.\nIn other cases, code may rely upon a database system that may or may not exist on a particular system.\nIn these cases, we can create a mock object that can stand in for and simulate the behavior of the system that the code needs to interact with.","position":{"start":{"line":888,"column":1},"end":{"line":888,"column":1}},"key":"e8vImV5Fex"}],"key":"qGx6nNCIdX"},{"type":"paragraph","position":{"start":{"line":893,"column":1},"end":{"line":895,"column":1}},"children":[{"type":"text","value":"In our example, we want to create a mock response that looks sufficiently like a response from the real API to pass our tests.\nUsing pytest’s ","position":{"start":{"line":893,"column":1},"end":{"line":893,"column":1}},"key":"hqJRc8ucd5"},{"type":"emphasis","position":{"start":{"line":893,"column":1},"end":{"line":893,"column":1}},"children":[{"type":"text","value":"monkeypatch","position":{"start":{"line":893,"column":1},"end":{"line":893,"column":1}},"key":"c7rA6wQXlY"}],"key":"yOju2PpBB5"},{"type":"text","value":" fixture, we can temporarily replace the real requests.get function with our own fake function that returns a predictable, controlled response.\nWe first need to create a class that can replace the ","position":{"start":{"line":893,"column":1},"end":{"line":893,"column":1}},"key":"GeN3zxLhMi"},{"type":"inlineCode","value":"requests.get","position":{"start":{"line":893,"column":1},"end":{"line":893,"column":1}},"key":"V9ssVuKY8h"},{"type":"text","value":" call in ","position":{"start":{"line":893,"column":1},"end":{"line":893,"column":1}},"key":"QizsHtHktH"},{"type":"inlineCode","value":"get_PubmedIDs_for_query","position":{"start":{"line":893,"column":1},"end":{"line":893,"column":1}},"key":"D16jZbOfhg"},{"type":"text","value":", replacing it with a mock version that outputs a fixed simulacrum of an API response via its ","position":{"start":{"line":893,"column":1},"end":{"line":893,"column":1}},"key":"woPBl5reQE"},{"type":"inlineCode","value":".json()","position":{"start":{"line":893,"column":1},"end":{"line":893,"column":1}},"key":"cAvmFmfE8D"},{"type":"text","value":" method.","position":{"start":{"line":893,"column":1},"end":{"line":893,"column":1}},"key":"LgB3QOASKM"}],"key":"qbN0rAoRZA"},{"type":"code","lang":"python","value":"class MockPubmedResponse:\n    status_code = 200\n\n    def json():\n        return {\n            'header': {'type': 'esearch', 'version': '0.3'},\n            'esearchresult': {\n                'count': '2',\n                'retmax': '20',\n                'retstart': '0',\n                'idlist': ['39312494', '39089179']\n            }\n        }","position":{"start":{"line":897,"column":1},"end":{"line":911,"column":1}},"key":"WC8WGYJw6f"},{"type":"paragraph","position":{"start":{"line":913,"column":1},"end":{"line":914,"column":1}},"children":[{"type":"text","value":"We now insert this mock response for the standard ","position":{"start":{"line":913,"column":1},"end":{"line":913,"column":1}},"key":"JcR03PslON"},{"type":"inlineCode","value":"requests.get","position":{"start":{"line":913,"column":1},"end":{"line":913,"column":1}},"key":"OL2cIw0Dar"},{"type":"text","value":" call within the test.\nIn my initial attempt, I created created a fixture based on the mocked response and then tested that fixture:","position":{"start":{"line":913,"column":1},"end":{"line":913,"column":1}},"key":"OYMfO6ux1U"}],"key":"DZjfMDvXPs"},{"type":"code","lang":"python","value":"@pytest.fixture\ndef ids_mocked(monkeypatch):\n\n    def mock_get(*args, **kwargs):\n        return MockPubmedResponse()\n\n    # apply the monkeypatch for requests.get to mock_get\n    monkeypatch.setattr(requests, \"get\", mock_get)\n\n    query = \"friston-k AND 'free energy'\"\n    ids = get_PubmedIDs_for_query(query)\n    return ids\n\ndef test_get_PubmedIDs_for_query_check_valid_mocked(ids_mocked):\n    assert isinstance(ids_mocked, list)\n    assert len(ids_mocked) == 2\n","position":{"start":{"line":916,"column":1},"end":{"line":934,"column":1}},"key":"ppgqusmJ6U"},{"type":"paragraph","position":{"start":{"line":936,"column":1},"end":{"line":938,"column":1}},"children":[{"type":"text","value":"Turning off my network connection shows that the mocked test passes, while the tests that require connecting to the actual API fail.\nHowever, my usual code review (using Google’s Gemini 2.5 Pro) identified a problem with this fixture: it conflates the setup (creating the mock API) with the execution of the function that uses the mock API.\nA better approach (recommended by Gemini) is move the function execution out of the fixture and into the test:","position":{"start":{"line":936,"column":1},"end":{"line":936,"column":1}},"key":"QHPlp8b8EL"}],"key":"rW9390LnpN"},{"type":"code","lang":"python","value":"# Fixture ONLY does the setup (the mocking)\n@pytest.fixture\ndef mock_pubmed_api(monkeypatch):\n\n    class MockPubmedResponse:\n        status_code = 200\n        def json(self):\n            return {\n                'header': {'type': 'esearch', 'version': '0.3'},\n                'esearchresult': {\n                    'count': '2',\n                    'retmax': '20',\n                    'retstart': '0',\n                    'idlist': ['39312494', '39089179']\n                }\n            }\n\n    def mock_get(*args, **kwargs):\n        return MockPubmedResponse()\n\n    # Apply the monkeypatch for requests.get to mock_get\n    monkeypatch.setattr(requests, \"get\", mock_get)\n\n# The test requests the setup, then performs the action and assertion.\ndef test_get_PubmedIDs_for_query_check_valid_mocked(mock_pubmed_api):\n    # Action: Call the function under test\n    query = \"friston-k AND 'free energy'\"\n    ids = get_PubmedIDs_for_query(query)\n\n    # Assertion: Check the result\n    assert isinstance(ids, list)\n    assert len(ids) == 2","position":{"start":{"line":940,"column":1},"end":{"line":973,"column":1}},"key":"VYOIurgvge"},{"type":"paragraph","position":{"start":{"line":975,"column":1},"end":{"line":976,"column":1}},"children":[{"type":"text","value":"Note that while mocking can be useful for testing specific components by saving time and increasing robustness, integration tests and smoke tests should usually be run without mocking, in order to catch any errors that arise through interaction with the relevant components that are being mocked.\nIn fact, it’s always a good idea to have tests that specifically assess the usage of the external service and the system’s response to failures in that service (e.g. by using features of the testing framework that allow one to shut down access to the network).","position":{"start":{"line":975,"column":1},"end":{"line":975,"column":1}},"key":"aG6Cm45f5H"}],"key":"DCorM8e9Nv"},{"type":"heading","depth":2,"position":{"start":{"line":978,"column":1},"end":{"line":978,"column":1}},"children":[{"type":"text","value":"Parametrized tests","position":{"start":{"line":978,"column":1},"end":{"line":978,"column":1}},"key":"aXTEm7rCE9"}],"identifier":"parametrized-tests","label":"Parametrized tests","html_id":"parametrized-tests","implicit":true,"key":"VUD36TNYz3"},{"type":"paragraph","position":{"start":{"line":980,"column":1},"end":{"line":981,"column":1}},"children":[{"type":"text","value":"Often a function needs to accept a range of inputs that can result in different behavior, and we want to test each of the possible inputs to ensure that the function works correctly across the range.\nWhen the different inputs are known, one way to achieve this is to use a ","position":{"start":{"line":980,"column":1},"end":{"line":980,"column":1}},"key":"aABD0KqWBX"},{"type":"emphasis","position":{"start":{"line":980,"column":1},"end":{"line":980,"column":1}},"children":[{"type":"text","value":"parameterized test","position":{"start":{"line":980,"column":1},"end":{"line":980,"column":1}},"key":"kRbYQwAGu1"}],"key":"HAlRV1SrhD"},{"type":"text","value":", in which the test is repeatedly run across combinations of different possible values.","position":{"start":{"line":980,"column":1},"end":{"line":980,"column":1}},"key":"HjKvZGAWsJ"}],"key":"RIqg5UrjvR"},{"type":"paragraph","position":{"start":{"line":983,"column":1},"end":{"line":986,"column":1}},"children":[{"type":"text","value":"For our example, let’s move forward and develop the function ","position":{"start":{"line":983,"column":1},"end":{"line":983,"column":1}},"key":"BnXTuqRC1W"},{"type":"inlineCode","value":"parse_year_from_Pubmed_record","position":{"start":{"line":983,"column":1},"end":{"line":983,"column":1}},"key":"JytzS54WVF"},{"type":"text","value":" to extract the year from Pubmed records, which can differ in their structure.\nWe first need to develop the function ","position":{"start":{"line":983,"column":1},"end":{"line":983,"column":1}},"key":"ucega8J9ZO"},{"type":"inlineCode","value":"get_record_from_PubmedID","position":{"start":{"line":983,"column":1},"end":{"line":983,"column":1}},"key":"UCG7zQAgjd"},{"type":"text","value":" to retrieve a record based on a Pubmed ID.\nFollowing our TDD approach, we first develop two simple tests: one to ensure that it returns a non-empty dictionary for a valid Pubmed ID, and one to ensure that it raises an exception for an invalid Pubmed ID.\nWe also need to create empty functions so that they can be imported to run the (failing) tests:","position":{"start":{"line":983,"column":1},"end":{"line":983,"column":1}},"key":"sxkUhL3guD"}],"key":"Dtqbxd9zN7"},{"type":"code","lang":"python","value":"def get_record_from_PubmedID(pmid: str) -> dict:\n    pass\n\ndef parse_year_from_Pubmed_record(pubmed_record: dict) -> int:\n    pass","position":{"start":{"line":988,"column":1},"end":{"line":994,"column":1}},"key":"cmc4Ir0Tj7"},{"type":"paragraph","position":{"start":{"line":996,"column":1},"end":{"line":997,"column":1}},"children":[{"type":"text","value":"Here are the initial tests; note that writing these tests requires a bit of knowledge about the expected structure of a Pubmed record.\nWe will generate a fixture so that the valid record and PubMed ID can be reused in a later test.","position":{"start":{"line":996,"column":1},"end":{"line":996,"column":1}},"key":"WyL0OnVBlY"}],"key":"OcBpMRoQNJ"},{"type":"code","lang":"python","value":"@pytest.fixture(scope=\"session\")\ndef valid_pmid():\n    return \"39312494\"\n\n@pytest.fixture(scope=\"session\")\ndef pmid_record(valid_pmid):\n    record = get_record_from_PubmedID(valid_pmid)\n    return record\n\ndef test_get_record_from_valid_PubmedID(pmid_record, valid_pmid):\n    assert pmid_record is not None\n    assert isinstance(pmid_record, dict)\n    assert pmid_record['uid'] == valid_pmid\n\ndef test_get_record_from_invalid_PubmedID():\n    pmid = \"10000000000\"\n    with pytest.raises(ValueError):\n        record = get_record_from_PubmedID(pmid)\n","position":{"start":{"line":999,"column":1},"end":{"line":1019,"column":1}},"key":"w5hjniHaG5"},{"type":"paragraph","position":{"start":{"line":1021,"column":1},"end":{"line":1021,"column":1}},"children":[{"type":"text","value":"Armed with these tests, we now work with Copilot to develop the code for ","position":{"start":{"line":1021,"column":1},"end":{"line":1021,"column":1}},"key":"RNiwDMKSwa"},{"type":"inlineCode","value":"get_record_from_PubmedID","position":{"start":{"line":1021,"column":1},"end":{"line":1021,"column":1}},"key":"ROVmEgb97N"},{"type":"text","value":":","position":{"start":{"line":1021,"column":1},"end":{"line":1021,"column":1}},"key":"uJhZa6KG8m"}],"key":"VgSIGMrK8F"},{"type":"code","lang":"python","value":"def get_record_from_PubmedID(pmid: str, esummary_url: str = None) -> dict:\n\n    if esummary_url is None:\n        esummary_url = f\"{BASE_URL}/esummary.fcgi?db=pubmed&id={pmid}&retmode=json\"\n\n    response = requests.get(esummary_url)\n\n    result_json = response.json()\n\n    if (\n        response.status_code != 200\n        or \"result\" not in result_json\n        or pmid not in result_json[\"result\"]\n        or \"error\" in result_json[\"result\"][pmid]\n    ):\n        raise ValueError(\"Bad request\")\n\n    return result_json[\"result\"][pmid]","position":{"start":{"line":1023,"column":1},"end":{"line":1042,"column":1}},"key":"XjUGYIBZVR"},{"type":"paragraph","position":{"start":{"line":1044,"column":1},"end":{"line":1044,"column":1}},"children":[{"type":"text","value":"This passes the tests, so we can now move to writing some initial tests for ","position":{"start":{"line":1044,"column":1},"end":{"line":1044,"column":1}},"key":"YVH12W7Pih"},{"type":"inlineCode","value":"parse_year_from_Pubmed_record","position":{"start":{"line":1044,"column":1},"end":{"line":1044,"column":1}},"key":"UDuRFqASrA"},{"type":"text","value":":","position":{"start":{"line":1044,"column":1},"end":{"line":1044,"column":1}},"key":"mUB6lB1wvM"}],"key":"ioUid4K8oU"},{"type":"code","lang":"python","value":"def test_parse_year_from_Pubmed_record():\n    record = {\n        \"pubdate\": \"2021 Jan 1\"\n    }\n    year = parse_year_from_Pubmed_record(record)\n    assert year == 2021\n\n\ndef test_parse_year_from_Pubmed_record_empty():\n    record = {\n        \"pubdate\": \"\"\n    }\n    year = parse_year_from_Pubmed_record(record)\n    assert year is None","position":{"start":{"line":1046,"column":1},"end":{"line":1061,"column":1}},"key":"GaCN3Lo0L8"},{"type":"paragraph","position":{"start":{"line":1063,"column":1},"end":{"line":1063,"column":1}},"children":[{"type":"text","value":"And then we use our AI tool to develop the implementation:","position":{"start":{"line":1063,"column":1},"end":{"line":1063,"column":1}},"key":"CzpNWWMEKf"}],"key":"r7Jiq9roey"},{"type":"code","lang":"python","value":"def parse_year_from_Pubmed_record(pubmed_record: dict) -> int:\n    pubdate = pubmed_record.get(\"pubdate\") \n    return int(pubdate.split()[0]) if pubdate else None","position":{"start":{"line":1065,"column":1},"end":{"line":1069,"column":1}},"key":"mBIar5qOfh"},{"type":"paragraph","position":{"start":{"line":1071,"column":1},"end":{"line":1072,"column":1}},"children":[{"type":"text","value":"Now let’s say that you had a specific set of Pubmed IDs that you wanted to test the code against; for example, you might select IDs from papers published in various years across various journals.\nTo do this, we first create a list of tuples that include the information that we will need for the test; in this case it’s the Pubmed ID and the true year of publication.","position":{"start":{"line":1071,"column":1},"end":{"line":1071,"column":1}},"key":"TOeXDaymzT"}],"key":"EENf95ihNC"},{"type":"code","lang":"python","value":"testdata = [\n    ('17773841', 1944),\n    ('13148370', 1954),\n    ('14208567', 1964),\n    ('4621244', 1974),\n    ('6728178', 1984),\n    ('10467601', 1994),\n    ('15050513', 2004)\n]","position":{"start":{"line":1074,"column":1},"end":{"line":1084,"column":1}},"key":"TwdeEyIwQp"},{"type":"paragraph","position":{"start":{"line":1086,"column":1},"end":{"line":1086,"column":1}},"children":[{"type":"text","value":"We then feed this into our test using the ","position":{"start":{"line":1086,"column":1},"end":{"line":1086,"column":1}},"key":"WeOpobAHwA"},{"type":"inlineCode","value":"@pytest.mark.parametrize","position":{"start":{"line":1086,"column":1},"end":{"line":1086,"column":1}},"key":"CVHkcdohag"},{"type":"text","value":" decorator on the test, which will feed in each of the values into the test:","position":{"start":{"line":1086,"column":1},"end":{"line":1086,"column":1}},"key":"s8q9yEHHGd"}],"key":"VukUi2Rl4h"},{"type":"code","lang":"python","value":"@pytest.mark.parametrize(\"pmid, year_true\", testdata)\ndef test_parse_year_from_pmid_parametric(pmid, year_true):\n    time.sleep(0.5) # delay to avoid hitting the PubMed API too quickly\n    record = get_record_from_PubmedID(pmid)\n    year_result = parse_year_from_Pubmed_record(record)\n    assert year_result == year_true","position":{"start":{"line":1088,"column":1},"end":{"line":1095,"column":1}},"key":"wtHLxojlyd"},{"type":"paragraph","position":{"start":{"line":1097,"column":1},"end":{"line":1098,"column":1}},"children":[{"type":"text","value":"Note that we inserted a delay at the beginning of the test; this is necessary because the PubMed API will has a rate limit on requests, and running these tests without a delay to limit the request rate will result in intermittent test failures.\nLooking at the results of running the test, we will see that each parametric value is run as a separate test:","position":{"start":{"line":1097,"column":1},"end":{"line":1097,"column":1}},"key":"TKXSzuA1SU"}],"key":"nvGW1qOksT"},{"type":"code","lang":"bash","value":"...\ntests/textmining/test_textmining.py::test_parse_year_from_pmid_parametric[17773841-1944] PASSED       [ 62%]\ntests/textmining/test_textmining.py::test_parse_year_from_pmid_parametric[13148370-1954] PASSED       [ 68%]\ntests/textmining/test_textmining.py::test_parse_year_from_pmid_parametric[14208567-1964] PASSED       [ 75%]\ntests/textmining/test_textmining.py::test_parse_year_from_pmid_parametric[4621244-1974] PASSED        [ 81%]\ntests/textmining/test_textmining.py::test_parse_year_from_pmid_parametric[6728178-1984] PASSED        [ 87%]\ntests/textmining/test_textmining.py::test_parse_year_from_pmid_parametric[10467601-1994] PASSED       [ 93%]\ntests/textmining/test_textmining.py::test_parse_year_from_pmid_parametric[15050513-2004] PASSED       [100%]","position":{"start":{"line":1100,"column":1},"end":{"line":1109,"column":1}},"key":"LDZf2833cG"},{"type":"paragraph","position":{"start":{"line":1111,"column":1},"end":{"line":1112,"column":1}},"children":[{"type":"text","value":"This test requires a live API, so it would fail in cases where one didn’t have a proper network connection or if the API was down, and it would also be slow for a large number of tests.\nIt would be more efficient to mock the ","position":{"start":{"line":1111,"column":1},"end":{"line":1111,"column":1}},"key":"zRJv8JviPQ"},{"type":"inlineCode","value":"get_record_from_PubmedID","position":{"start":{"line":1111,"column":1},"end":{"line":1111,"column":1}},"key":"Mm1pneV64b"},{"type":"text","value":" function to avoid dependency on the live API, but for our simple purposes it’s fine to use the live API.","position":{"start":{"line":1111,"column":1},"end":{"line":1111,"column":1}},"key":"BLuZKZoX1p"}],"key":"pNXwSb9pPp"},{"type":"heading","depth":2,"position":{"start":{"line":1114,"column":1},"end":{"line":1114,"column":1}},"children":[{"type":"text","value":"Property-based testing","position":{"start":{"line":1114,"column":1},"end":{"line":1114,"column":1}},"key":"OKeLmZnzHA"}],"identifier":"property-based-testing","label":"Property-based testing","html_id":"property-based-testing","implicit":true,"key":"KxWeu2aKJ4"},{"type":"paragraph","position":{"start":{"line":1116,"column":1},"end":{"line":1117,"column":1}},"children":[{"type":"text","value":"Parameterized testing can be useful when we have specific values that we want to test, but sometimes we wish to test a large range of possible values drawn from some sort of distribution.\nOne approach to doing this is known as ","position":{"start":{"line":1116,"column":1},"end":{"line":1116,"column":1}},"key":"kMlkql10o0"},{"type":"emphasis","position":{"start":{"line":1116,"column":1},"end":{"line":1116,"column":1}},"children":[{"type":"text","value":"property-based testing","position":{"start":{"line":1116,"column":1},"end":{"line":1116,"column":1}},"key":"vHqQGIYfFc"}],"key":"r3ERxHhMaX"},{"type":"text","value":", and basically involves generating random values that match some specification and testing the code against those.","position":{"start":{"line":1116,"column":1},"end":{"line":1116,"column":1}},"key":"qmNhP5zR8x"}],"key":"sdVCffE6aO"},{"type":"paragraph","position":{"start":{"line":1119,"column":1},"end":{"line":1121,"column":1}},"children":[{"type":"text","value":"Property-based testing can be particularly useful for testing mathematical code, so we will develop another simple example to show how to use the ","position":{"start":{"line":1119,"column":1},"end":{"line":1119,"column":1}},"key":"NzxRFxvSa8"},{"type":"inlineCode","value":"hypothesis","position":{"start":{"line":1119,"column":1},"end":{"line":1119,"column":1}},"key":"qE4CZ3Zcg8"},{"type":"text","value":" module in Python to perform property-based testing.\nLet’s say that we have developed a function to perform linear regression, taking in two vectors (X and y variables) and return a vector of length 2 (parameter estimates for slope and intercept).\nCopilot generates some very terse code for us:","position":{"start":{"line":1119,"column":1},"end":{"line":1119,"column":1}},"key":"ucXnKGiUmZ"}],"key":"lmgeSdFxWa"},{"type":"code","lang":"python","value":"def linear_regression(X, y):\n    X = np.c_[np.ones(X.shape[0]), X]\n    return np.linalg.inv(X.T @ X) @ X.T @ y","position":{"start":{"line":1123,"column":1},"end":{"line":1127,"column":1}},"key":"rkmuulhwoa"},{"type":"paragraph","position":{"start":{"line":1129,"column":1},"end":{"line":1129,"column":1}},"children":[{"type":"text","value":"Asking Copilot to make the code more readable, we get this somewhat overly verbose version:","position":{"start":{"line":1129,"column":1},"end":{"line":1129,"column":1}},"key":"JqkcznLqGj"}],"key":"vhHSlmHWws"},{"type":"code","lang":"python","value":"def linear_regression_verbose(X, y):\n    # Add a column of ones to the input data to account for the intercept term\n    X_with_intercept = np.c_[np.ones(X.shape[0]), X]\n\n    # Compute the parameters using the normal equation\n    X_transpose = X_with_intercept.T\n    X_transpose_X = X_transpose @ X_with_intercept\n    X_transpose_y = X_transpose @ y\n    beta = np.linalg.inv(X_transpose_X) @ X_transpose_y\n\n    return beta","position":{"start":{"line":1131,"column":1},"end":{"line":1143,"column":1}},"key":"JV5H0ohSYM"},{"type":"paragraph","position":{"start":{"line":1145,"column":1},"end":{"line":1145,"column":1}},"children":[{"type":"text","value":"The linear regression computation requires several things to be true of the input data in order to proceed without error:","position":{"start":{"line":1145,"column":1},"end":{"line":1145,"column":1}},"key":"GOd2xzJI8D"}],"key":"iliZgWo8Ea"},{"type":"list","ordered":false,"spread":false,"position":{"start":{"line":1147,"column":1},"end":{"line":1150,"column":1}},"children":[{"type":"listItem","spread":true,"position":{"start":{"line":1147,"column":1},"end":{"line":1147,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"The data must not contain any infinite or ","position":{"start":{"line":1147,"column":1},"end":{"line":1147,"column":1}},"key":"jdA7MHinlP"},{"type":"emphasis","position":{"start":{"line":1147,"column":1},"end":{"line":1147,"column":1}},"children":[{"type":"text","value":"NaN","position":{"start":{"line":1147,"column":1},"end":{"line":1147,"column":1}},"key":"yXZRhtjrh2"}],"key":"FdYp1NTuzm"},{"type":"text","value":" values","position":{"start":{"line":1147,"column":1},"end":{"line":1147,"column":1}},"key":"oY1ag9SIVT"}],"key":"IianDLBD7X"}],"key":"sSatM4UdTI"},{"type":"listItem","spread":true,"position":{"start":{"line":1148,"column":1},"end":{"line":1148,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"The input data for X and y must each have at least two unique values","position":{"start":{"line":1148,"column":1},"end":{"line":1148,"column":1}},"key":"oKYE0qG8ZL"}],"key":"WNv3B67E3T"}],"key":"lSpBG3fQ11"},{"type":"listItem","spread":true,"position":{"start":{"line":1149,"column":1},"end":{"line":1150,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"The X matrix must be full rank","position":{"start":{"line":1149,"column":1},"end":{"line":1149,"column":1}},"key":"MUG8IgYu6R"}],"key":"ft9Oilsgqs"}],"key":"nxOM1swxnv"}],"key":"LifKRkMJnT"},{"type":"paragraph","position":{"start":{"line":1151,"column":1},"end":{"line":1151,"column":1}},"children":[{"type":"text","value":"In order to ensure that the input data are valid, we generate a function that validates the inputs, raising an exception if they are not, and we include this in our linear regression function:","position":{"start":{"line":1151,"column":1},"end":{"line":1151,"column":1}},"key":"JPHValsBOi"}],"key":"QfOMsqPYWd"},{"type":"code","lang":"python","value":"\ndef _validate_input(X, y):\n    if np.isinf(X).any() or np.isinf(y).any():\n        raise ValueError(\"Input data contains infinite values\")\n    if np.isnan(X).any() or np.isnan(y).any():\n        raise ValueError(\"Input data contains NaN values\")\n    if len(np.unique(X)) < 2 or len(np.unique(y)) < 2:\n        raise ValueError(\"Input data must have at least 2 unique values\")\n\n    X_with_intercept = np.c_[np.ones(X.shape[0]), X]\n    if np.linalg.matrix_rank(X_with_intercept) < X_with_intercept.shape[1]:\n        raise ValueError(\"Input data is not full rank\")\n\n\ndef linear_regression(X, y, validate=True):\n\n    if validate:\n        _validate_input(X, y)\n\n    X = np.c_[np.ones(X.shape[0]), X]\n    return np.linalg.inv(X.T @ X) @ X.T @ y","position":{"start":{"line":1153,"column":1},"end":{"line":1175,"column":1}},"key":"Oq7AVYhx8s"},{"type":"paragraph","position":{"start":{"line":1177,"column":1},"end":{"line":1177,"column":1}},"children":[{"type":"text","value":"Now we can use the ","position":{"start":{"line":1177,"column":1},"end":{"line":1177,"column":1}},"key":"DeFpyBj2re"},{"type":"inlineCode","value":"hypothesis","position":{"start":{"line":1177,"column":1},"end":{"line":1177,"column":1}},"key":"qPEAWFxDT0"},{"type":"text","value":" module to throw a range of data at this function and see if it fails, using the following test:","position":{"start":{"line":1177,"column":1},"end":{"line":1177,"column":1}},"key":"Xgzgjpbxyp"}],"key":"KJoIdf3sUO"},{"type":"code","lang":"python","value":"@given(\n    # Only generate data that is likely to be valid to start with\n    nps.arrays(np.float64, (10, 1), elements=st.floats(-1e6, 1e6)),\n    nps.arrays(np.float64, (10,), elements=st.floats(-1e6, 1e6)),\n)\ndef test_linear_regression_without_validation(X, y):\n    \"\"\"Tests that our algorithm matches a reference implementation (scipy).\"\"\"\n\n    # Now we can safely test the math against a reference implementation (scipy), \n    # knowing the input is valid.\n    params = linear_regression(X, y, validate=False)\n    assert params is not None, \"Parameters should not be None\"","position":{"start":{"line":1179,"column":1},"end":{"line":1192,"column":1}},"key":"tSUqGIXsw9"},{"type":"paragraph","position":{"start":{"line":1194,"column":1},"end":{"line":1197,"column":1}},"children":[{"type":"text","value":"The ","position":{"start":{"line":1194,"column":1},"end":{"line":1194,"column":1}},"key":"ltvOtUsrk7"},{"type":"inlineCode","value":"@given","position":{"start":{"line":1194,"column":1},"end":{"line":1194,"column":1}},"key":"kSXbJmGll8"},{"type":"text","value":" decorator contains commands that will generate two arrays of the same size, which are then used as our X and y variables.\nThe main purpose of the test is to see whether the function successfully executes (i.e. a smoke test), but we include a minimal assertion to make sure that it returns a value that is not None.\nWe will turn off the validation in order to see what happens if the linear regression function is given invalid data.\nRunning this test, we see that the test fails, with the following output:","position":{"start":{"line":1194,"column":1},"end":{"line":1194,"column":1}},"key":"leQai2WjWE"}],"key":"rr6x1zVhom"},{"type":"code","lang":"bash","value":"❯ pytest tests/property_based_testing/test_propertybased_smoke.py\n=========================== test session starts ===========================\ntests/property_based_testing/test_propertybased_smoke.py F          [100%]\n\n================================ FAILURES =================================\n________________ test_linear_regression_without_validation ________________\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\ntests/property_based_testing/test_propertybased_smoke.py:19: in test_linear_regression_without_validation\n    params = linear_regression(X, y, validate=False)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nsrc/BetterCodeBetterScience/my_linear_regression.py:61: in linear_regression\n    return np.linalg.inv(X.T @ X) @ X.T @ y\n           ^^^^^^^^^^^^^^^^^^^^^^\n.venv/lib/python3.12/site-packages/numpy/linalg/_linalg.py:615: in inv\n    ainv = _umath_linalg.inv(a, signature=signature)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n\nerr = 'invalid value', flag = 8\n\n    def _raise_linalgerror_singular(err, flag):\n>       raise LinAlgError(\"Singular matrix\")\nE       numpy.linalg.LinAlgError: Singular matrix\nE       Falsifying example: test_linear_regression_without_validation(\nE           X=array([[0.],\nE                  [0.],\nE                  [0.],\nE                  [0.],\nE                  [0.],\nE                  [0.],\nE                  [0.],\nE                  [0.],\nE                  [0.],\nE                  [0.]]),\nE           y=array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]),  # or any other generated value\nE       )\nE       Explanation:\nE           These lines were always and only run by failing examples:\nE               /Users/poldrack/Dropbox/code/BetterCodeBetterScience/.venv/lib/python3.12/site-packages/numpy/linalg/_linalg.py:104\n\n========================= short test summary info =========================\nFAILED tests/property_based_testing/test_propertybased_smoke.py::test_linear_regression_without_validation - numpy.linalg.LinAlgError: Singular matrix\n============================ 1 failed in 2.33s ============================","position":{"start":{"line":1199,"column":1},"end":{"line":1243,"column":1}},"key":"fOT5KcGQws"},{"type":"paragraph","position":{"start":{"line":1245,"column":1},"end":{"line":1246,"column":1}},"children":[{"type":"text","value":"The test has identified a specific input that will cause the code to fail - namely, when the X variable is all zeros, which leads to an error when trying to invert the singular matrix.\nWe could get the test to pass by causing the function to return ","position":{"start":{"line":1245,"column":1},"end":{"line":1245,"column":1}},"key":"LG76uaPyr8"},{"type":"inlineCode","value":"None","position":{"start":{"line":1245,"column":1},"end":{"line":1245,"column":1}},"key":"KsjgW5u4S5"},{"type":"text","value":" when the matrix is no invertible, but this is not a great practice; we should announce problems loudly by raising an exception, rather than burying them quietly by returning ","position":{"start":{"line":1245,"column":1},"end":{"line":1245,"column":1}},"key":"LaSHlOP910"},{"type":"inlineCode","value":"None","position":{"start":{"line":1245,"column":1},"end":{"line":1245,"column":1}},"key":"doQzQnup6i"},{"type":"text","value":".","position":{"start":{"line":1245,"column":1},"end":{"line":1245,"column":1}},"key":"h9aYaayHKQ"}],"key":"Zw7hSveaxU"},{"type":"paragraph","position":{"start":{"line":1248,"column":1},"end":{"line":1249,"column":1}},"children":[{"type":"text","value":"Now that we have seen how ","position":{"start":{"line":1248,"column":1},"end":{"line":1248,"column":1}},"key":"meVhbyLJAy"},{"type":"inlineCode","value":"hypothesis","position":{"start":{"line":1248,"column":1},"end":{"line":1248,"column":1}},"key":"c0IX6PdFvr"},{"type":"text","value":" can identify errors, let’s develop some tests for the code that we can use to make sure that it works properly.\nWe will first separately test the validator function, making sure that it can detect any of the potential problems that it should be able to detect:","position":{"start":{"line":1248,"column":1},"end":{"line":1248,"column":1}},"key":"lBKE9I2E2J"}],"key":"O4bhjNm4LO"},{"type":"code","lang":"python","value":"@given(\n    nps.arrays(\n        np.float64, (10, 1), elements=st.floats(allow_nan=True, allow_infinity=True)\n    ),\n    nps.arrays(\n        np.float64, (10,), elements=st.floats(allow_nan=True, allow_infinity=True)\n    ),\n)\ndef test_validate_input(X, y):\n    \"\"\"Tests that our validation function correctly identifies and rejects bad data.\"\"\"\n    try:\n        # Call the validation function directly\n        _validate_input(X, y)\n        linear_regression(X, y, validate=False)\n        # If it gets here, hypothesis generated valid data and the function ran successfully. \n    except ValueError:\n        # If we get here, the data was invalid. The validator correctly\n        # raised an error. This is also a successful test case.\n        pass # Explicitly show that catching the error is the goal.","position":{"start":{"line":1251,"column":1},"end":{"line":1271,"column":1}},"key":"oDWv9fKx1a"},{"type":"paragraph","position":{"start":{"line":1273,"column":1},"end":{"line":1276,"column":1}},"children":[{"type":"text","value":"Note that this doesn’t actually whether our code actually gives the right answer, only that it runs without error and catches the appropriate problem cases, ensuring that any data passing the validator can run without error on the linear regression function.\nWhen a reference implementation exists for a function (as it does in the case of linear regression), then we can compare our results to the results from the reference.\nHere we will compare to the outputs from the from the linear regression function from the ","position":{"start":{"line":1273,"column":1},"end":{"line":1273,"column":1}},"key":"OOFsmygk5F"},{"type":"inlineCode","value":"scipy","position":{"start":{"line":1273,"column":1},"end":{"line":1273,"column":1}},"key":"iEMs8FKcST"},{"type":"text","value":" module.\nUsing this, we can check the randomly generated input to see whether it should raise an exception, and otherwise compare the results of our function to the scipy function:","position":{"start":{"line":1273,"column":1},"end":{"line":1273,"column":1}},"key":"L9SDeI9EyU"}],"key":"ge5Jw20d3l"},{"type":"code","lang":"python","value":"# Test 2: Test the algorithm's correctness, assuming valid input\n# --------------------------------------------------------------\n@given(\n    # Only generate data that is likely to be valid to start with\n    nps.arrays(np.float64, (10, 1), elements=st.floats(-1e6, 1e6)),\n    nps.arrays(np.float64, (10,), elements=st.floats(-1e6, 1e6)),\n)\ndef test_linear_regression_correctness(X, y):\n    \"\"\"Tests that our algorithm matches a reference implementation (scipy).\"\"\"\n    # Use `hypothesis.assume` to filter out any edge cases the validator would catch.\n    # This tells hypothesis: \"If this data is bad, just discard it and try another.\"\n    try:\n        _validate_input(X, y)\n    except ValueError:\n        assume(False)  # Prunes this example from the test run\n\n    # Now we can safely test the math against a reference implementation (scipy), \n    # knowing the input is valid.\n    params = linear_regression(X, y)\n    lr_result = linregress(X.flatten(), y.flatten())\n\n    assert np.allclose(params, [lr_result.intercept, lr_result.slope])","position":{"start":{"line":1278,"column":1},"end":{"line":1301,"column":1}},"key":"T70wuEBkVq"},{"type":"paragraph","position":{"start":{"line":1303,"column":1},"end":{"line":1305,"column":1}},"children":[{"type":"text","value":"This test passes, showing that our function closely matches the scipy reference implementation.\nNote that we restricted the range of the values generated by the test to ","position":{"start":{"line":1303,"column":1},"end":{"line":1303,"column":1}},"key":"otBNOK4tr7"},{"type":"inlineCode","value":"[-1e6, 1e6]","position":{"start":{"line":1303,"column":1},"end":{"line":1303,"column":1}},"key":"Bs6VUcz7Hv"},{"type":"text","value":"; when the test values were allowed to vary across the full range of 64-bit floating point values (+/- 1.79e+308), we observed minute differences in the parameter estimates between the two functions that nonetheless exceeded the tolerance limits of ","position":{"start":{"line":1303,"column":1},"end":{"line":1303,"column":1}},"key":"cilh3gMbzW"},{"type":"inlineCode","value":"np.allclose()","position":{"start":{"line":1303,"column":1},"end":{"line":1303,"column":1}},"key":"UuVD1uKPL6"},{"type":"text","value":".\nWe decided to restrict the test values to a range that is within the usual range of input data; if one were planning to work with very small or very large numbers, they would want to possibly test the input over a wider range and understand the nature and magnitude of differences in results between the methods.","position":{"start":{"line":1303,"column":1},"end":{"line":1303,"column":1}},"key":"kycP5iooJM"}],"key":"WbVqjjr7kM"},{"type":"heading","depth":2,"position":{"start":{"line":1308,"column":1},"end":{"line":1308,"column":1}},"children":[{"type":"text","value":"Automated testing and continuous integration","position":{"start":{"line":1308,"column":1},"end":{"line":1308,"column":1}},"key":"Aw1zX8ZQMD"}],"identifier":"automated-testing-and-continuous-integration","label":"Automated testing and continuous integration","html_id":"automated-testing-and-continuous-integration","implicit":true,"key":"kAlQG5iy8D"},{"type":"paragraph","position":{"start":{"line":1310,"column":1},"end":{"line":1310,"column":1}},"children":[{"type":"text","value":"Once we have a set of tests for a project, it’s important to integrate them into our workflow so that we ensure that new changes to the code don’t break the existing code (known as a ","position":{"start":{"line":1310,"column":1},"end":{"line":1310,"column":1}},"key":"OBEaErTwSD"},{"type":"emphasis","position":{"start":{"line":1310,"column":1},"end":{"line":1310,"column":1}},"children":[{"type":"text","value":"regression","position":{"start":{"line":1310,"column":1},"end":{"line":1310,"column":1}},"key":"FbjcSA9j9u"}],"key":"ciS4OO7WNV"},{"type":"text","value":").","position":{"start":{"line":1310,"column":1},"end":{"line":1310,"column":1}},"key":"dEjhDYWSMq"}],"key":"QxET095I91"},{"type":"paragraph","position":{"start":{"line":1312,"column":1},"end":{"line":1316,"column":1}},"children":[{"type":"text","value":"One useful way to ensure that the tests are run regularly is to run them automatically every time changes are pushed to version control.\nThis is known as “continuous integration” (CI for short), referring to the fact that it allows changes to be continuously integrated into the main code branch once they are confirmed to pass all of the tests.\nThere are a number of different platforms one can use for CI; we will focus on ","position":{"start":{"line":1312,"column":1},"end":{"line":1312,"column":1}},"key":"knOSPpb1gG"},{"type":"emphasis","position":{"start":{"line":1312,"column":1},"end":{"line":1312,"column":1}},"children":[{"type":"text","value":"Github Actions","position":{"start":{"line":1312,"column":1},"end":{"line":1312,"column":1}},"key":"cnk1BEb9H6"}],"key":"XKh6gw96rW"},{"type":"text","value":" since it is the most tightly integrated into the Github version control system.\nTesting using CI also has a useful side effect: Since the CI system uses a virtual machine to run the tests, the use of CI for testing ensures that the code can run on a separate machine from the one where it was developed.\nBecause setting up the CI system also requires understanding all of the dependencies that are required for the code to run, the CI setup provides a recipe to run the code on any other system.","position":{"start":{"line":1312,"column":1},"end":{"line":1312,"column":1}},"key":"YwoIDDqbRi"}],"key":"H6nUmLSvg3"},{"type":"heading","depth":3,"position":{"start":{"line":1318,"column":1},"end":{"line":1318,"column":1}},"children":[{"type":"text","value":"Using GitHub Actions","position":{"start":{"line":1318,"column":1},"end":{"line":1318,"column":1}},"key":"XNNs9kHrgf"}],"identifier":"using-github-actions","label":"Using GitHub Actions","html_id":"using-github-actions","implicit":true,"key":"qX0kIP8TbF"},{"type":"paragraph","position":{"start":{"line":1320,"column":1},"end":{"line":1321,"column":1}},"children":[{"type":"text","value":"GitHub Actions is a system that allows one to automatically execute workflows in response to events related to any GitHub repository.\nAt the time of writing, Github Actions are free and unlimited for public repositories when using the standard GitHub-based workflow runners.","position":{"start":{"line":1320,"column":1},"end":{"line":1320,"column":1}},"key":"VvONecCpeA"}],"key":"t6PEhmV9GP"},{"type":"paragraph","position":{"start":{"line":1323,"column":1},"end":{"line":1323,"column":1}},"children":[{"type":"text","value":"When setting up an automated action using GitHub Actions, there are two primary decisions to specify:","position":{"start":{"line":1323,"column":1},"end":{"line":1323,"column":1}},"key":"zNZQJczFDF"}],"key":"DNZFETWR9Z"},{"type":"list","ordered":false,"spread":false,"position":{"start":{"line":1325,"column":1},"end":{"line":1327,"column":1}},"children":[{"type":"listItem","spread":true,"position":{"start":{"line":1325,"column":1},"end":{"line":1325,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"What is the workflow that I want to run?","position":{"start":{"line":1325,"column":1},"end":{"line":1325,"column":1}},"key":"jpiWWhGVdS"}],"key":"UpGSL4uQlx"}],"key":"yKpvsV0mWu"},{"type":"listItem","spread":true,"position":{"start":{"line":1326,"column":1},"end":{"line":1327,"column":1}},"children":[{"type":"paragraph","children":[{"type":"text","value":"What are the events that I want to trigger the workflow?","position":{"start":{"line":1326,"column":1},"end":{"line":1326,"column":1}},"key":"Eq7IO2DM4b"}],"key":"x6gH3AlhsH"}],"key":"cNo0YL4RV2"}],"key":"TKTIi3ppzl"},{"type":"paragraph","position":{"start":{"line":1328,"column":1},"end":{"line":1332,"column":1}},"children":[{"type":"text","value":"As an example, we will implement a workflow running to execute tests for a simple python package that was generated for this book project, called ","position":{"start":{"line":1328,"column":1},"end":{"line":1328,"column":1}},"key":"Bq7peVSs1C"},{"type":"link","url":"https://github.com/poldrack/mdnewline/","position":{"start":{"line":1328,"column":1},"end":{"line":1328,"column":1}},"children":[{"type":"text","value":"mdnewline","position":{"start":{"line":1328,"column":1},"end":{"line":1328,"column":1}},"key":"d9udGEXj1I"}],"urlSource":"https://github.com/poldrack/mdnewline/","error":true,"key":"vOoCi8n5mA"},{"type":"text","value":".\nWe start by going to the GitHub Actions tab in the repository, and selecting the “Python Package” option, which creates a workflow that builds and tests a Python package.\nGenerating the workflow results in a file that contains a description of the workflow, located at ","position":{"start":{"line":1328,"column":1},"end":{"line":1328,"column":1}},"key":"hHJghYwHB5"},{"type":"inlineCode","value":".github/workflows/python-package.yml","position":{"start":{"line":1328,"column":1},"end":{"line":1328,"column":1}},"key":"fBXnJDy71e"},{"type":"text","value":".\nLooking more closely at the workflow file, we can see how it works.\nThe first section specifies the name of the workflow, and defines the events that will trigger the workflow:","position":{"start":{"line":1328,"column":1},"end":{"line":1328,"column":1}},"key":"AlDrBtgrd5"}],"key":"nvyvPhd1XR"},{"type":"code","lang":"yaml","value":"name: Python package\n\non:\n  push:\n    branches: [ \"main\" ]\n  pull_request:\n    branches: [ \"main\" ]\n","position":{"start":{"line":1334,"column":1},"end":{"line":1343,"column":1}},"key":"rwsf70r7FV"},{"type":"paragraph","position":{"start":{"line":1345,"column":1},"end":{"line":1346,"column":1}},"children":[{"type":"text","value":"This section specifies that the workflow will be run any time there is a push or a pull request to the main branch of the repo.\nThe next section sets up workflow jobs:","position":{"start":{"line":1345,"column":1},"end":{"line":1345,"column":1}},"key":"d8XVvkxtBC"}],"key":"odmie7nlwq"},{"type":"code","lang":"yaml","value":"jobs:\n  build:\n    name: python-build\n    runs-on: ubuntu-latest\n    strategy:\n      matrix:\n        python-version: [\"3.12\", \"3.13\"]","position":{"start":{"line":1348,"column":1},"end":{"line":1356,"column":1}},"key":"akQ5rYYYiP"},{"type":"paragraph","position":{"start":{"line":1358,"column":1},"end":{"line":1359,"column":1}},"children":[{"type":"text","value":"The ","position":{"start":{"line":1358,"column":1},"end":{"line":1358,"column":1}},"key":"h7iIwet2UF"},{"type":"inlineCode","value":"runs-on","position":{"start":{"line":1358,"column":1},"end":{"line":1358,"column":1}},"key":"pdCVUivTfD"},{"type":"text","value":" argument tells the workflow runner which virtual machine to use to run the workflows; in this case, we will use the default which is the latest release of Ubuntu Linux.\nWe also tell it which versions of Python we want to test the code on, updating it to test on all current versions that are compatible with the package requirements (Python >= 3.12).","position":{"start":{"line":1358,"column":1},"end":{"line":1358,"column":1}},"key":"ssW7MXM3Ol"}],"key":"b1hBRzG2AP"},{"type":"paragraph","position":{"start":{"line":1361,"column":1},"end":{"line":1363,"column":1}},"children":[{"type":"text","value":"We then specify the actual steps in the workflow.\nThe first steps set up the workflow runner so that it can check out the repository, and set up the Python installation.\nSince this project uses ","position":{"start":{"line":1361,"column":1},"end":{"line":1361,"column":1}},"key":"SXwUOZzmQ3"},{"type":"inlineCode","value":"uv","position":{"start":{"line":1361,"column":1},"end":{"line":1361,"column":1}},"key":"X4iDPoV7LW"},{"type":"text","value":" to manage packages, we will use the recommended setup code from the ","position":{"start":{"line":1361,"column":1},"end":{"line":1361,"column":1}},"key":"y2CzfftOv6"},{"type":"link","url":"https://docs.astral.sh/uv/guides/integration/github/","position":{"start":{"line":1361,"column":1},"end":{"line":1361,"column":1}},"children":[{"type":"text","value":"uv documentation","position":{"start":{"line":1361,"column":1},"end":{"line":1361,"column":1}},"key":"TshMfT7g6D"}],"urlSource":"https://docs.astral.sh/uv/guides/integration/github/","key":"V1R5msYsDm"},{"type":"text","value":" for multiple Python versions:","position":{"start":{"line":1361,"column":1},"end":{"line":1361,"column":1}},"key":"s5SCXMntsu"}],"key":"DCXhLZ1Kg5"},{"type":"code","lang":"yaml","value":"    steps:\n      - uses: actions/checkout@v4\n\n      - name: Install uv and set the python version\n        uses: astral-sh/setup-uv@v6\n        with:\n          python-version: ${{ matrix.python-version }}","position":{"start":{"line":1365,"column":1},"end":{"line":1373,"column":1}},"key":"pcdghRQmar"},{"type":"paragraph","position":{"start":{"line":1375,"column":1},"end":{"line":1376,"column":1}},"children":[{"type":"text","value":"This installs uv with the appropriate python version.\nWe can then install the project using ","position":{"start":{"line":1375,"column":1},"end":{"line":1375,"column":1}},"key":"PAjUBMvAgV"},{"type":"inlineCode","value":"uv sync","position":{"start":{"line":1375,"column":1},"end":{"line":1375,"column":1}},"key":"oNRAApfcmb"},{"type":"text","value":", pip install the package within the ","position":{"start":{"line":1375,"column":1},"end":{"line":1375,"column":1}},"key":"BSjomN0CBX"},{"type":"inlineCode","value":"uv","position":{"start":{"line":1375,"column":1},"end":{"line":1375,"column":1}},"key":"n3PEY53PJ5"},{"type":"text","value":" environment, and run the tests:","position":{"start":{"line":1375,"column":1},"end":{"line":1375,"column":1}},"key":"k7lkO6Bcw8"}],"key":"znDPaQ8OLU"},{"type":"code","lang":"yaml","value":"      - name: Install the project\n        run: uv sync --locked --all-extras --dev\n\n      - name: pip install the package\n        run: uv pip install .\n\n      - name: Run tests\n        # For example, using `pytest`\n        run: uv run pytest tests","position":{"start":{"line":1378,"column":1},"end":{"line":1388,"column":1}},"key":"BlyC775Kfc"},{"type":"paragraph","position":{"start":{"line":1390,"column":1},"end":{"line":1391,"column":1}},"children":[{"type":"text","value":"When we commit and push this workflow file, it is automatically run by Github Actions.\nIf we got to the Actions tab in the repository, we will see that the tests failed, and by looking at the logs we can see that the ","position":{"start":{"line":1390,"column":1},"end":{"line":1390,"column":1}},"key":"RgXaj8zHke"},{"type":"inlineCode","value":"uv","position":{"start":{"line":1390,"column":1},"end":{"line":1390,"column":1}},"key":"j3pmlAGyut"},{"type":"text","value":" installation process failed:","position":{"start":{"line":1390,"column":1},"end":{"line":1390,"column":1}},"key":"iKU7KzaaOt"}],"key":"GkAuxHGrBo"},{"type":"image","url":"/build/github_actions_failu-25d4b04ead335da5128a6f4198042f8f.png","alt":"Github actions failure","position":{"start":{"line":1393,"column":1},"end":{"line":1393,"column":1}},"key":"efO71bbKDe","urlSource":"images/github_actions_failure.png"},{"type":"paragraph","position":{"start":{"line":1395,"column":1},"end":{"line":1395,"column":1}},"children":[{"type":"text","value":"After fixing the ","position":{"start":{"line":1395,"column":1},"end":{"line":1395,"column":1}},"key":"GDvzwIApGh"},{"type":"inlineCode","value":"uv","position":{"start":{"line":1395,"column":1},"end":{"line":1395,"column":1}},"key":"fY80lmXDnq"},{"type":"text","value":" command, we now get the green light!:","position":{"start":{"line":1395,"column":1},"end":{"line":1395,"column":1}},"key":"QIsMC0WLSO"}],"key":"P568m2Hs7T"},{"type":"image","url":"/build/github_actions_succe-86f09e52a44c71973d502d66b668c8ba.png","alt":"Github actions success","position":{"start":{"line":1397,"column":1},"end":{"line":1397,"column":1}},"key":"IDskqMOSWi","urlSource":"images/github_actions_success.png"},{"type":"paragraph","position":{"start":{"line":1399,"column":1},"end":{"line":1399,"column":1}},"children":[{"type":"text","value":"It’s nice to advertise our testing to the world, which we can do by ","position":{"start":{"line":1399,"column":1},"end":{"line":1399,"column":1}},"key":"LYylIdptOy"},{"type":"link","url":"https://docs.github.com/en/actions/how-tos/monitor-workflows/add-a-status-badge","position":{"start":{"line":1399,"column":1},"end":{"line":1399,"column":1}},"children":[{"type":"text","value":"adding a status badge","position":{"start":{"line":1399,"column":1},"end":{"line":1399,"column":1}},"key":"POCvUpgYDc"}],"urlSource":"https://docs.github.com/en/actions/how-tos/monitor-workflows/add-a-status-badge","key":"nv0p9G3BkE"},{"type":"text","value":" to our home page, using the markdown generated by Actions for us:","position":{"start":{"line":1399,"column":1},"end":{"line":1399,"column":1}},"key":"EklElRnjH4"}],"key":"aT3eyXzYvi"},{"type":"image","url":"/build/actions_status_badge-142358ebfe5a2a6fc42cfdf397b9d882.png","alt":"Github actions status badge","position":{"start":{"line":1401,"column":1},"end":{"line":1401,"column":1}},"key":"s8lvipLa0O","urlSource":"images/actions_status_badge.png"},{"type":"heading","depth":2,"position":{"start":{"line":1404,"column":1},"end":{"line":1404,"column":1}},"children":[{"type":"text","value":"Optimizing the testing workflow","position":{"start":{"line":1404,"column":1},"end":{"line":1404,"column":1}},"key":"zjMxibFaUH"}],"identifier":"optimizing-the-testing-workflow","label":"Optimizing the testing workflow","html_id":"optimizing-the-testing-workflow","implicit":true,"key":"iuTms62kCe"},{"type":"paragraph","position":{"start":{"line":1406,"column":1},"end":{"line":1409,"column":1}},"children":[{"type":"text","value":"As a project becomes larger and more complex, the tests will necessarily take longer to run - and whenever the tests take too long, it’s likely that you won’t run them as often.\nThis is particularly the case for data analysis tools, where testing the code on real data can take a very long time.\nAs an example, our lab develops a tool for fMRI data preprocessing called ","position":{"start":{"line":1406,"column":1},"end":{"line":1406,"column":1}},"key":"gvIjbxqjc2"},{"type":"link","url":"https://fmriprep.org/en/stable/","position":{"start":{"line":1406,"column":1},"end":{"line":1406,"column":1}},"children":[{"type":"text","value":"fMRIPrep","position":{"start":{"line":1406,"column":1},"end":{"line":1406,"column":1}},"key":"LJpk1SY2TY"}],"urlSource":"https://fmriprep.org/en/stable/","key":"Awa0XMfYvy"},{"type":"text","value":" that performs a large set of operations on functional MRI datasets.\nWithout optimization, running the full test suite with real data would take roughly two hours, whereas with optimization we can complete it in 10-15 minutes.","position":{"start":{"line":1406,"column":1},"end":{"line":1406,"column":1}},"key":"tckJ36a5yz"}],"key":"VAGbdhiDSK"},{"type":"paragraph","position":{"start":{"line":1411,"column":1},"end":{"line":1411,"column":1}},"children":[{"type":"text","value":"There are a number of strategies to optimize one’s testing workflows.","position":{"start":{"line":1411,"column":1},"end":{"line":1411,"column":1}},"key":"p9Z4Ef7MEt"}],"key":"a9bPxHyX7K"},{"type":"heading","depth":3,"position":{"start":{"line":1413,"column":1},"end":{"line":1413,"column":1}},"children":[{"type":"text","value":"Cherry-picking specific tests","position":{"start":{"line":1413,"column":1},"end":{"line":1413,"column":1}},"key":"EA0nekycVH"}],"identifier":"cherry-picking-specific-tests","label":"Cherry-picking specific tests","html_id":"cherry-picking-specific-tests","implicit":true,"key":"rCe2rJUQHW"},{"type":"paragraph","position":{"start":{"line":1415,"column":1},"end":{"line":1417,"column":1}},"children":[{"type":"text","value":"When developing a new function, it’s usually sufficient to run only the tests that directly address that function rather than running the entire test suite.\nIf all of the tests for a specific function are located within a single test file, then one can simply call ","position":{"start":{"line":1415,"column":1},"end":{"line":1415,"column":1}},"key":"PUGcMQHuVL"},{"type":"inlineCode","value":"pytest","position":{"start":{"line":1415,"column":1},"end":{"line":1415,"column":1}},"key":"II2j9pwfo6"},{"type":"text","value":" with that file.\nIt’s also possible to run a specific test within a file by referring to a specific class or function using a double-colon marker:","position":{"start":{"line":1415,"column":1},"end":{"line":1415,"column":1}},"key":"YjePQcH17A"}],"key":"LIUWfWahJ6"},{"type":"code","lang":"bash","value":"❯ pytest tests/textmining/test_textmining.py::test_parse_year_from_Pubmed_record\n\n============================= test session starts ==============================\ncollected 1 item\n\ntests/textmining/test_textmining.py .                                    [100%]\n============================== 1 passed in 0.17s ===============================","position":{"start":{"line":1419,"column":1},"end":{"line":1427,"column":1}},"key":"VbSbWz2wgz"},{"type":"paragraph","position":{"start":{"line":1429,"column":1},"end":{"line":1429,"column":1}},"children":[{"type":"text","value":"This allows one to focus on the tests that are immediately relevant to a specific development task.","position":{"start":{"line":1429,"column":1},"end":{"line":1429,"column":1}},"key":"Cwr9IDOAiv"}],"key":"BR1a407R49"},{"type":"heading","depth":3,"position":{"start":{"line":1431,"column":1},"end":{"line":1431,"column":1}},"children":[{"type":"text","value":"Controlling test execution order","position":{"start":{"line":1431,"column":1},"end":{"line":1431,"column":1}},"key":"Azw9cV1NYQ"}],"identifier":"controlling-test-execution-order","label":"Controlling test execution order","html_id":"controlling-test-execution-order","implicit":true,"key":"TaGGQGRZ0d"},{"type":"paragraph","position":{"start":{"line":1433,"column":1},"end":{"line":1434,"column":1}},"children":[{"type":"text","value":"Several of the strategies described below require the ability to execute tests in specific orders.\nThere are several strategies one can use to do this.","position":{"start":{"line":1433,"column":1},"end":{"line":1433,"column":1}},"key":"gE1z6VytMi"}],"key":"EyTlAA3lom"},{"type":"paragraph","position":{"start":{"line":1436,"column":1},"end":{"line":1437,"column":1}},"children":[{"type":"text","value":"One strategy that ","position":{"start":{"line":1436,"column":1},"end":{"line":1436,"column":1}},"key":"ylZE2Ej6j7"},{"type":"emphasis","position":{"start":{"line":1436,"column":1},"end":{"line":1436,"column":1}},"children":[{"type":"text","value":"will not","position":{"start":{"line":1436,"column":1},"end":{"line":1436,"column":1}},"key":"irMGsPzKuQ"}],"key":"nhINZ9YCJy"},{"type":"text","value":" work reliably is to place the tests in a specific order in the test file.\nWhile tests are often executed in the order that the functions appear in the file, this not guaranteed.","position":{"start":{"line":1436,"column":1},"end":{"line":1436,"column":1}},"key":"uGJSjM9xAe"}],"key":"pu2fRBkk3T"},{"type":"paragraph","position":{"start":{"line":1439,"column":1},"end":{"line":1440,"column":1}},"children":[{"type":"text","value":"For precise ordering of particular tests, one can use the ","position":{"start":{"line":1439,"column":1},"end":{"line":1439,"column":1}},"key":"v8uh1dXulw"},{"type":"link","url":"https://pypi.org/project/pytest-order/","position":{"start":{"line":1439,"column":1},"end":{"line":1439,"column":1}},"children":[{"type":"text","value":"pytest-order","position":{"start":{"line":1439,"column":1},"end":{"line":1439,"column":1}},"key":"jBr0mY4eUe"}],"urlSource":"https://pypi.org/project/pytest-order/","key":"VOZuihGKFM"},{"type":"text","value":" plugin, which allows the use of marks to specify test order","position":{"start":{"line":1439,"column":1},"end":{"line":1439,"column":1}},"key":"fDyooPVI9N"},{"type":"footnoteReference","identifier":"2","label":"2","position":{"start":{"line":1439,"column":1},"end":{"line":1439,"column":1}},"enumerator":"2","key":"af3UGpvAYQ"},{"type":"text","value":".\nWe start with two tests that are out of order in the test code:","position":{"start":{"line":1439,"column":1},"end":{"line":1439,"column":1}},"key":"FBK9HGAEY8"}],"key":"Ezhv5MbuH8"},{"type":"code","lang":"python","value":"import pytest\n\ndef test_second():\n    \"\"\"This is the second test.\"\"\"\n    assert True\n\ndef test_first():\n    \"\"\"This is the first test.\"\"\"\n    assert True","position":{"start":{"line":1442,"column":1},"end":{"line":1452,"column":1}},"key":"D9XQip8dXo"},{"type":"paragraph","position":{"start":{"line":1454,"column":1},"end":{"line":1454,"column":1}},"children":[{"type":"text","value":"Running this, we see that these are run in the order they are defined in the test code:","position":{"start":{"line":1454,"column":1},"end":{"line":1454,"column":1}},"key":"rUPeGq4yOm"}],"key":"hgP0hOveyr"},{"type":"code","lang":"bash","value":"❯ pytest -vv tests/ordering/test_order.py\n=========================== test session starts ============================\ncollected 2 items\n\ntests/ordering/test_order.py::test_second PASSED                     [ 50%]\ntests/ordering/test_order.py::test_first PASSED                      [100%]\n============================ 2 passed in 0.05s =============================","position":{"start":{"line":1456,"column":1},"end":{"line":1464,"column":1}},"key":"U1TBgUGetI"},{"type":"paragraph","position":{"start":{"line":1466,"column":1},"end":{"line":1466,"column":1}},"children":[{"type":"text","value":"After installing the ","position":{"start":{"line":1466,"column":1},"end":{"line":1466,"column":1}},"key":"cHr86EcEud"},{"type":"inlineCode","value":"pytest-order","position":{"start":{"line":1466,"column":1},"end":{"line":1466,"column":1}},"key":"SngK5YRTAa"},{"type":"text","value":" plugin, we can now add marks that control the execution order of the tests:","position":{"start":{"line":1466,"column":1},"end":{"line":1466,"column":1}},"key":"DuiaJZmKaQ"}],"key":"prrOy7WIJC"},{"type":"code","lang":"python","value":"import pytest\n\n@pytest.mark.order(2)\ndef test_second():\n    \"\"\"This is the second test.\"\"\"\n    assert True\n\n@pytest.mark.order(1)\ndef test_first():\n    \"\"\"This is the first test.\"\"\"\n    assert True","position":{"start":{"line":1468,"column":1},"end":{"line":1480,"column":1}},"key":"btEv7Ijz2z"},{"type":"paragraph","position":{"start":{"line":1482,"column":1},"end":{"line":1482,"column":1}},"children":[{"type":"text","value":"Running this, we see that these are run in the order they are defined in the test code:","position":{"start":{"line":1482,"column":1},"end":{"line":1482,"column":1}},"key":"obAYC4VNez"}],"key":"IxfHKvpIA2"},{"type":"code","lang":"bash","value":"❯ pytest -vv tests/ordering/test_order.py\n=========================== test session starts ============================\ncollected 2 items\n\ntests/ordering/test_order.py::test_first PASSED                      [ 50%]\ntests/ordering/test_order.py::test_second PASSED                     [100%]\n============================ 2 passed in 0.05s =============================","position":{"start":{"line":1484,"column":1},"end":{"line":1492,"column":1}},"key":"Ut8z2njuPt"},{"type":"paragraph","position":{"start":{"line":1494,"column":1},"end":{"line":1494,"column":1}},"children":[{"type":"text","value":"This package also provides other useful ways to control execution order, such as relative markers like “before” or “last”.","position":{"start":{"line":1494,"column":1},"end":{"line":1494,"column":1}},"key":"Vf5rb7QpN5"}],"key":"JGEHje8BNb"},{"type":"paragraph","position":{"start":{"line":1496,"column":1},"end":{"line":1497,"column":1}},"children":[{"type":"text","value":"Note that while it might be tempting to use test ordering to ensure the proper executions of tests that rely upon one another, this is generally a bad practice.\nInstead, one should generate fixtures for any dependencies that are shared by tests.","position":{"start":{"line":1496,"column":1},"end":{"line":1496,"column":1}},"key":"pt045uvK4Y"}],"key":"npXCEPyRFG"},{"type":"heading","depth":3,"position":{"start":{"line":1499,"column":1},"end":{"line":1499,"column":1}},"children":[{"type":"text","value":"Only run failed tests, or run them first","position":{"start":{"line":1499,"column":1},"end":{"line":1499,"column":1}},"key":"jQ4eOKScQ2"}],"identifier":"only-run-failed-tests-or-run-them-first","label":"Only run failed tests, or run them first","html_id":"only-run-failed-tests-or-run-them-first","implicit":true,"key":"AeM1b9hJ5H"},{"type":"paragraph","position":{"start":{"line":1501,"column":1},"end":{"line":1503,"column":1}},"children":[{"type":"text","value":"Pytest has a set of features that allow one to focus testing on those tests that have most recently failed, which can be useful in the context of test-driven development.\nThe first command, ","position":{"start":{"line":1501,"column":1},"end":{"line":1501,"column":1}},"key":"mhzH6vgYRF"},{"type":"inlineCode","value":"pytest --lf","position":{"start":{"line":1501,"column":1},"end":{"line":1501,"column":1}},"key":"JiizB0RydW"},{"type":"text","value":" (short for ","position":{"start":{"line":1501,"column":1},"end":{"line":1501,"column":1}},"key":"HeiclemscX"},{"type":"inlineCode","value":"--last-failed","position":{"start":{"line":1501,"column":1},"end":{"line":1501,"column":1}},"key":"stBuAHCxc3"},{"type":"text","value":") will check the cache of the most recent test run results, and only run those tests that failed in the most recent run.\nIt’s important to realize that if one introduces a condition that would cause a previously passed test to fail, it may not be caught, such that one could run a string of tests using the ","position":{"start":{"line":1501,"column":1},"end":{"line":1501,"column":1}},"key":"ivA1UK9W41"},{"type":"inlineCode","value":"--lf","position":{"start":{"line":1501,"column":1},"end":{"line":1501,"column":1}},"key":"pdD6Cp3wWE"},{"type":"text","value":" flag, get to the point where the tests all seem to pass, but then find that some tests that had passed earlier will fail due to conditions introduced when fixing the later tests.","position":{"start":{"line":1501,"column":1},"end":{"line":1501,"column":1}},"key":"gdHp1e47zb"}],"key":"kCLCXJYz1u"},{"type":"paragraph","position":{"start":{"line":1505,"column":1},"end":{"line":1508,"column":1}},"children":[{"type":"text","value":"A related command, ","position":{"start":{"line":1505,"column":1},"end":{"line":1505,"column":1}},"key":"oj7NeX0hJb"},{"type":"inlineCode","value":"pytest --ff","position":{"start":{"line":1505,"column":1},"end":{"line":1505,"column":1}},"key":"hR8Yabelhc"},{"type":"text","value":" (for ","position":{"start":{"line":1505,"column":1},"end":{"line":1505,"column":1}},"key":"ROABZgBNCC"},{"type":"inlineCode","value":"--failed-first","position":{"start":{"line":1505,"column":1},"end":{"line":1505,"column":1}},"key":"YYaczDg4IC"},{"type":"text","value":") will run the most recent failed tests before running the rest of the tasks.\nThis command is probably better for general use than the ","position":{"start":{"line":1505,"column":1},"end":{"line":1505,"column":1}},"key":"Naizw1aA8C"},{"type":"inlineCode","value":"--lf","position":{"start":{"line":1505,"column":1},"end":{"line":1505,"column":1}},"key":"dKtqHW5hUq"},{"type":"text","value":" flag, since it ensures that all tests are run, but prioritizes the running of the failing tests first.\nThis might be an option that one would consider using by default, which can be achieved by adding the option to one of the relevant configuration files.\nIn our case, this would be ","position":{"start":{"line":1505,"column":1},"end":{"line":1505,"column":1}},"key":"plTfqmw58d"},{"type":"inlineCode","value":"pyproject.toml","position":{"start":{"line":1505,"column":1},"end":{"line":1505,"column":1}},"key":"VwTAOTswmm"},{"type":"text","value":", where we could add:","position":{"start":{"line":1505,"column":1},"end":{"line":1505,"column":1}},"key":"OGxmgTdxEA"}],"key":"FezG2VjQFY"},{"type":"code","lang":"toml","value":"[tool.pytest.ini_options]\naddopts = \"--ff\"","position":{"start":{"line":1510,"column":1},"end":{"line":1513,"column":1}},"key":"Eqe9wXzgTP"},{"type":"heading","depth":3,"position":{"start":{"line":1515,"column":1},"end":{"line":1515,"column":1}},"children":[{"type":"text","value":"Separate unit tests from longer-running tests","position":{"start":{"line":1515,"column":1},"end":{"line":1515,"column":1}},"key":"CSwtlnAINM"}],"identifier":"separate-unit-tests-from-longer-running-tests","label":"Separate unit tests from longer-running tests","html_id":"separate-unit-tests-from-longer-running-tests","implicit":true,"key":"MGzKOkIVBh"},{"type":"paragraph","position":{"start":{"line":1517,"column":1},"end":{"line":1519,"column":1}},"children":[{"type":"text","value":"If we are writing short functions (which we are, right?), then our unit tests should usually run fairly quickly, whereas our integration tests (which integrate across a potentially large number of modules) may take much longer.\nOne thing we can do is to run only unit tests while we are developing, while having the integration tests run less frequently.\nFor example, we could have the unit tests run any time we commit our code (for example, by running them as pre-commit items), while having the integration tests run automatically on our CI system when the code is actually pushed to the GitHub repository.","position":{"start":{"line":1517,"column":1},"end":{"line":1517,"column":1}},"key":"FFqjpWPWr9"}],"key":"PUYtUuWNU4"},{"type":"paragraph","position":{"start":{"line":1521,"column":1},"end":{"line":1522,"column":1}},"children":[{"type":"text","value":"One way to accomplish this is to use pytest markers (as we saw earlier) to mark groups of tests.\nWe first need to define our own custom markers in our ","position":{"start":{"line":1521,"column":1},"end":{"line":1521,"column":1}},"key":"zi9ENQGEge"},{"type":"inlineCode","value":"pyproject.toml","position":{"start":{"line":1521,"column":1},"end":{"line":1521,"column":1}},"key":"dxkQGzJub8"},{"type":"text","value":" file:","position":{"start":{"line":1521,"column":1},"end":{"line":1521,"column":1}},"key":"nV0MoEhLOQ"}],"key":"CvZAa1Ik7e"},{"type":"code","lang":"toml","value":"[tool.pytest.ini_options]\nmarkers = [\n    \"unit: mark a test as a unit test.\",\n    \"integration: mark a test as an integration test.\"\n]","position":{"start":{"line":1524,"column":1},"end":{"line":1530,"column":1}},"key":"RIMpDJ2CUq"},{"type":"paragraph","position":{"start":{"line":1532,"column":1},"end":{"line":1532,"column":1}},"children":[{"type":"text","value":"We can then generate tests using these markers:","position":{"start":{"line":1532,"column":1},"end":{"line":1532,"column":1}},"key":"KXO0p7FYl8"}],"key":"SeIjH5cBUV"},{"type":"code","lang":"python","value":"import pytest\n\n@pytest.mark.unit\ndef test_unit1():\n    \"\"\"This is a unit test.\"\"\"\n    assert True\n\n@pytest.mark.unit\ndef test_unit2():\n    \"\"\"This is a unit test.\"\"\"\n    assert True\n\n@pytest.mark.integration\ndef test_integration():\n    \"\"\"This is an integration test.\"\"\"\n    assert True","position":{"start":{"line":1534,"column":1},"end":{"line":1551,"column":1}},"key":"dkMx4XYZ5J"},{"type":"paragraph","position":{"start":{"line":1553,"column":1},"end":{"line":1553,"column":1}},"children":[{"type":"text","value":"Running these using the standard pytest command, both tests are executed:","position":{"start":{"line":1553,"column":1},"end":{"line":1553,"column":1}},"key":"b4EtYhEHwl"}],"key":"CnpiJeFuUP"},{"type":"code","lang":"bash","value":"❯ pytest -vv tests/markers/test_markers.py\n=========================== test session starts ============================\ncollected 3 items\n\ntests/markers/test_markers.py::test_unit1 PASSED                     [ 33%]\ntests/markers/test_markers.py::test_unit2 PASSED                     [ 66%]\ntests/markers/test_markers.py::test_integration PASSED               [100%]\n============================ 3 passed in 0.05s =============================","position":{"start":{"line":1555,"column":1},"end":{"line":1564,"column":1}},"key":"vh0GIOTCpT"},{"type":"paragraph","position":{"start":{"line":1566,"column":1},"end":{"line":1566,"column":1}},"children":[{"type":"text","value":"However, we can also specify that only the unit tests should be executed, using the ","position":{"start":{"line":1566,"column":1},"end":{"line":1566,"column":1}},"key":"vGLkopHaeL"},{"type":"inlineCode","value":"-m","position":{"start":{"line":1566,"column":1},"end":{"line":1566,"column":1}},"key":"aUKXTqeBOA"},{"type":"text","value":" flag to deselect all other flags:","position":{"start":{"line":1566,"column":1},"end":{"line":1566,"column":1}},"key":"NDTuClQOMf"}],"key":"UvbY68Pw7L"},{"type":"code","lang":"bash","value":"❯ pytest -vv -m unit tests/markers/test_markers.py\n=========================== test session starts ============================\ncollected 3 items / 1 deselected / 2 selected\n\ntests/markers/test_markers.py::test_unit1 PASSED                     [ 50%]\ntests/markers/test_markers.py::test_unit2 PASSED                     [100%]\n===================== 2 passed, 1 deselected in 0.05s ======================\n","position":{"start":{"line":1568,"column":1},"end":{"line":1577,"column":1}},"key":"FgZ0JHovyO"},{"type":"paragraph","position":{"start":{"line":1579,"column":1},"end":{"line":1579,"column":1}},"children":[{"type":"text","value":"We could also run all tests that to not match a particular marker:","position":{"start":{"line":1579,"column":1},"end":{"line":1579,"column":1}},"key":"GjsA1cuTeH"}],"key":"aNFDqMyYk3"},{"type":"code","lang":"bash","value":"❯ pytest -vv -m \"not unit\" tests/markers/test_markers.py\n=========================== test session starts ============================\ncollected 3 items / 2 deselected / 1 selected\n\ntests/markers/test_markers.py::test_integration PASSED               [100%]\n===================== 1 passed, 2 deselected in 0.05s ======================","position":{"start":{"line":1581,"column":1},"end":{"line":1588,"column":1}},"key":"oBoMXqbTc7"},{"type":"heading","depth":3,"position":{"start":{"line":1591,"column":1},"end":{"line":1591,"column":1}},"children":[{"type":"text","value":"Longest tests last","position":{"start":{"line":1591,"column":1},"end":{"line":1591,"column":1}},"key":"dgrN7PplJC"}],"identifier":"longest-tests-last","label":"Longest tests last","html_id":"longest-tests-last","implicit":true,"key":"dEVeIGxyyS"},{"type":"paragraph","position":{"start":{"line":1593,"column":1},"end":{"line":1594,"column":1}},"children":[{"type":"text","value":"Another simple strategy that can help optimize the testing workflow is to run tests in order to the time for completion.\nThis ensures that one doesn’t end up waiting a long time for long-running tests to complete, only to find that a quick test fails.","position":{"start":{"line":1593,"column":1},"end":{"line":1593,"column":1}},"key":"XNrxnKRgMm"}],"key":"IBdwZt2Leb"},{"type":"paragraph","position":{"start":{"line":1596,"column":1},"end":{"line":1597,"column":1}},"children":[{"type":"text","value":"We can use pytest’s ","position":{"start":{"line":1596,"column":1},"end":{"line":1596,"column":1}},"key":"weubUvtVsD"},{"type":"inlineCode","value":"--durations","position":{"start":{"line":1596,"column":1},"end":{"line":1596,"column":1}},"key":"QmSraN2EaG"},{"type":"text","value":" flag to get the durations for each test.\nHere is an example with three tests with different durations to completion:","position":{"start":{"line":1596,"column":1},"end":{"line":1596,"column":1}},"key":"VABBkTDC51"}],"key":"vK1egnatqM"},{"type":"code","lang":"python","value":"import pytest\nfrom time import sleep\n\ndef test_duration_3():\n    sleep(3)\n    assert True\n\ndef test_duration_5():\n    sleep(5)\n    assert True\n\ndef test_duration_1():\n    sleep(1)\n    assert True","position":{"start":{"line":1599,"column":1},"end":{"line":1614,"column":1}},"key":"CyfUxyurYm"},{"type":"paragraph","position":{"start":{"line":1616,"column":1},"end":{"line":1616,"column":1}},"children":[{"type":"text","value":"We can call pytest with ","position":{"start":{"line":1616,"column":1},"end":{"line":1616,"column":1}},"key":"bQdQH2bwwh"},{"type":"inlineCode","value":"--durations=0","position":{"start":{"line":1616,"column":1},"end":{"line":1616,"column":1}},"key":"hWk7pRkWpI"},{"type":"text","value":" which will return the duration for all of the tests:","position":{"start":{"line":1616,"column":1},"end":{"line":1616,"column":1}},"key":"SJKD3VU2On"}],"key":"jasLYjJrq0"},{"type":"code","lang":"bash","value":"❯ pytest --durations=0 tests/ordering/test_duration.py\n============================= test session starts ==============================\ncollected 3 items\n\ntests/ordering/test_duration.py ...                                      [100%]\n\n============================== slowest durations ===============================\n5.00s call     tests/ordering/test_duration.py::test_duration_5\n3.01s call     tests/ordering/test_duration.py::test_duration_3\n1.00s call     tests/ordering/test_duration.py::test_duration_1\n============================== 3 passed in 9.10s ===============================","position":{"start":{"line":1618,"column":1},"end":{"line":1630,"column":1}},"key":"EGFjWd71Ho"},{"type":"paragraph","position":{"start":{"line":1632,"column":1},"end":{"line":1632,"column":1}},"children":[{"type":"text","value":"Using the results of this, we could impose ordering on test execution so that the slowest ones are executed last, or add a ","position":{"start":{"line":1632,"column":1},"end":{"line":1632,"column":1}},"key":"kwvhsBcogd"},{"type":"inlineCode","value":"slow","position":{"start":{"line":1632,"column":1},"end":{"line":1632,"column":1}},"key":"MNPSPto74I"},{"type":"text","value":" marker that we could exclude from our regular testing.","position":{"start":{"line":1632,"column":1},"end":{"line":1632,"column":1}},"key":"CXQg5oQBB5"}],"key":"N3h0NHplbf"},{"type":"paragraph","position":{"start":{"line":1634,"column":1},"end":{"line":1639,"column":1}},"children":[{"type":"text","value":"There are plugins for ","position":{"start":{"line":1634,"column":1},"end":{"line":1634,"column":1}},"key":"Ld7QeR9uly"},{"type":"inlineCode","value":"pytest","position":{"start":{"line":1634,"column":1},"end":{"line":1634,"column":1}},"key":"lFCr5Bc5kG"},{"type":"text","value":" that can measure the duration of each test and order tests accordingly, but they don’t seem to be very well maintained so I am avoiding them.\nHowever, with agentic AI coding tools we can have our AI system make the appropriate call to pytest to obtain the durations and then add the appropriate ordering markers.\nI tried this using the agentic chat window in VSCode (with Claude Sonnet 4).\nOn the first pass, Claude immediately noticed that it could infer the completion times directly from the ","position":{"start":{"line":1634,"column":1},"end":{"line":1634,"column":1}},"key":"j0SaByjOIy"},{"type":"inlineCode","value":"sleep()","position":{"start":{"line":1634,"column":1},"end":{"line":1634,"column":1}},"key":"gBjsoHEzEE"},{"type":"text","value":" commands in the code.\nI asked it not to do this, and instead to use the outputs from ","position":{"start":{"line":1634,"column":1},"end":{"line":1634,"column":1}},"key":"FcZPDOhXHM"},{"type":"inlineCode","value":"pytest --durations","position":{"start":{"line":1634,"column":1},"end":{"line":1634,"column":1}},"key":"SBykVKYnDU"},{"type":"text","value":" but it had trouble waiting for the tests to finish, and ended up continuing to try to things that didn’t work.\nHowever, Claude Code was able to successfully solve the problem; the first line marked with “>” was my prompt, and the remainder is Claude Code’s output:","position":{"start":{"line":1634,"column":1},"end":{"line":1634,"column":1}},"key":"yt8EhAwti9"}],"key":"GiU04orUFB"},{"type":"code","lang":"","value":"> the file tests/ordering/test_duration.py contains several\n  tests.  Use pytest --duration to determine the durations for\n  each of the tests in this file. DO NOT try to infer the \n  durations from the code itself. The tests will take about\n  fifteen seconds to complete, so be sure to wait for them. \n  Then, add marks to each test using pytest-order to specify \n  that the fastest tests are executed first.\n\n⏺ I'll run pytest with the --durations flag to measure the \nactual test durations, then add pytest-order marks to run the\nfastest tests first.\n\n⏺ Update Todos\n  ⎿  ☐ Run pytest with --durations flag on test_duration.py\n     ☐ Analyze test duration results\n     ☐ Add pytest-order marks to tests based on duration\n\n⏺ Bash(pytest tests/ordering/test_duration.py --durations=0 -v)\n  ⎿ ============================= test session starts =====================\n    =========\n    platform darwin -- Python 3.12.0, pytest-8.4.1, pluggy-1.5.0 -- /Users/\n    … +27 lines (ctrl+r to expand)\n\n⏺ Based on the test durations:\n  - test_duration_1: 1.00s (fastest)\n  - test_duration_3: 3.00s\n  - test_duration_5: 5.01s (slowest)\n\n⏺ Now I'll add pytest-order marks to run tests from fastest to slowest:\n\n⏺ Update(tests/ordering/test_duration.py)\n  ⎿  Updated tests/ordering/test_duration.py with 3 additions\n        2    from time import sleep\n        3\n        4\n        5 +  @pytest.mark.order(2)\n        6    def test_duration_3():\n        7        sleep(3)\n        8        assert True\n        9\n       10 +  @pytest.mark.order(3)\n       11    def test_duration_5():\n       12        sleep(5)\n       13        assert True\n       14\n       15 +  @pytest.mark.order(1)\n       16    def test_duration_1():\n       17        sleep(1)\n       18        assert True\n\n⏺ Let me verify the ordering works correctly:\n\n⏺ Bash(pytest tests/ordering/test_duration.py -v)\n  ⎿ ============================= test session starts =====================\n    =========\n    platform darwin -- Python 3.12.0, pytest-8.4.1, pluggy-1.5.0 -- /Users/\n    … +20 lines (ctrl+r to expand)\n\n⏺ Perfect! The tests now run in order from fastest to slowest:\n  1. test_duration_1 (1 second)\n  2. test_duration_3 (3 seconds)\n  3. test_duration_5 (5 seconds)\n","position":{"start":{"line":1641,"column":1},"end":{"line":1705,"column":1}},"key":"qGbbB5bVil"},{"type":"paragraph","position":{"start":{"line":1707,"column":1},"end":{"line":1707,"column":1}},"children":[{"type":"text","value":"This is a great example of how we can increasingly rely upon AI coding models to solve problems that would once have required specialized tools.","position":{"start":{"line":1707,"column":1},"end":{"line":1707,"column":1}},"key":"SBQEt865C4"}],"key":"AFmSG1pMWh"},{"type":"heading","depth":3,"position":{"start":{"line":1709,"column":1},"end":{"line":1709,"column":1}},"children":[{"type":"text","value":"Using minimal mock datasets","position":{"start":{"line":1709,"column":1},"end":{"line":1709,"column":1}},"key":"NjvDXkV84p"}],"identifier":"using-minimal-mock-datasets","label":"Using minimal mock datasets","html_id":"using-minimal-mock-datasets","implicit":true,"key":"Skv53jy9xR"},{"type":"paragraph","position":{"start":{"line":1711,"column":1},"end":{"line":1715,"column":1}},"children":[{"type":"text","value":"For code that performs data processing operations, the processing of full size datasets can often take a very long time.\nOne strategy in these cases is to generate minimal mock datasets that can exercise the functions without taking the full amount of time that a real dataset would.\nIn the tests for ","position":{"start":{"line":1711,"column":1},"end":{"line":1711,"column":1}},"key":"MKvRTdmEJq"},{"type":"inlineCode","value":"fMRIPrep","position":{"start":{"line":1711,"column":1},"end":{"line":1711,"column":1}},"key":"l9pgR7pXd3"},{"type":"text","value":", we use fMRI datasets that have been reduced in length, and structural MRI datasets that have been downsampled to reduce their spatial resolution.\nThe specific way to reduce the dataset will depend on the particular processes being run.\nFor example, downsampling the data too much for MRI preprocessing would likely cause some operations to fail, so one needs to have a good intuition for the data requirements for the relevant code.","position":{"start":{"line":1711,"column":1},"end":{"line":1711,"column":1}},"key":"i7tmhXeVZW"}],"key":"DhhxsmnBht"},{"type":"heading","depth":3,"position":{"start":{"line":1717,"column":1},"end":{"line":1717,"column":1}},"children":[{"type":"text","value":"Adding minimal processing modes for integration tests","position":{"start":{"line":1717,"column":1},"end":{"line":1717,"column":1}},"key":"rwhDb9CCEY"}],"identifier":"adding-minimal-processing-modes-for-integration-tests","label":"Adding minimal processing modes for integration tests","html_id":"adding-minimal-processing-modes-for-integration-tests","implicit":true,"key":"uv759CBDMM"},{"type":"paragraph","position":{"start":{"line":1719,"column":1},"end":{"line":1722,"column":1}},"children":[{"type":"text","value":"When the goal is to the test the integration of components rather than the function of each component, one way to minimize testing time is to provide configuration features that minimize execution time for the component.\nFor example, in ","position":{"start":{"line":1719,"column":1},"end":{"line":1719,"column":1}},"key":"wG9pQbNoJD"},{"type":"inlineCode","value":"fMRIPrep","position":{"start":{"line":1719,"column":1},"end":{"line":1719,"column":1}},"key":"VRXfmOEavh"},{"type":"text","value":" there are a number of steps that involve optimization processes that take time to converge.\nHowever, there the package has a “sloppy mode” configuration flag that one can turn on for testing, which provides a more lenient threshold for convergence of those operations, allowing them to finish faster.\nAgain, knowing where one can cut corners requires a good understanding of the specific requirements of the processing operations.","position":{"start":{"line":1719,"column":1},"end":{"line":1719,"column":1}},"key":"kPRLcIWhcp"}],"key":"xoOLPiYuip"},{"type":"heading","depth":3,"position":{"start":{"line":1724,"column":1},"end":{"line":1724,"column":1}},"children":[{"type":"text","value":"Parallelizing testing","position":{"start":{"line":1724,"column":1},"end":{"line":1724,"column":1}},"key":"OjlF5ke5OD"}],"identifier":"parallelizing-testing","label":"Parallelizing testing","html_id":"parallelizing-testing","implicit":true,"key":"UlK9bcgLKP"},{"type":"paragraph","position":{"start":{"line":1726,"column":1},"end":{"line":1728,"column":1}},"children":[{"type":"text","value":"If we have written good tests, they should be able to run independently, and thus their execution should be parallelizable, assuming that we are using a system with multiple CPU cores.\nIf we are using ","position":{"start":{"line":1726,"column":1},"end":{"line":1726,"column":1}},"key":"OapFjSjApL"},{"type":"inlineCode","value":"pytest","position":{"start":{"line":1726,"column":1},"end":{"line":1726,"column":1}},"key":"W1d06SH2qY"},{"type":"text","value":" as our testing framework, then we can use the ","position":{"start":{"line":1726,"column":1},"end":{"line":1726,"column":1}},"key":"D4fR9tcgjO"},{"type":"inlineCode","value":"pytest-xdist","position":{"start":{"line":1726,"column":1},"end":{"line":1726,"column":1}},"key":"Hwqp3nynVa"},{"type":"text","value":" extension to enable the parallel execution of tests in pytest.\nFor example, let’s set up a parameterized test that includes a ","position":{"start":{"line":1726,"column":1},"end":{"line":1726,"column":1}},"key":"yQuMpSUF96"},{"type":"inlineCode","value":"time.sleep()","position":{"start":{"line":1726,"column":1},"end":{"line":1726,"column":1}},"key":"K7QqIHZloG"},{"type":"text","value":" command so that execution will take a significant amount of time.","position":{"start":{"line":1726,"column":1},"end":{"line":1726,"column":1}},"key":"OKO37r0nsk"}],"key":"aoltsBw1Ey"},{"type":"code","lang":"python","value":"import pytest\nimport time\n\n@pytest.mark.parametrize(\"x\", range(10))\ndef test_parallel(x):\n    assert x in range(10), f\"Value {x} is not in the expected list.\"\n    time.sleep(1) # wait for one second","position":{"start":{"line":1730,"column":1},"end":{"line":1738,"column":1}},"key":"ZPpIwmtik7"},{"type":"paragraph","position":{"start":{"line":1739,"column":1},"end":{"line":1739,"column":1}},"children":[{"type":"text","value":"If we run this using the standard ","position":{"start":{"line":1739,"column":1},"end":{"line":1739,"column":1}},"key":"qkqqGisLBf"},{"type":"inlineCode","value":"pytest","position":{"start":{"line":1739,"column":1},"end":{"line":1739,"column":1}},"key":"iZGw6xjxMq"},{"type":"text","value":" command, we should see that it takes about ten seconds, given that there are ten tests:","position":{"start":{"line":1739,"column":1},"end":{"line":1739,"column":1}},"key":"HfqEuCcc8z"}],"key":"zdlXbFXfGE"},{"type":"code","lang":"bash","value":"❯ pytest tests/parallel/tests_parallel.py\n============================= test session starts ==============================\ncollected 10 items\n\ntests/parallel/tests_parallel.py ..........                              [100%]\n\n============================= 10 passed in 10.18s ==============================","position":{"start":{"line":1741,"column":1},"end":{"line":1749,"column":1}},"key":"gmcqGEuwDQ"},{"type":"paragraph","position":{"start":{"line":1751,"column":1},"end":{"line":1751,"column":1}},"children":[{"type":"text","value":"If we have installed ","position":{"start":{"line":1751,"column":1},"end":{"line":1751,"column":1}},"key":"xjoURiZoXG"},{"type":"inlineCode","value":"pytest-xdist","position":{"start":{"line":1751,"column":1},"end":{"line":1751,"column":1}},"key":"KCHMN2BEZt"},{"type":"text","value":" then we can add the ","position":{"start":{"line":1751,"column":1},"end":{"line":1751,"column":1}},"key":"jltnPRLT7Q"},{"type":"inlineCode","value":"-n auto","position":{"start":{"line":1751,"column":1},"end":{"line":1751,"column":1}},"key":"tImVj9436A"},{"type":"text","value":" flag which will automatically detect how manu CPU cores we have available and run the tests in parallel across those cores:","position":{"start":{"line":1751,"column":1},"end":{"line":1751,"column":1}},"key":"ibv7LdiBL5"}],"key":"jADhifHG3K"},{"type":"code","lang":"bash","value":"❯ pytest tests/parallel/tests_parallel.py -n auto\n============================= test session starts ==============================\n16 workers [10 items]\n..........                                                               [100%]\n============================== 10 passed in 1.97s ==============================","position":{"start":{"line":1753,"column":1},"end":{"line":1759,"column":1}},"key":"X7MTqavLVB"},{"type":"paragraph","position":{"start":{"line":1761,"column":1},"end":{"line":1761,"column":1}},"children":[{"type":"text","value":"You can see that it detected the 16 cores in my laptop and ran the 10 tests in parallel, greatly reducing the testing time.","position":{"start":{"line":1761,"column":1},"end":{"line":1761,"column":1}},"key":"RdoMflf7rb"}],"key":"jxtWiP2J80"},{"type":"footnoteDefinition","identifier":"1","label":"1","position":{"start":{"line":1761,"column":1},"end":{"line":1761,"column":1}},"children":[{"type":"paragraph","position":{"start":{"line":1763,"column":1},"end":{"line":1764,"column":1}},"children":[{"type":"text","value":"This is slightly inaccurate, because a true positive control would contain the actual virus.\nIt would be more precise to call it a “procedural control” but these seem to be also referred to as “positive controls” so I am sticking with the more understandable terminology here.","position":{"start":{"line":1763,"column":1},"end":{"line":1763,"column":1}},"key":"L7WOAtvRCo"}],"key":"DT4GnSakso"}],"enumerator":"1","key":"CGp4sThYUv"},{"type":"footnoteDefinition","identifier":"2","label":"2","position":{"start":{"line":1763,"column":1},"end":{"line":1763,"column":1}},"children":[{"type":"paragraph","position":{"start":{"line":1765,"column":1},"end":{"line":1767,"column":1}},"children":[{"type":"text","value":"As discussed in the earlier section on technical debt, I don’t think it’s generally a good policy to rely upon packages that one randomly finds on Pypi or GitHub.\nBefore recommending ","position":{"start":{"line":1765,"column":1},"end":{"line":1765,"column":1}},"key":"pRrXAQZPmc"},{"type":"inlineCode","value":"python-order","position":{"start":{"line":1765,"column":1},"end":{"line":1765,"column":1}},"key":"OBLVaTe2RB"},{"type":"text","value":" as a possible solution, I looked at its ","position":{"start":{"line":1765,"column":1},"end":{"line":1765,"column":1}},"key":"wHumpoipY9"},{"type":"link","url":"https://github.com/pytest-dev/pytest-order","position":{"start":{"line":1765,"column":1},"end":{"line":1765,"column":1}},"children":[{"type":"text","value":"GitHub page","position":{"start":{"line":1765,"column":1},"end":{"line":1765,"column":1}},"key":"oc3GZ6F8hi"}],"urlSource":"https://github.com/pytest-dev/pytest-order","error":true,"key":"gGuZZwVWYc"},{"type":"text","value":", where I saw that it appears to be a well-maintained and currently active package, with recent commits and solid handling of issues.\nConversely, during the course of writing I came across a number of other packages that had been recommended on Stack Overflow to solve various problems, some of which had not seen commits in several years or had longstanding unaddressed issues.","position":{"start":{"line":1765,"column":1},"end":{"line":1765,"column":1}},"key":"DLuRGUho2l"}],"key":"nwLfAHnXv6"}],"enumerator":"2","key":"nanrWDfjav"}],"key":"OraFcwN1od"}],"key":"GWFLOLi2ff"},"references":{"cite":{"order":["Poldrack:2023aa"],"data":{"Poldrack:2023aa":{"label":"Poldrack:2023aa","enumerator":"1","html":"Poldrack, R. A., Lu, T., & Beguš, G. (2023). <i>AI-assisted coding: Experiments with GPT-4</i>. <a target=\"_blank\" rel=\"noreferrer\" href=\"https://arxiv.org/abs/2304.13187\">https://arxiv.org/abs/2304.13187</a>","url":"https://arxiv.org/abs/2304.13187"}}}},"footer":{"navigation":{"prev":{"title":"Principles of software engineering","url":"/software-engineering","group":"Better Code, Better Science"},"next":{"title":"Coding with AI","url":"/ai-coding-assistants","group":"Better Code, Better Science"}}},"domain":"http://localhost:3000"}